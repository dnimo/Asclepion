#!/usr/bin/env python3
"""
LLM APIé…ç½®å’Œæµ‹è¯•å·¥å…·
æ”¯æŒé…ç½®å’Œæµ‹è¯•ä¸åŒçš„LLMæä¾›è€…
"""

import os
import asyncio
import json
from typing import Dict, Any, Optional

def load_env_config():
    """åŠ è½½ç¯å¢ƒé…ç½®"""
    config = {}
    
    # å°è¯•ä».envæ–‡ä»¶åŠ è½½
    env_file = '.env'
    if os.path.exists(env_file):
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    config[key.strip()] = value.strip()
    
    # ä»ç¯å¢ƒå˜é‡è¦†ç›–
    config.update(os.environ)
    
    return config

def create_llm_config(provider: str, model: str = None, api_key: str = None) -> Dict[str, Any]:
    """åˆ›å»ºLLMé…ç½®"""
    env_config = load_env_config()
    
    configs = {
        'openai': {
            'provider_type': 'openai',
            'model_name': model or env_config.get('DEFAULT_MODEL', 'gpt-4'),
            'api_key': api_key or env_config.get('OPENAI_API_KEY'),
            'base_url': env_config.get('OPENAI_BASE_URL', 'https://api.openai.com/v1'),
            'temperature': 0.7,
            'max_tokens': 800,
            'timeout': 30.0
        },
        'anthropic': {
            'provider_type': 'anthropic',
            'model_name': model or 'claude-3-sonnet-20240229',
            'api_key': api_key or env_config.get('ANTHROPIC_API_KEY'),
            'base_url': env_config.get('ANTHROPIC_BASE_URL', 'https://api.anthropic.com/v1'),
            'temperature': 0.7,
            'max_tokens': 800,
            'timeout': 30.0
        },
        'local': {
            'provider_type': 'local',
            'model_name': model or 'llama2:7b',
            'base_url': env_config.get('LOCAL_MODEL_URL', 'http://localhost:11434'),
            'temperature': 0.7,
            'max_tokens': 800,
            'timeout': 60.0
        },
        'mock': {
            'provider_type': 'mock',
            'model_name': 'mock-model',
            'temperature': 0.0,
            'max_tokens': 500,
            'timeout': 1.0
        }
    }
    
    if provider not in configs:
        raise ValueError(f"Unsupported provider: {provider}. Available: {list(configs.keys())}")
    
    return configs[provider]

async def test_llm_provider(provider: str, model: str = None, api_key: str = None):
    """æµ‹è¯•LLMæä¾›è€…"""
    print(f"ğŸ§ª æµ‹è¯• {provider} LLMæä¾›è€…...")
    
    try:
        config = create_llm_config(provider, model, api_key)
        
        if provider == 'mock':
            # æµ‹è¯•æ¨¡æ‹Ÿæä¾›è€…
            from hospital_simulation_complete import SimpleLLMProvider
            llm = SimpleLLMProvider('mock')
            
            test_obs = [0.3, 0.7, 0.2, 0.8]
            test_constraints = {'min_quality_control': 0.5}
            
            result = llm.generate_action('doctors', test_obs, test_constraints)
            print(f"âœ… æ¨¡æ‹ŸLLMæµ‹è¯•æˆåŠŸ")
            print(f"   è¾“å…¥è§‚æµ‹: {test_obs}")
            print(f"   è¾“å‡ºåŠ¨ä½œ: {result.tolist()}")
            return True
            
        elif provider == 'openai':
            # æµ‹è¯•OpenAI API
            if not config['api_key'] or config['api_key'] == 'your_openai_api_key_here':
                print("âŒ OpenAI APIå¯†é’¥æœªé…ç½®")
                return False
            
            import httpx
            headers = {
                "Authorization": f"Bearer {config['api_key']}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": config['model_name'],
                "messages": [
                    {"role": "user", "content": "è¯·è¿”å›ä¸€ä¸ªç®€å•çš„æµ‹è¯•å“åº”"}
                ],
                "max_tokens": 50
            }
            
            async with httpx.AsyncClient(timeout=config['timeout']) as client:
                response = await client.post(
                    f"{config['base_url']}/chat/completions",
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    print("âœ… OpenAI APIæµ‹è¯•æˆåŠŸ")
                    print(f"   æ¨¡å‹: {config['model_name']}")
                    print(f"   å“åº”: {result['choices'][0]['message']['content'][:100]}...")
                    return True
                else:
                    print(f"âŒ OpenAI APIæµ‹è¯•å¤±è´¥: {response.status_code}")
                    print(f"   é”™è¯¯: {response.text}")
                    return False
        
        elif provider == 'anthropic':
            # æµ‹è¯•Anthropic API
            if not config['api_key'] or config['api_key'] == 'your_anthropic_api_key_here':
                print("âŒ Anthropic APIå¯†é’¥æœªé…ç½®")
                return False
            
            import httpx
            headers = {
                "x-api-key": config['api_key'],
                "Content-Type": "application/json",
                "anthropic-version": "2023-06-01"
            }
            
            payload = {
                "model": config['model_name'],
                "max_tokens": 50,
                "messages": [
                    {"role": "user", "content": "è¯·è¿”å›ä¸€ä¸ªç®€å•çš„æµ‹è¯•å“åº”"}
                ]
            }
            
            async with httpx.AsyncClient(timeout=config['timeout']) as client:
                response = await client.post(
                    f"{config['base_url']}/messages",
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    print("âœ… Anthropic APIæµ‹è¯•æˆåŠŸ")
                    print(f"   æ¨¡å‹: {config['model_name']}")
                    print(f"   å“åº”: {result['content'][0]['text'][:100]}...")
                    return True
                else:
                    print(f"âŒ Anthropic APIæµ‹è¯•å¤±è´¥: {response.status_code}")
                    print(f"   é”™è¯¯: {response.text}")
                    return False
        
        elif provider == 'local':
            # æµ‹è¯•æœ¬åœ°æ¨¡å‹
            import httpx
            payload = {
                "model": config['model_name'],
                "prompt": "è¯·è¿”å›ä¸€ä¸ªç®€å•çš„æµ‹è¯•å“åº”",
                "options": {"num_predict": 50}
            }
            
            try:
                async with httpx.AsyncClient(timeout=config['timeout']) as client:
                    response = await client.post(
                        f"{config['base_url']}/api/generate",
                        json=payload
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        print("âœ… æœ¬åœ°æ¨¡å‹æµ‹è¯•æˆåŠŸ")
                        print(f"   æ¨¡å‹: {config['model_name']}")
                        print(f"   å“åº”: {result['response'][:100]}...")
                        return True
                    else:
                        print(f"âŒ æœ¬åœ°æ¨¡å‹æµ‹è¯•å¤±è´¥: {response.status_code}")
                        return False
            except httpx.ConnectError:
                print("âŒ æ— æ³•è¿æ¥åˆ°æœ¬åœ°æ¨¡å‹æœåŠ¡")
                print("   è¯·ç¡®ä¿Ollamaç­‰æœ¬åœ°æœåŠ¡æ­£åœ¨è¿è¡Œ")
                return False
        
    except Exception as e:
        print(f"âŒ {provider} æµ‹è¯•å¤±è´¥: {e}")
        return False

def interactive_setup():
    """äº¤äº’å¼è®¾ç½®LLM API"""
    print("ğŸ”§ åŒ»é™¢æ²»ç†ç³»ç»ŸLLMé…ç½®å‘å¯¼")
    print("=" * 50)
    
    providers = ['mock', 'openai', 'anthropic', 'local']
    
    print("å¯ç”¨çš„LLMæä¾›è€…:")
    for i, provider in enumerate(providers, 1):
        descriptions = {
            'mock': 'æ¨¡æ‹ŸLLMï¼ˆæ— éœ€APIï¼Œç”¨äºæµ‹è¯•ï¼‰',
            'openai': 'OpenAI GPTæ¨¡å‹ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰',
            'anthropic': 'Anthropic Claudeæ¨¡å‹ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰',
            'local': 'æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚Ollamaï¼Œéœ€è¦æœ¬åœ°æœåŠ¡ï¼‰'
        }
        print(f"  {i}. {provider} - {descriptions[provider]}")
    
    while True:
        try:
            choice = input("\\nè¯·é€‰æ‹©LLMæä¾›è€… (1-4): ").strip()
            provider_idx = int(choice) - 1
            if 0 <= provider_idx < len(providers):
                selected_provider = providers[provider_idx]
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-4ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    print(f"\\né€‰æ‹©çš„æä¾›è€…: {selected_provider}")
    
    # æ ¹æ®é€‰æ‹©çš„æä¾›è€…è·å–é…ç½®
    api_key = None
    model = None
    
    if selected_provider == 'openai':
        api_key = input("è¯·è¾“å…¥OpenAI APIå¯†é’¥ (æˆ–æŒ‰å›è½¦ä½¿ç”¨ç¯å¢ƒå˜é‡): ").strip()
        model = input("è¯·è¾“å…¥æ¨¡å‹åç§° (é»˜è®¤: gpt-4): ").strip() or 'gpt-4'
    elif selected_provider == 'anthropic':
        api_key = input("è¯·è¾“å…¥Anthropic APIå¯†é’¥ (æˆ–æŒ‰å›è½¦ä½¿ç”¨ç¯å¢ƒå˜é‡): ").strip()
        model = input("è¯·è¾“å…¥æ¨¡å‹åç§° (é»˜è®¤: claude-3-sonnet-20240229): ").strip() or 'claude-3-sonnet-20240229'
    elif selected_provider == 'local':
        model = input("è¯·è¾“å…¥æœ¬åœ°æ¨¡å‹åç§° (é»˜è®¤: llama2:7b): ").strip() or 'llama2:7b'
    
    return selected_provider, model, api_key or None

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ åŒ»é™¢æ²»ç†ç³»ç»ŸLLM APIé…ç½®å’Œæµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥ç°æœ‰é…ç½®
    env_config = load_env_config()
    print("\\nå½“å‰ç¯å¢ƒé…ç½®:")
    for key in ['OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'LOCAL_MODEL_URL']:
        value = env_config.get(key, 'æœªè®¾ç½®')
        if 'API_KEY' in key and value and value != 'æœªè®¾ç½®':
            value = value[:8] + '***' if len(value) > 8 else '***'
        print(f"  {key}: {value}")
    
    # æµ‹è¯•æ‰€æœ‰å¯ç”¨æä¾›è€…
    print("\\nğŸ§ª æµ‹è¯•æ‰€æœ‰LLMæä¾›è€…:")
    print("-" * 30)
    
    test_results = {}
    
    # æµ‹è¯•Mockï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰
    test_results['mock'] = await test_llm_provider('mock')
    
    # æµ‹è¯•å…¶ä»–æä¾›è€…
    for provider in ['openai', 'anthropic', 'local']:
        test_results[provider] = await test_llm_provider(provider)
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\\nğŸ“‹ æµ‹è¯•ç»“æœæ‘˜è¦:")
    print("-" * 30)
    available_providers = []
    for provider, success in test_results.items():
        status = "âœ… å¯ç”¨" if success else "âŒ ä¸å¯ç”¨"
        print(f"  {provider}: {status}")
        if success:
            available_providers.append(provider)
    
    print(f"\\nå¯ç”¨æä¾›è€…: {', '.join(available_providers)}")
    
    # äº¤äº’å¼é…ç½®ï¼ˆå¯é€‰ï¼‰
    setup_choice = input("\\næ˜¯å¦éœ€è¦äº¤äº’å¼é…ç½®? (y/n): ").strip().lower()
    if setup_choice == 'y':
        provider, model, api_key = interactive_setup()
        print(f"\\né…ç½®å®Œæˆ: {provider}")
        if model:
            print(f"æ¨¡å‹: {model}")
        
        # æµ‹è¯•æ–°é…ç½®
        print("\\næµ‹è¯•æ–°é…ç½®...")
        success = await test_llm_provider(provider, model, api_key)
        if success:
            print("âœ… é…ç½®æµ‹è¯•æˆåŠŸï¼å¯ä»¥å¼€å§‹ä»¿çœŸäº†")
            
            # ä¿å­˜é…ç½®ç¤ºä¾‹
            config_example = create_llm_config(provider, model, api_key)
            print("\\nç¤ºä¾‹é…ç½®:")
            print(json.dumps(config_example, indent=2))
        else:
            print("âŒ é…ç½®æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è®¾ç½®")
    
    print("\\nğŸš€ LLMé…ç½®å®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥è¿è¡Œ 'python hospital_simulation_complete.py' å¼€å§‹ä»¿çœŸ")

if __name__ == '__main__':
    asyncio.run(main())