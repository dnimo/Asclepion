#!/usr/bin/env python3
"""
LLM-Actorä¸ç°æœ‰Agentç³»ç»Ÿé›†æˆæ¼”ç¤º

å±•ç¤ºå¦‚ä½•å°†LLM-Actorå†³ç­–ç³»ç»Ÿé›†æˆåˆ°åŸºäºä»·å€¼å‡½æ•°çš„ç­–ç•¥æ¢¯åº¦æ¶æ„ä¸­
"""

import numpy as np
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.hospital_governance.agents.role_agents import (
    RoleManager, SystemState, AgentConfig
)
from src.hospital_governance.agents.llm_actor_system import LLMActorDecisionSystem


def demo_llm_actor_integration():
    """æ¼”ç¤ºLLM-Actoré›†æˆ"""
    print("=" * 80)
    print("ğŸ¯ LLM-Actorä¸ä»·å€¼å‡½æ•°æ¶æ„é›†æˆæ¼”ç¤º")
    print("=" * 80)
    
    # 1. åˆ›å»ºä¼ ç»Ÿçš„Agentç³»ç»Ÿ
    print("\nğŸ“¦ æ­¥éª¤1: åˆ›å»ºä¼ ç»ŸAgentç³»ç»Ÿï¼ˆç­–ç•¥æ¢¯åº¦ + ä»·å€¼å‡½æ•°ï¼‰")
    print("-" * 80)
    
    role_manager = RoleManager()
    role_manager.create_all_agents()
    
    print(f"âœ… åˆ›å»ºäº† {role_manager.get_agent_count()} ä¸ªæ™ºèƒ½ä½“:")
    for role in role_manager.agents.keys():
        agent = role_manager.get_agent(role)
        print(f"  â€¢ {role}: observation_dim={agent.state_dim}, action_dim={agent.action_dim}")
        print(f"    æ”¶ç›Šæƒé‡: Î±={agent.alpha:.2f}, Î²={agent.beta:.2f}, Î³={agent.gamma:.2f}")
    
    # 2. åˆ›å»ºLLM-Actorå†³ç­–ç³»ç»Ÿ
    print("\nğŸ¤– æ­¥éª¤2: åˆ›å»ºLLM-Actorå†³ç­–ç³»ç»Ÿ")
    print("-" * 80)
    
    llm_actor_system = LLMActorDecisionSystem(
        llm_provider="mock",
        n_candidates=5,
        state_dim=16,  # å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€ç»´åº¦
        device='cpu'
    )
    
    print("âœ… LLM-Actorç³»ç»Ÿå·²åˆ›å»º")
    print(f"  â€¢ å€™é€‰ç”Ÿæˆå™¨: æ¯æ¬¡ç”Ÿæˆ {llm_actor_system.n_candidates} ä¸ªå€™é€‰")
    print(f"  â€¢ è¯­ä¹‰åµŒå…¥å™¨: {llm_actor_system.embedder.embedding_dim}ç»´å‘é‡")
    print(f"  â€¢ Actoré€‰æ‹©å™¨: çŠ¶æ€ç»´åº¦={llm_actor_system.selector.state_dim}")
    
    # 3. ä¸ºæ‰€æœ‰Agentå¯ç”¨LLM-Actor
    print("\nğŸ”— æ­¥éª¤3: å¯ç”¨LLM-Actoræ¨¡å¼")
    print("-" * 80)
    
    role_manager.enable_llm_actor_for_all(llm_actor_system)
    
    print("âœ… æ‰€æœ‰æ™ºèƒ½ä½“å·²åˆ‡æ¢åˆ°LLM-Actoræ¨¡å¼")
    print("  å†³ç­–æµç¨‹: LLMç”Ÿæˆå€™é€‰ â†’ Actoré€‰æ‹© â†’ è§£æä¸ºæ§åˆ¶å‘é‡")
    
    # 4. æ¨¡æ‹Ÿç¯å¢ƒå¹¶ç”Ÿæˆå†³ç­–
    print("\nğŸ® æ­¥éª¤4: æ¨¡æ‹Ÿå¤šè½®å†³ç­–")
    print("-" * 80)
    
    # åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒ
    env_state = {
        'medical_resource_utilization': 0.82,
        'patient_waiting_time': 0.35,
        'financial_indicator': 0.68,
        'ethical_compliance': 0.91,
        'education_training_quality': 0.75,
        'intern_satisfaction': 0.70,
        'patient_satisfaction': 0.83,
        'service_accessibility': 0.78,
        'care_quality_index': 0.88,
        'safety_incident_rate': 0.08,
        'operational_efficiency': 0.72,
        'staff_workload_balance': 0.65,
        'crisis_response_capability': 0.80,
        'regulatory_compliance_score': 0.89
    }
    
    system_state = SystemState(
        medical_resource_utilization=env_state['medical_resource_utilization'],
        patient_waiting_time=env_state['patient_waiting_time'],
        financial_indicator=env_state['financial_indicator'],
        ethical_compliance=env_state['ethical_compliance'],
        education_training_quality=env_state['education_training_quality'],
        intern_satisfaction=env_state['intern_satisfaction'],
        professional_development=0.68,
        mentorship_effectiveness=0.76,
        patient_satisfaction=env_state['patient_satisfaction'],
        service_accessibility=env_state['service_accessibility'],
        care_quality_index=env_state['care_quality_index'],
        safety_incident_rate=env_state['safety_incident_rate'],
        operational_efficiency=env_state['operational_efficiency'],
        staff_workload_balance=env_state['staff_workload_balance'],
        crisis_response_capability=env_state['crisis_response_capability'],
        regulatory_compliance_score=env_state['regulatory_compliance_score']
    )
    # å°†å…¨å±€16ç»´çŠ¶æ€æ³¨å…¥åˆ°æ‰€æœ‰agentï¼Œç¡®ä¿ä»·å€¼ç½‘ç»œè¯„ä¼°æ˜¯ç¯å¢ƒçº§
    for agent in role_manager.agents.values():
        try:
            agent.set_global_state(system_state.to_vector())
        except Exception:
            pass
    
    ideal_state = SystemState.from_vector(np.full(16, 0.9))  # ç†æƒ³çŠ¶æ€
    
    # æ‰§è¡Œ3è½®å†³ç­–
    n_steps = 3
    for step in range(n_steps):
        print(f"\n--- ç¬¬ {step+1}/{n_steps} è½®å†³ç­– ---")
        
        step_tokens = 0
        step_actions = {}
        
        for role, agent in role_manager.agents.items():
            # è·å–è§‚æµ‹
            observation = agent.observe(env_state)
            
            # é€‰æ‹©åŠ¨ä½œï¼ˆä½¿ç”¨LLM-Actorï¼‰
            action = agent.select_action(observation, training=True)
            step_actions[role] = action
            
            # æ˜¾ç¤ºå†³ç­–ä¿¡æ¯
            if agent._last_llm_result:
                result = agent._last_llm_result
                print(f"\n{role}:")
                print(f"  å€™é€‰æ•°é‡: {len(result.candidates)}")
                print(f"  é€‰æ‹©çš„åŠ¨ä½œ: {result.selected_action}")
                print(f"  Tokenæ¶ˆè€—: {result.tokens_used}")
                print(f"  åŠ¨ä½œå‘é‡: {result.action_vector[:3]}... (å‰3ç»´)")
                step_tokens += result.tokens_used
            else:
                print(f"\n{role}: [ç­–ç•¥æ¢¯åº¦æ¨¡å¼]")
        
        print(f"\næœ¬è½®æ€»Tokenæ¶ˆè€—: {step_tokens}")
    
    # 5. å±•ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ­¥éª¤5: LLMä½¿ç”¨ç»Ÿè®¡")
    print("-" * 80)
    
    llm_stats = role_manager.get_llm_statistics_summary()
    
    for role, stats in llm_stats.items():
        if role == '_aggregate':
            print(f"\nğŸ“ˆ æ±‡æ€»ç»Ÿè®¡:")
            print(f"  â€¢ æ€»Tokenæ¶ˆè€—: {stats['total_tokens_all_agents']}")
            print(f"  â€¢ æ€»è°ƒç”¨æ¬¡æ•°: {stats['total_calls_all_agents']}")
            print(f"  â€¢ å¹³å‡æ¯æ¬¡Token: {stats['avg_tokens_per_call']:.1f}")
        else:
            print(f"\n{role}:")
            print(f"  â€¢ è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}")
            print(f"  â€¢ Tokenæ¶ˆè€—: {stats['total_tokens']}")
            print(f"  â€¢ å¹³å‡Token: {stats['avg_tokens_per_call']:.1f}")
    
    # 6. æ¼”ç¤ºå¥–åŠ±è®¡ç®—ï¼ˆåŒ…å«Tokenæˆæœ¬ï¼‰
    print("\nğŸ’° æ­¥éª¤6: å¥–åŠ±è®¡ç®—ï¼ˆåŒ…å«Tokenæˆæœ¬ï¼‰")
    print("-" * 80)
    
    token_cost_factor = 0.001  # æ¯ä¸ªtokençš„æˆæœ¬ç³»æ•°
    rejection_penalty_factor = 0.1  # æ‹’ç»æƒ©ç½šç³»æ•°
    
    for role, agent in role_manager.agents.items():
        if agent._last_llm_result:
            result = agent._last_llm_result
            
            # è®¡ç®—Tokenæˆæœ¬
            token_cost = result.tokens_used * token_cost_factor
            
            # è®¡ç®—æ‹’ç»æƒ©ç½š
            rejection_penalty = rejection_penalty_factor if result.was_rejected else 0.0
            
            # è®¡ç®—åŸºç¡€å¥–åŠ±ï¼ˆå‡è®¾ï¼‰
            base_reward = agent.compute_reward(
                system_state=system_state,
                action=0,  # ç®€åŒ–ï¼Œå®é™…éœ€è¦ä»action_vectorè§£æ
                global_utility=0.8,
                ideal_state=ideal_state,
                token_cost=token_cost,
                rejection_penalty=rejection_penalty
            )
            
            print(f"\n{role}:")
            print(f"  åŸºç¡€æ”¶ç›Š: Î±*U + Î²*V - Î³*D = {base_reward + token_cost + rejection_penalty:.3f}")
            print(f"  Tokenæˆæœ¬: -{token_cost:.4f} ({result.tokens_used} tokens)")
            print(f"  æ‹’ç»æƒ©ç½š: -{rejection_penalty:.4f}")
            print(f"  æœ€ç»ˆæ”¶ç›Š: {base_reward:.3f}")
    
    # 7. å¯¹æ¯”æ¨¡å¼åˆ‡æ¢
    print("\nğŸ”„ æ­¥éª¤7: æ¼”ç¤ºæ¨¡å¼åˆ‡æ¢")
    print("-" * 80)
    
    print("\nç¦ç”¨LLM-Actorï¼Œåˆ‡æ¢å›ç­–ç•¥æ¢¯åº¦...")
    role_manager.disable_llm_actor_for_all()
    
    # ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ¨¡å¼
    doctor = role_manager.get_agent('doctors')
    observation = doctor.observe(env_state)
    action_pg = doctor.select_action(observation, training=True)
    
    print(f"âœ… doctorsä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ¨¡å¼: {action_pg[:3]}... (å‰3ç»´)")
    
    print("\né‡æ–°å¯ç”¨LLM-Actor...")
    role_manager.enable_llm_actor_for_all(llm_actor_system)
    
    action_llm = doctor.select_action(observation, training=True)
    print(f"âœ… doctorsä½¿ç”¨LLM-Actoræ¨¡å¼: {action_llm[:3]}... (å‰3ç»´)")
    
    # 8. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    print("\nğŸ“ˆ æ­¥éª¤8: æ€§èƒ½æŒ‡æ ‡")
    print("-" * 80)
    
    perf_summary = role_manager.get_performance_summary()
    
    for role, metrics in perf_summary.items():
        print(f"\n{role}:")
        print(f"  æ€§èƒ½åˆ†æ•°: {metrics.get('performance_score', 0.0):.3f}")
        print(f"  ç´¯ç§¯å¥–åŠ±: {metrics.get('cumulative_reward', 0.0):.3f}")
        print(f"  ç­–ç•¥èŒƒæ•°: {metrics.get('policy_norm', 0.0):.3f}")
        
        if 'llm_enabled' in metrics:
            print(f"  LLMå¯ç”¨: {metrics['llm_enabled']}")
            print(f"  LLMè°ƒç”¨: {metrics.get('total_calls', 0)}")
            print(f"  Tokenæ€»è®¡: {metrics.get('total_tokens', 0)}")
    
    print("\n" + "=" * 80)
    print("âœ… LLM-Actoré›†æˆæ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)
    
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("  1. âœ… LLM-Actorä¸ä»·å€¼å‡½æ•°æ¶æ„å®Œå…¨å…¼å®¹")
    print("  2. âœ… å¯ä»¥åŠ¨æ€åˆ‡æ¢å†³ç­–æ¨¡å¼ï¼ˆLLM vs ç­–ç•¥æ¢¯åº¦ï¼‰")
    print("  3. âœ… å¥–åŠ±å‡½æ•°æ‰©å±•æ”¯æŒTokenæˆæœ¬å’Œæ‹’ç»æƒ©ç½š")
    print("  4. âœ… ä¿ç•™åŸæœ‰çš„ç­–ç•¥æ›´æ–°å’Œä»·å€¼ä¼°è®¡æœºåˆ¶")
    print("  5. âœ… å®Œæ•´çš„ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½")
    
    return role_manager, llm_actor_system


def demo_hybrid_training():
    """æ¼”ç¤ºæ··åˆè®­ç»ƒæ¨¡å¼"""
    print("\n" + "=" * 80)
    print("ğŸ”¬ æ··åˆè®­ç»ƒæ¨¡å¼æ¼”ç¤º")
    print("=" * 80)
    
    print("\næ··åˆè®­ç»ƒç­–ç•¥:")
    print("  1. åˆæœŸï¼ˆEpisode 0-100ï¼‰: çº¯ç­–ç•¥æ¢¯åº¦ï¼Œå¿«é€Ÿæ¢ç´¢")
    print("  2. ä¸­æœŸï¼ˆEpisode 100-500ï¼‰: LLMè¾…åŠ©ï¼Œç”Ÿæˆé«˜è´¨é‡å€™é€‰")
    print("  3. åæœŸï¼ˆEpisode 500+ï¼‰: é€‰æ‹©æ€§ä½¿ç”¨LLMï¼ˆå…³é”®æ—¶åˆ»ï¼‰")
    
    # åˆ›å»ºç³»ç»Ÿ
    role_manager = RoleManager()
    role_manager.create_all_agents()
    
    llm_actor_system = LLMActorDecisionSystem(
        llm_provider="mock",
        n_candidates=5,
        state_dim=16
    )
    
    # æ¨¡æ‹Ÿè®­ç»ƒé˜¶æ®µ
    for episode in [50, 150, 300, 600]:
        print(f"\n--- Episode {episode} ---")
        
        if episode < 100:
            print("ğŸ¯ ç­–ç•¥: çº¯ç­–ç•¥æ¢¯åº¦ï¼ˆæ¢ç´¢ï¼‰")
            role_manager.disable_llm_actor_for_all()
            mode = "Policy Gradient"
        elif episode < 500:
            print("ğŸ¤– ç­–ç•¥: LLM-Actorï¼ˆå­¦ä¹ é«˜è´¨é‡åŠ¨ä½œï¼‰")
            role_manager.enable_llm_actor_for_all(llm_actor_system)
            mode = "LLM-Actor"
        else:
            # é€‰æ‹©æ€§ä½¿ç”¨ï¼šåªåœ¨å…³é”®æƒ…å†µä¸‹è°ƒç”¨LLM
            print("âš¡ ç­–ç•¥: é€‰æ‹©æ€§LLMï¼ˆä»…å…³é”®æ—¶åˆ»ï¼‰")
            # è¿™é‡Œå¯ä»¥æ ¹æ®çŠ¶æ€ç‰¹å¾å†³å®šæ˜¯å¦å¯ç”¨
            role_manager.enable_llm_actor_for_all(llm_actor_system)
            mode = "Selective LLM"
        
        print(f"  å½“å‰æ¨¡å¼: {mode}")
    
    print("\nâœ… æ··åˆè®­ç»ƒæ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    # ä¸»é›†æˆæ¼”ç¤º
    role_manager, llm_system = demo_llm_actor_integration()
    
    # æ··åˆè®­ç»ƒæ¼”ç¤º
    demo_hybrid_training()
    
    print("\nğŸŠ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“š ä¸‹ä¸€æ­¥:")
    print("  1. åœ¨simulatorä¸­é›†æˆinteraction_engine")
    print("  2. å®ç°å®Œæ•´çš„è®­ç»ƒå¾ªç¯")
    print("  3. æ·»åŠ å¯è§†åŒ–å’Œç›‘æ§")
    print("  4. æ¥å…¥çœŸå®LLM APIï¼ˆOpenAI/Claudeï¼‰")
