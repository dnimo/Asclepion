"""
Report 3 æ¶æ„é›†æˆç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•å°†ä»¥ä¸‹ç»„ä»¶æ•´åˆï¼š
1. Fixed LLM Actorï¼ˆå›ºå®šå‚æ•°ç”Ÿæˆå™¨ï¼‰
2. Semantic Criticï¼ˆQ å€¼è¯„ä¼°ï¼‰
3. Holy Code è¯­ä¹‰ç¼–ç 
4. Bellman è®­ç»ƒå¾ªç¯
"""

import numpy as np
import torch
from typing import List, Dict, Any, Optional
import logging

from src.hospital_governance.agents.learning_models import (
    FixedLLMCandidateGenerator,
    NaturalLanguageActionParser,
    LLMGenerationResult,
    LLM_PARAMETERS_FROZEN
)
from src.hospital_governance.agents.semantic_critic import (
    SemanticEncoder,
    SemanticCritic,
    SemanticCriticTrainer,
    SemanticReplayBuffer,
    SemanticTransition,
    create_augmented_state
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Report3Agent:
    """
    åŸºäº Report 3 æ¶æ„çš„æ™ºèƒ½ä½“
    
    æ ¸å¿ƒæµç¨‹ï¼š
    1. LLM ç”Ÿæˆ K ä¸ªå€™é€‰åŠ¨ä½œï¼ˆå‚æ•°å†»ç»“ï¼‰
    2. Holy Code ç¼–ç ä¸ºè¯­ä¹‰çº¦æŸ Î¾(HC_t)
    3. æ„å»ºå¢å¼ºçŠ¶æ€ sÌƒ_t = [Ï†(x_t), Î¾(HC_t)]
    4. Critic è¯„ä¼°æ‰€æœ‰å€™é€‰çš„ Q å€¼
    5. é€‰æ‹© argmax Q çš„åŠ¨ä½œæ‰§è¡Œ
    6. æ”¶é›†è½¬æ¢å¹¶è®­ç»ƒ Criticï¼ˆBellman æ›´æ–°ï¼‰
    """
    
    def __init__(self,
                 role: str,
                 state_dim: int = 16,
                 hc_embedding_dim: int = 384,
                 action_embedding_dim: int = 384,
                 num_candidates: int = 5,
                 use_real_llm: bool = False):
        
        self.role = role
        self.state_dim = state_dim
        
        # 1. å›ºå®š LLM ç”Ÿæˆå™¨
        self.llm_generator = FixedLLMCandidateGenerator(
            num_candidates=num_candidates,
            use_mock=not use_real_llm  # Convert to use_mock parameter
        )
        
        # 2. è¯­ä¹‰ç¼–ç å™¨
        self.semantic_encoder = SemanticEncoder(
            llm_embedding_dim=hc_embedding_dim,
            use_mock=True
        )
        
        # 3. Critic ç½‘ç»œ
        self.critic = SemanticCritic(
            state_dim=state_dim,
            hc_embedding_dim=hc_embedding_dim,
            action_embedding_dim=action_embedding_dim
        )
        
        # 4. Critic è®­ç»ƒå™¨
        self.critic_trainer = SemanticCriticTrainer(
            critic=self.critic,
            encoder=self.semantic_encoder,
            lr=3e-4,
            gamma=0.99
        )
        
        # 5. ç»éªŒå›æ”¾
        self.replay_buffer = SemanticReplayBuffer(capacity=10000)
        
        # 6. åŠ¨ä½œè§£æå™¨
        self.action_parser = NaturalLanguageActionParser()
        
        # ç»Ÿè®¡
        self.episode_count = 0
        self.training_steps = 0
    
    def select_action(self,
                     system_state: np.ndarray,
                     holy_code_state: Dict[str, Any],
                     exploration_epsilon: float = 0.0) -> Dict[str, Any]:
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆReport 3 æµç¨‹ï¼‰
        
        Args:
            system_state: 16 ç»´ç³»ç»ŸçŠ¶æ€ Ï†(x_t)
            holy_code_state: Holy Code çŠ¶æ€
            exploration_epsilon: æ¢ç´¢ç‡ï¼ˆ0 = çº¯åˆ©ç”¨ï¼‰
            
        Returns:
            action_info: åŒ…å«åŠ¨ä½œæ–‡æœ¬ã€å‘é‡ã€Q å€¼ç­‰ä¿¡æ¯
        """
        # Step 1: LLM ç”Ÿæˆå€™é€‰ï¼ˆå‚æ•°å†»ç»“ï¼‰
        llm_result = self.llm_generator.generate_candidates(
            role=self.role,
            state={"system_state": system_state.tolist() if hasattr(system_state, 'tolist') else system_state},
            holy_code=holy_code_state
        )
        
        logger.info(f"âœ“ LLM ç”Ÿæˆ {len(llm_result.candidates)} ä¸ªå€™é€‰åŠ¨ä½œ")
        
        # Step 2: ç¼–ç  Holy Code
        hc_embedding = self.semantic_encoder.encode_holy_code(holy_code_state)
        
        # Step 3: æ„å»ºå¢å¼ºçŠ¶æ€
        augmented_state = create_augmented_state(system_state, hc_embedding)
        
        # Step 4: Exploration vs Exploitation
        if np.random.random() < exploration_epsilon:
            # éšæœºé€‰æ‹©ï¼ˆæ¢ç´¢ï¼‰
            selected_idx = np.random.randint(len(llm_result.candidates))
            selected_action = llm_result.candidates[selected_idx]
            q_value = 0.0
            logger.info(f"ğŸ² æ¢ç´¢æ¨¡å¼ï¼šéšæœºé€‰æ‹©å€™é€‰ {selected_idx}")
        else:
            # Critic é€‰æ‹©æœ€ä¼˜ï¼ˆåˆ©ç”¨ï¼‰
            selected_action, q_value = self.critic_trainer.select_best_action(
                augmented_state=augmented_state,
                action_candidates=llm_result.candidates
            )
            selected_idx = llm_result.candidates.index(selected_action)
            logger.info(f"ğŸ¯ åˆ©ç”¨æ¨¡å¼ï¼šé€‰æ‹©æœ€ä¼˜å€™é€‰ (Q={q_value:.3f})")
        
        # Step 5: è§£æä¸ºåŠ¨ä½œå‘é‡
        action_vector = self.action_parser.parse(selected_action, self.role)
        
        # Step 6: ç¼–ç åŠ¨ä½œ
        action_embedding = self.semantic_encoder.encode_action(selected_action)
        
        return {
            'action_text': selected_action,
            'action_vector': action_vector,
            'action_embedding': action_embedding.detach().cpu().numpy(),
            'q_value': q_value,
            'augmented_state': augmented_state,
            'candidates': llm_result.candidates,
            'selected_idx': selected_idx,
            'generation_id': llm_result.metadata.get('generation_id', 0),
            'holy_code_embedding': hc_embedding.detach().cpu().numpy()
        }
    
    def store_transition(self,
                        action_info: Dict[str, Any],
                        reward: float,
                        next_system_state: np.ndarray,
                        next_holy_code_state: Dict[str, Any],
                        done: bool):
        """
        å­˜å‚¨è½¬æ¢åˆ°ç»éªŒå›æ”¾
        
        Args:
            action_info: select_action è¿”å›çš„ä¿¡æ¯
            reward: å³æ—¶å¥–åŠ± r_t
            next_system_state: ä¸‹ä¸€çŠ¶æ€ x_{t+1}
            next_holy_code_state: ä¸‹ä¸€ä¸ª Holy Code çŠ¶æ€
            done: æ˜¯å¦ç»ˆæ­¢
        """
        # æ„å»ºä¸‹ä¸€ä¸ªå¢å¼ºçŠ¶æ€
        next_hc_embedding = self.semantic_encoder.encode_holy_code(next_holy_code_state)
        next_augmented_state = create_augmented_state(next_system_state, next_hc_embedding)
        
        # åˆ›å»ºè½¬æ¢
        transition = SemanticTransition(
            augmented_state=action_info['augmented_state'],
            action_text=action_info['action_text'],
            action_embedding=action_info['action_embedding'],
            reward=reward,
            next_augmented_state=next_augmented_state,
            done=done,
            role=self.role
        )
        
        self.replay_buffer.add(transition)
        logger.info(f"ğŸ’¾ å­˜å‚¨è½¬æ¢ï¼šreward={reward:.3f}, buffer_size={len(self.replay_buffer)}")
    
    def train_critic(self,
                    batch_size: int = 32,
                    next_candidates_fn: Optional[callable] = None) -> Dict[str, float]:
        """
        è®­ç»ƒ Critic ç½‘ç»œï¼ˆBellman æ›´æ–°ï¼‰
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            next_candidates_fn: ç”Ÿæˆä¸‹ä¸€çŠ¶æ€å€™é€‰çš„å‡½æ•°
            
        Returns:
            training_stats: è®­ç»ƒç»Ÿè®¡
        """
        if len(self.replay_buffer) < batch_size:
            logger.info(f"â¸ï¸  ç»éªŒä¸è¶³ï¼š{len(self.replay_buffer)}/{batch_size}")
            return {}
        
        # é‡‡æ · batch
        batch = self.replay_buffer.sample(batch_size)
        
        # é»˜è®¤å€™é€‰ç”Ÿæˆå‡½æ•°
        if next_candidates_fn is None:
            def default_fn(role, state):
                # ä½¿ç”¨ LLM ç”Ÿæˆä¸‹ä¸€æ­¥å€™é€‰
                result = self.llm_generator.generate_candidates(
                    role=role,
                    state={"augmented_state": state[:self.state_dim].tolist()},
                    holy_code=None
                )
                return result.candidates
            next_candidates_fn = default_fn
        
        # è®­ç»ƒæ­¥éª¤
        stats = self.critic_trainer.train_step(batch, next_candidates_fn)
        
        self.training_steps += 1
        
        logger.info(f"ğŸ“Š è®­ç»ƒæ­¥éª¤ {self.training_steps}: loss={stats.get('loss', 0):.4f}, "
                   f"mean_Q={stats.get('mean_q', 0):.3f}")
        
        return stats
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'role': self.role,
            'episode_count': self.episode_count,
            'training_steps': self.training_steps,
            'replay_buffer_size': len(self.replay_buffer),
            'llm_generation_count': self.llm_generator.generation_count,
            'critic_stats': {
                'losses': self.critic_trainer.training_stats['losses'][-10:],
                'q_values': self.critic_trainer.training_stats['q_values'][-10:],
            }
        }


def demo_report3_architecture():
    """æ¼”ç¤º Report 3 æ¶æ„çš„å®Œæ•´æµç¨‹"""
    
    print("=" * 80)
    print("Report 3 æ¶æ„æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»º Agent
    agent = Report3Agent(
        role='doctors',
        num_candidates=5,
        use_real_llm=False  # ä½¿ç”¨ Mock
    )
    
    print(f"\nâœ“ åˆ›å»º Agent: {agent.role}")
    print(f"  - LLM å‚æ•°å†»ç»“: {LLM_PARAMETERS_FROZEN}")
    print(f"  - ä½¿ç”¨ Mock LLM: {agent.llm_generator.use_mock}")
    print(f"  - Critic ç½‘ç»œ: {agent.critic.state_dim} + {agent.critic.hc_embedding_dim} â†’ Q")
    
    # æ¨¡æ‹Ÿç¯å¢ƒçŠ¶æ€
    system_state = np.array([
        0.7,  # æ‚£è€…æ»¡æ„åº¦
        0.8,  # åŒ»ç–—è´¨é‡
        0.6,  # èµ„æºåˆ©ç”¨ç‡
        0.5,  # æˆæœ¬æ•ˆç›Š
        0.7,  # ç³»ç»Ÿç¨³å®šæ€§
        0.65, # åºŠä½å ç”¨ç‡
        0.4,  # ç­‰å¾…æ—¶é—´
        0.75, # æ²»ç–—æˆåŠŸç‡
        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  # å¡«å……åˆ° 16 ç»´
    ])
    
    holy_code_state = {
        'active_rules': [
            {'description': 'æ‚£è€…å®‰å…¨ç¬¬ä¸€'},
            {'description': 'å…¬å¹³åˆ†é…åŒ»ç–—èµ„æº'},
            {'description': 'çŸ¥æƒ…åŒæ„åŸåˆ™'}
        ]
    }
    
    print(f"\nåˆå§‹çŠ¶æ€:")
    print(f"  - ç³»ç»ŸçŠ¶æ€: æ‚£è€…æ»¡æ„åº¦={system_state[0]:.2f}, åŒ»ç–—è´¨é‡={system_state[1]:.2f}")
    print(f"  - Holy Code: {len(holy_code_state['active_rules'])} æ¡æ´»è·ƒè§„åˆ™")
    
    # è¿è¡Œ 5 ä¸ª episodes
    for episode in range(5):
        print(f"\n{'='*80}")
        print(f"Episode {episode + 1}")
        print(f"{'='*80}")
        
        # Step 1: é€‰æ‹©åŠ¨ä½œ
        action_info = agent.select_action(
            system_state=system_state,
            holy_code_state=holy_code_state,
            exploration_epsilon=0.2 if episode < 3 else 0.0  # å‰3è½®æ¢ç´¢
        )
        
        print(f"\nğŸ“‹ å€™é€‰åŠ¨ä½œ:")
        for i, cand in enumerate(action_info['candidates']):
            marker = "âœ“" if i == action_info['selected_idx'] else " "
            print(f"  [{marker}] {i+1}. {cand}")
        
        print(f"\nğŸ¯ é€‰å®šåŠ¨ä½œ: {action_info['action_text']}")
        print(f"   Q å€¼: {action_info['q_value']:.3f}")
        print(f"   ç”Ÿæˆ ID: {action_info['generation_id']}")
        
        # Step 2: æ¨¡æ‹Ÿç¯å¢ƒåé¦ˆ
        reward = np.random.uniform(0.3, 0.9)  # æ¨¡æ‹Ÿå¥–åŠ±
        next_system_state = system_state + np.random.randn(16) * 0.05  # çŠ¶æ€æ¼”åŒ–
        next_system_state = np.clip(next_system_state, 0, 1)
        done = False
        
        print(f"\nğŸ“ˆ ç¯å¢ƒåé¦ˆ:")
        print(f"   å¥–åŠ±: {reward:.3f}")
        print(f"   ä¸‹ä¸€çŠ¶æ€: æ»¡æ„åº¦={next_system_state[0]:.2f}, è´¨é‡={next_system_state[1]:.2f}")
        
        # Step 3: å­˜å‚¨è½¬æ¢
        agent.store_transition(
            action_info=action_info,
            reward=reward,
            next_system_state=next_system_state,
            next_holy_code_state=holy_code_state,
            done=done
        )
        
        # Step 4: è®­ç»ƒ Criticï¼ˆä»ç¬¬ 2 è½®å¼€å§‹ï¼‰
        if episode >= 1:
            print(f"\nğŸ”§ è®­ç»ƒ Critic:")
            train_stats = agent.train_critic(batch_size=min(4, len(agent.replay_buffer)))
            
            if train_stats:
                print(f"   æŸå¤±: {train_stats.get('loss', 0):.4f}")
                print(f"   å¹³å‡ Q: {train_stats.get('mean_q', 0):.3f}")
                print(f"   ç›®æ ‡ Q: {train_stats.get('mean_target_q', 0):.3f}")
        
        # æ›´æ–°çŠ¶æ€
        system_state = next_system_state
        agent.episode_count += 1
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*80}")
    print("æœ€ç»ˆç»Ÿè®¡")
    print(f"{'='*80}")
    
    stats = agent.get_statistics()
    print(f"\nAgent: {stats['role']}")
    print(f"  - Episodes: {stats['episode_count']}")
    print(f"  - Training steps: {stats['training_steps']}")
    print(f"  - Replay buffer: {stats['replay_buffer_size']}")
    print(f"  - LLM generation count: {stats['llm_generation_count']}")
    print(f"  - Parameters frozen: {LLM_PARAMETERS_FROZEN}")
    
    print(f"\nCritic è®­ç»ƒæ›²çº¿ï¼ˆæœ€è¿‘ 10 æ­¥ï¼‰:")
    if stats['critic_stats']['losses']:
        print(f"  - æŸå¤±: {[f'{x:.4f}' for x in stats['critic_stats']['losses']]}")
        print(f"  - Q å€¼: {[f'{x:.3f}' for x in stats['critic_stats']['q_values']]}")
    
    print(f"\nâœ“ Report 3 æ¶æ„æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo_report3_architecture()
