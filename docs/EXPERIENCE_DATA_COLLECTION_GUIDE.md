# ç»éªŒæ•°æ®æ”¶é›†æœºåˆ¶è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†AsclepionåŒ»é™¢æ²»ç†ä»¿çœŸç³»ç»Ÿä¸­çš„ç»éªŒæ•°æ®æ”¶é›†æœºåˆ¶ï¼ŒåŒ…æ‹¬æ•°æ®ç»“æ„ã€æ”¶é›†æµç¨‹ã€å­˜å‚¨ç®¡ç†å’Œåœ¨MADDPGè®­ç»ƒä¸­çš„ä½¿ç”¨ã€‚

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ç»éªŒæ•°æ®çš„ä½œç”¨
- **å¼ºåŒ–å­¦ä¹ åŸºç¡€**: ä¸ºMADDPGæ¨¡å‹æä¾›è®­ç»ƒæ•°æ®
- **å†³ç­–ä¼˜åŒ–**: é€šè¿‡å†å²ç»éªŒæ”¹è¿›æ™ºèƒ½ä½“å†³ç­–è´¨é‡
- **åä½œå­¦ä¹ **: è¿æ¥LLMå†³ç­–å’ŒMADDPGå­¦ä¹ çš„æ¡¥æ¢
- **æŒç»­æ”¹è¿›**: å®ç°ç³»ç»Ÿçš„è‡ªæˆ‘ä¼˜åŒ–èƒ½åŠ›

## ğŸ”„ æ•°æ®æ”¶é›†æµç¨‹

### æ•´ä½“æµç¨‹å›¾

```
ä»¿çœŸæ­¥éª¤æ‰§è¡Œ â†’ æ™ºèƒ½ä½“å†³ç­– â†’ å¥–åŠ±è®¡ç®— â†’ ç»éªŒæ•°æ®æ”¶é›† â†’ å­˜å‚¨åˆ°ç¼“å†²åŒº
    â†“              â†“           â†“           â†“              â†“
  step()    LLM/MADDPGå†³ç­–   å¥–åŠ±ç³»ç»Ÿ    experience    experience_buffer
    â†“              â†“           â†“           â†“              â†“
è®®ä¼šå¬å¼€ â† æ£€æŸ¥ç¼“å†²åŒº â† æ•°æ®ç§¯ç´¯ â† æ ¼å¼éªŒè¯ â† é“¾æ¥çŠ¶æ€è½¬æ¢
    â†“
MADDPGè®­ç»ƒ â†’ æ¨¡å‹æ”¹è¿› â†’ æ›´å¥½çš„è¡¥å……å†³ç­–
```

### è¯¦ç»†æ‰§è¡Œæ­¥éª¤

#### 1. æ•°æ®æ”¶é›†è§¦å‘
```python
# åœ¨æ¯ä¸ªä»¿çœŸæ­¥éª¤çš„æœ€åé˜¶æ®µ
if self.config.enable_learning:
    self._collect_experience_data(step_data)
```

#### 2. æ•°æ®æå–å’Œæ„å»º
```python
def _collect_experience_data(self, step_data):
    current_state = self._get_current_state_dict()
    
    # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“æ”¶é›†ç»éªŒ
    for role, action_data in step_data['agent_actions'].items():
        experience = {
            'role': role,                    # æ™ºèƒ½ä½“è§’è‰²æ ‡è¯†
            'state': observation,            # å½“å‰çŠ¶æ€è§‚æµ‹
            'action': action_vector,         # æ‰§è¡Œçš„åŠ¨ä½œ
            'reward': reward_value,          # è·å¾—çš„å¥–åŠ±
            'next_state': None,             # ä¸‹ä¸€çŠ¶æ€(å»¶åå¡«å……)
            'done': False,                  # å›åˆç»“æŸæ ‡å¿—
            'step': self.current_step       # ä»¿çœŸæ­¥æ•°
        }
        self.experience_buffer.append(experience)
```

#### 3. çŠ¶æ€è½¬æ¢é“¾æ¥
```python
# æ™ºèƒ½åœ°é“¾æ¥å‰ä¸€æ­¥çš„next_state
if len(self.experience_buffer) >= 2:
    for i in range(len(self.experience_buffer) - len(step_data['agent_actions']), 
                   len(self.experience_buffer)):
        if i > 0 and self.experience_buffer[i-1]['next_state'] is None:
            self.experience_buffer[i-1]['next_state'] = self.experience_buffer[i]['state']
```

## ğŸ“Š æ•°æ®ç»“æ„è¯¦è§£

### ç»éªŒæ ·æœ¬æ ¼å¼

```python
experience = {
    # åŸºç¡€ä¿¡æ¯
    'role': str,              # æ™ºèƒ½ä½“è§’è‰² ('doctors', 'interns', 'patients', 'accountants', 'government')
    'step': int,              # ä»¿çœŸæ­¥æ•° (0, 1, 2, ...)
    'done': bool,             # å›åˆç»“æŸæ ‡å¿— (é€šå¸¸ä¸ºFalse)
    
    # å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ•°æ®
    'state': np.ndarray,      # å½“å‰çŠ¶æ€è§‚æµ‹ (16ç»´æµ®ç‚¹å‘é‡)
    'action': np.ndarray,     # æ‰§è¡Œçš„åŠ¨ä½œ (3-4ç»´æµ®ç‚¹å‘é‡)
    'reward': float,          # è·å¾—çš„å¥–åŠ± (-1.0 ~ 1.0)
    'next_state': np.ndarray  # ä¸‹ä¸€çŠ¶æ€è§‚æµ‹ (16ç»´æµ®ç‚¹å‘é‡)
}
```

### çŠ¶æ€è§‚æµ‹ (16ç»´å‘é‡)

çŠ¶æ€è§‚æµ‹é€šè¿‡ä»¥ä¸‹æ–¹å¼ç”Ÿæˆï¼š

```python
def _get_observation_for_role(self, role, current_state):
    if self.state_space:
        # ä¼˜å…ˆä½¿ç”¨ç³»ç»ŸçŠ¶æ€ç©ºé—´
        return self.state_space.get_state_vector().astype(np.float32)
    else:
        # é™çº§è§‚æµ‹ï¼šä»ç³»ç»ŸçŠ¶æ€å­—å…¸æå–
        state_values = list(current_state.values())
        while len(state_values) < 16:
            state_values.append(0.0)
        return np.array(state_values[:16], dtype=np.float32)
```

åŒ…å«ä¿¡æ¯ï¼š
- åŒ»ç–—è´¨é‡æŒ‡æ ‡
- è´¢åŠ¡çŠ¶å†µæŒ‡æ ‡
- æ‚£è€…æ»¡æ„åº¦
- äººå‘˜å·¥ä½œè´Ÿè½½
- ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
- é£é™©è¯„ä¼°æ•°æ®

### åŠ¨ä½œå‘é‡ (3-4ç»´)

åŠ¨ä½œå‘é‡æ¥æºå¤šæ ·åŒ–ï¼š

```python
# å¯èƒ½çš„åŠ¨ä½œæ¥æº
action_sources = {
    'LLM_Enhanced': 'ç”±LLMç”Ÿæˆçš„æ™ºèƒ½å†³ç­–',
    'MADDPG_Supplement': 'ç”±MADDPGæ¨¡å‹æä¾›çš„è¡¥å……å†³ç­–',
    'RoleAgent': 'åŸºäºè§’è‰²ç‰¹å¾çš„é»˜è®¤å†³ç­–',
    'Fallback': 'é™çº§æœºåˆ¶æä¾›çš„å®‰å…¨å†³ç­–'
}
```

ç»´åº¦å«ä¹‰ï¼ˆå› è§’è‰²è€Œå¼‚ï¼‰ï¼š
- åŒ»ç”Ÿ(4ç»´): æ²»ç–—å¼ºåº¦, èµ„æºé…ç½®, åä½œæ°´å¹³, åˆ›æ–°ç¨‹åº¦
- æŠ¤å£«(3ç»´): æŠ¤ç†è´¨é‡, åè°ƒæ•ˆç‡, åŸ¹è®­æŠ•å…¥
- æ‚£è€…(3ç»´): é…åˆåº¦, æ»¡æ„åº¦è¡¨è¾¾, åé¦ˆç§¯ææ€§
- ä¼šè®¡(3ç»´): æˆæœ¬æ§åˆ¶, é¢„ç®—ä¼˜åŒ–, è´¢åŠ¡æŠ¥å‘Š
- æ”¿åºœ(3ç»´): æ”¿ç­–æ”¯æŒ, ç›‘ç®¡å¼ºåº¦, èµ„æºæŠ•å…¥

### å¥–åŠ±è®¡ç®—

å¥–åŠ±é€šè¿‡å¤šå±‚ç³»ç»Ÿè®¡ç®—ï¼š

```python
def _compute_and_distribute_rewards(self, step_data):
    # 1. åŸºç¡€æ€§èƒ½å¥–åŠ±
    performance = step_data['metrics'].get('overall_performance', 0.5)
    base_rewards[role] = performance + np.random.normal(0, 0.1)
    
    # 2. å¥–åŠ±æ§åˆ¶ç³»ç»Ÿè°ƒæ•´
    if self.reward_control_system:
        # ä½¿ç”¨åˆ†å¸ƒå¼å¥–åŠ±æ§åˆ¶ç³»ç»Ÿ
        adjusted_rewards = self.reward_control_system.distribute_rewards(...)
    
    # 3. é™çº§å¥–åŠ±æœºåˆ¶
    else:
        fallback_rewards = self._compute_fallback_rewards()
```

## ğŸ—„ï¸ ç¼“å†²åŒºç®¡ç†

### é…ç½®å‚æ•°

```python
class SimulationConfig:
    maddpg_batch_size: int = 32      # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    maddpg_buffer_size: int = 10000  # ç¼“å†²åŒºæœ€å¤§å®¹é‡
    enable_learning: bool = True      # å¯ç”¨å­¦ä¹ åŠŸèƒ½
```

### å†…å­˜ç®¡ç†ç­–ç•¥

```python
# é˜²æ­¢å†…å­˜æº¢å‡º
if len(self.experience_buffer) > self.config.maddpg_buffer_size:
    # ä¿ç•™æœ€è¿‘ä¸€åŠçš„æ•°æ®
    self.experience_buffer = self.experience_buffer[-self.config.maddpg_buffer_size//2:]
    
# æ•°æ®è´¨é‡æ£€æŸ¥
valid_experiences = [exp for exp in self.experience_buffer 
                    if exp['next_state'] is not None]
```

### ç¼“å†²åŒºçŠ¶æ€ç›‘æ§

```python
buffer_status = {
    'total_size': len(self.experience_buffer),
    'valid_transitions': len([exp for exp in self.experience_buffer 
                             if exp['next_state'] is not None]),
    'roles_coverage': set(exp['role'] for exp in self.experience_buffer),
    'step_range': (min_step, max_step)
}
```

## ğŸ“ MADDPGè®­ç»ƒé›†æˆ

### è®­ç»ƒè§¦å‘æ¡ä»¶

```python
# è®®ä¼šç»“æŸåå¯åŠ¨è®­ç»ƒ
def _start_maddpg_training_after_parliament(self):
    # æ£€æŸ¥æ•°æ®å……è¶³æ€§
    if len(self.experience_buffer) < self.config.maddpg_batch_size:
        logger.info(f"ğŸ“Š ç»éªŒæ•°æ®ä¸è¶³({len(self.experience_buffer)}/{self.config.maddpg_batch_size})ï¼Œè·³è¿‡è®­ç»ƒ")
        return
    
    # å¯åŠ¨è®­ç»ƒ
    self.is_training_maddpg = True
    self._train_maddpg_model()
```

### æ•°æ®é¢„å¤„ç†

```python
def _train_maddpg_model(self):
    # æŒ‰è§’è‰²åˆ†ç»„ç»éªŒæ•°æ®
    role_batches = {}
    for role in ['doctors', 'interns', 'patients', 'accountants', 'government']:
        role_experiences = [exp for exp in self.experience_buffer 
                           if exp['role'] == role and exp['next_state'] is not None]
        if len(role_experiences) >= self.config.maddpg_batch_size:
            role_batches[role] = role_experiences[-self.config.maddpg_batch_size:]
    
    # æ•°æ®æ ¼å¼ç»Ÿä¸€åŒ–
    unified_batch = []
    for role, experiences in role_batches.items():
        for exp in experiences:
            unified_exp = {
                'role': role,
                'state': np.array(exp['state'], dtype=np.float32).flatten(),
                'action': np.array(exp['action'], dtype=np.float32).flatten(),
                'reward': float(exp['reward']),
                'next_state': np.array(exp['next_state'], dtype=np.float32).flatten(),
                'done': bool(exp.get('done', False))
            }
            unified_batch.append(unified_exp)
```

### è®­ç»ƒæ‰§è¡Œ

```python
# è®­ç»ƒæ¨¡å‹
losses = self.maddpg_model.train(unified_batch)
logger.info(f"ğŸ“ MADDPGè®­ç»ƒå®Œæˆ - æŸå¤±: {losses}")

# ä¿å­˜æ¨¡å‹
self.maddpg_model.save_models(self.config.maddpg_model_save_path)
logger.info(f"ğŸ’¾ MADDPGæ¨¡å‹å·²ä¿å­˜")
```

## ğŸ“ˆ æ•°æ®è´¨é‡ä¿è¯

### æ•°æ®éªŒè¯æœºåˆ¶

```python
def validate_experience(experience):
    checks = {
        'role_valid': experience['role'] in valid_roles,
        'state_shape': experience['state'].shape == (16,),
        'action_shape': len(experience['action']) in [3, 4],
        'reward_range': -2.0 <= experience['reward'] <= 2.0,
        'next_state_exists': experience['next_state'] is not None
    }
    return all(checks.values())
```

### å¼‚å¸¸å¤„ç†

```python
try:
    self._collect_experience_data(step_data)
except Exception as e:
    logger.warning(f"âš ï¸ æ”¶é›†ç»éªŒæ•°æ®å¤±è´¥: {e}")
    # ä½¿ç”¨é™çº§æ•°æ®æ”¶é›†æœºåˆ¶
    self._collect_fallback_experience(step_data)
```

## ğŸ”„ æŒç»­å­¦ä¹ å¾ªç¯

### å®Œæ•´å­¦ä¹ æµç¨‹

```
1. LLMæ™ºèƒ½ä½“å†³ç­– â†’ 2. æ‰§è¡ŒåŠ¨ä½œ â†’ 3. ç¯å¢ƒåé¦ˆ â†’ 4. å¥–åŠ±è®¡ç®—
                                           â†“
8. æ”¹è¿›çš„MADDPGè¡¥å…… â† 7. æ¨¡å‹æ›´æ–° â† 6. MADDPGè®­ç»ƒ â† 5. ç»éªŒæ”¶é›†
```

### æ—¶åºåè°ƒ

```python
# æ­£å¸¸ä»¿çœŸçŠ¶æ€
if self._should_hold_parliament():
    ğŸ›ï¸ å¬å¼€è®®ä¼š â†’ å¯åŠ¨MADDPGè®­ç»ƒ
    self._start_maddpg_training_after_parliament()

elif self.is_training_maddpg:
    â³ è®®ä¼šç­‰å¾…çŠ¶æ€ â†’ MADDPGè®­ç»ƒä¸­
    # LLMå†³ç­–æš‚åœä½¿ç”¨MADDPGè¡¥å……
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### å…³é”®æŒ‡æ ‡

```python
training_metrics = {
    'buffer_utilization': len(self.experience_buffer) / self.config.maddpg_buffer_size,
    'data_quality_score': valid_transitions / total_transitions,
    'role_balance': min(role_counts.values()) / max(role_counts.values()),
    'training_frequency': training_count / total_steps,
    'model_improvement': latest_loss - baseline_loss
}
```

### æ—¥å¿—è®°å½•

```python
logger.info(f"ğŸ“Š ç»éªŒæ•°æ®æ”¶é›†çŠ¶æ€:")
logger.info(f"   ç¼“å†²åŒºå¤§å°: {len(self.experience_buffer)}")
logger.info(f"   æœ‰æ•ˆè½¬æ¢: {valid_transitions}")
logger.info(f"   è§’è‰²è¦†ç›–: {covered_roles}")
logger.info(f"   æœ€æ–°å¥–åŠ±: {recent_rewards}")
```

## ğŸš€ æœ€ä½³å®è·µ

### æ•°æ®æ”¶é›†ä¼˜åŒ–

1. **åŠæ—¶æ”¶é›†**: æ¯æ­¥ç«‹å³æ”¶é›†ï¼Œé¿å…æ•°æ®ä¸¢å¤±
2. **æ ¼å¼ç»Ÿä¸€**: ç¡®ä¿æ•°æ®ç±»å‹å’Œç»´åº¦ä¸€è‡´æ€§
3. **è´¨é‡æ£€æŸ¥**: éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
4. **å†…å­˜ç®¡ç†**: é€‚æ—¶æ¸…ç†è¿‡æœŸæ•°æ®

### è®­ç»ƒæ•ˆç‡æå‡

1. **æ‰¹é‡å¤„ç†**: ç§¯ç´¯è¶³å¤Ÿæ•°æ®åæ‰¹é‡è®­ç»ƒ
2. **è§’è‰²å¹³è¡¡**: ç¡®ä¿æ¯ä¸ªè§’è‰²æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ ·æœ¬
3. **å¼‚æ­¥è®­ç»ƒ**: é¿å…è®­ç»ƒé˜»å¡ä»¿çœŸè¿›ç¨‹
4. **æ¨¡å‹ä¿å­˜**: å®šæœŸä¿å­˜è®­ç»ƒè¿›åº¦

### æ•…éšœæ¢å¤

1. **é™çº§æœºåˆ¶**: æ•°æ®æ”¶é›†å¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
2. **æ•°æ®æ¢å¤**: ä»å†å²è®°å½•æ¢å¤ä¸¢å¤±çš„ç»éªŒ
3. **å¢é‡ä¿®å¤**: ä¿®å¤ä¸å®Œæ•´çš„çŠ¶æ€è½¬æ¢
4. **ç›‘æ§æŠ¥è­¦**: åŠæ—¶å‘ç°æ•°æ®è´¨é‡é—®é¢˜

## ğŸ“‹ æ€»ç»“

ç»éªŒæ•°æ®æ”¶é›†æœºåˆ¶æ˜¯è¿æ¥LLMæ™ºèƒ½å†³ç­–å’ŒMADDPGå¼ºåŒ–å­¦ä¹ çš„å…³é”®æ¡¥æ¢ã€‚é€šè¿‡ï¼š

- **å®Œæ•´æ€§**: æ”¶é›†çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±-ä¸‹ä¸€çŠ¶æ€çš„å®Œæ•´è½¬æ¢
- **é«˜æ•ˆæ€§**: æ¯æ­¥è‡ªåŠ¨æ”¶é›†ï¼Œæ— éœ€é¢å¤–å¼€é”€  
- **å¯é æ€§**: å¤šå±‚éªŒè¯å’Œå¼‚å¸¸å¤„ç†æœºåˆ¶
- **å¯æ‰©å±•æ€§**: æ”¯æŒå¤šæ™ºèƒ½ä½“å¹¶è¡Œæ•°æ®æ”¶é›†

ç³»ç»Ÿå®ç°äº†çœŸæ­£çš„åä½œå­¦ä¹ ï¼Œè®©LLMçš„æ™ºèƒ½å†³ç­–ç»éªŒæŒç»­ç§¯ç´¯å¹¶ç”¨äºè®­ç»ƒMADDPGæ¨¡å‹ï¼Œå½¢æˆæ™ºèƒ½å†³ç­–çš„æ­£å‘å¾ªç¯ã€‚

---

*æœ€åæ›´æ–°: 2025å¹´10æœˆ7æ—¥*
*æ–‡æ¡£ç‰ˆæœ¬: v1.0*
*ç»´æŠ¤è€…: Asclepionå¼€å‘å›¢é˜Ÿ*