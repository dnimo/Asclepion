# MADDPG-LLMåä½œå†³ç­–æ¶æ„è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†AsclepionåŒ»é™¢æ²»ç†ä»¿çœŸç³»ç»Ÿä¸­MADDPGï¼ˆMulti-Agent Deep Deterministic Policy Gradientï¼‰å’ŒLLMï¼ˆLarge Language Modelï¼‰ä¹‹é—´çš„åä½œå†³ç­–æ¶æ„ï¼ŒåŒ…æ‹¬é€»è¾‘å…³ç³»ã€å†³ç­–æµç¨‹ã€èåˆæœºåˆ¶å’Œç³»ç»Ÿä¼˜åŠ¿ã€‚

## ğŸ§  æ ¸å¿ƒæ¶æ„è®¾è®¡

### ğŸ¯ è®¾è®¡ç†å¿µ

**ä»ç«äº‰åˆ°åä½œçš„æ¼”è¿›**
- âŒ **æ—§æ¨¡å¼**: `MADDPG OR LLM` (äºŒé€‰ä¸€ç«äº‰æ¨¡å¼)
- âœ… **æ–°æ¨¡å¼**: `LLM + MADDPG` (åä½œèåˆæ¨¡å¼)

**æ ¸å¿ƒåŸåˆ™**
1. **æ™ºèƒ½ä¸»å¯¼**: LLMæä¾›è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›
2. **æ•°æ®è¡¥å……**: MADDPGæä¾›ç»éªŒå­¦ä¹ çš„æ•°å€¼ä¼˜åŒ–
3. **æŒç»­æ”¹è¿›**: é€šè¿‡ç»éªŒæ”¶é›†ä¸æ–­æå‡æ€§èƒ½
4. **å®¹é”™æœºåˆ¶**: å¤šå±‚å†³ç­–ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åä½œå¼å¤šå±‚å†³ç­–ç³»ç»Ÿ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ¯ ä¸»å¯¼å±‚: LLM + è§’è‰²æ™ºèƒ½ä½“                                    â”‚
â”‚   â”œâ”€â”€ åŸºäºè§‚æµ‹è‡ªåŠ¨ç”ŸæˆåŠ¨ä½œ                                     â”‚
â”‚   â”œâ”€â”€ èåˆè§’è‰²ç‰¹å¾å’ŒLLMæ¨ç†                                    â”‚
â”‚   â”œâ”€â”€ è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥                                     â”‚
â”‚   â””â”€â”€ ä¼˜å…ˆçº§: æœ€é«˜ (confidence: 0.85)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ¤– è¡¥å……å±‚: MADDPGæ¨¡å‹                                         â”‚
â”‚   â”œâ”€â”€ æä¾›æ•°æ®é©±åŠ¨çš„å†³ç­–å‚è€ƒ                                   â”‚
â”‚   â”œâ”€â”€ åŸºäºå†å²ç»éªŒçš„æ•°å€¼ä¼˜åŒ–                                   â”‚
â”‚   â”œâ”€â”€ ä½œä¸ºLLMå†³ç­–çš„éªŒè¯å’Œè¡¥å……                                  â”‚
â”‚   â””â”€â”€ ä¼˜å…ˆçº§: ä¸­ç­‰ (confidence: 0.8)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”„ åå¤‡å±‚: é™çº§å†³ç­–æœºåˆ¶                                        â”‚
â”‚   â”œâ”€â”€ è§’è‰²æ™ºèƒ½ä½“é»˜è®¤å†³ç­–                                       â”‚
â”‚   â”œâ”€â”€ ç³»ç»Ÿå®‰å…¨å†³ç­–                                           â”‚
â”‚   â”œâ”€â”€ å½“LLMå’ŒMADDPGéƒ½å¤±è´¥æ—¶å¯ç”¨                               â”‚
â”‚   â””â”€â”€ ä¼˜å…ˆçº§: æœ€ä½ (confidence: 0.5)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ å†³ç­–æµç¨‹è¯¦è§£

### 1. å†³ç­–ç”Ÿæˆé˜¶æ®µ

```python
# æ­¥éª¤1: å°è¯•LLM+è§’è‰²æ™ºèƒ½ä½“å†³ç­–
llm_decisions = None
if self.agent_registry and self.config.enable_llm_integration:
    llm_decisions = self._process_llm_agent_decisions()

# æ­¥éª¤2: è·å–MADDPGè¡¥å……å†³ç­–  
maddpg_decisions = None
if self.maddpg_model and self.config.enable_learning and not self.is_training_maddpg:
    maddpg_decisions = self._get_maddpg_decisions()

# æ­¥éª¤3: èåˆå†³ç­–
final_actions = self._combine_decisions(llm_decisions, maddpg_decisions)
```

### 2. LLMå†³ç­–å¤„ç†æµç¨‹

```python
def _process_llm_agent_decisions(self):
    """å¤„ç†LLM+è§’è‰²æ™ºèƒ½ä½“çš„è‡ªåŠ¨å†³ç­–ç”Ÿæˆ"""
    actions = {}
    
    for role, agent in agents.items():
        # 1. ç”Ÿæˆè§’è‰²ç‰¹å®šè§‚æµ‹
        observation = self._generate_observation_for_agent(role)
        
        # 2. æ„å»ºä¸°å¯Œä¸Šä¸‹æ–‡
        context = {
            'role': role,
            'observation': observation.tolist(),
            'system_state': current_state,
            'step': self.current_step,
            'simulation_time': self.simulation_time
        }
        
        # 3. LLMå¢å¼ºå†³ç­–ç”Ÿæˆ
        if hasattr(agent, 'llm_generator') and agent.llm_generator:
            holy_code_state = self.holy_code_manager.get_current_state()
            llm_response = agent.llm_generator.generate_action_sync(
                role=role,
                observation=observation,
                holy_code_state=holy_code_state,
                context=context
            )
            
            # 4. è§£æå’Œæ ¼å¼åŒ–
            action_vector, reasoning = self._parse_llm_response(llm_response, role)
            actions[role] = {
                'action_vector': action_vector,
                'agent_type': 'LLM_Enhanced',
                'confidence': 0.85,
                'reasoning': reasoning,
                'llm_response': llm_response[:200] + '...'
            }
```

### 3. MADDPGè¡¥å……å†³ç­–æµç¨‹

```python
def _get_maddpg_decisions(self):
    """è·å–MADDPGå†³ç­–ï¼ˆä¸ç›´æ¥ä½¿ç”¨ï¼Œä½œä¸ºè¡¥å……ï¼‰"""
    
    # 1. è·å–å„è§’è‰²è§‚æµ‹
    observations = {}
    current_state = self._get_current_state_dict()
    for role in ['doctors', 'interns', 'patients', 'accountants', 'government']:
        observations[role] = self._get_observation_for_role(role, current_state)
    
    # 2. ä½¿ç”¨MADDPGè·å–åŠ¨ä½œ
    maddpg_actions = self.maddpg_model.get_actions(observations, training=False)
    
    # 3. è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
    formatted_actions = {}
    for role, action_vector in maddpg_actions.items():
        formatted_actions[role] = {
            'action_vector': action_vector.tolist(),
            'agent_type': 'MADDPG_Supplement',
            'confidence': 0.8,
            'reasoning': f'{role}åŸºäºMADDPGæ¨¡å‹çš„è¡¥å……å†³ç­–'
        }
    
    return formatted_actions
```

### 4. å†³ç­–èåˆç­–ç•¥

```python
def _combine_decisions(self, llm_decisions, maddpg_decisions):
    """èåˆLLMå’ŒMADDPGå†³ç­–"""
    
    if llm_decisions:
        logger.info("ğŸ“ ä½¿ç”¨LLM+è§’è‰²æ™ºèƒ½ä½“ä¸»å¯¼å†³ç­–")
        
        # å¦‚æœæœ‰MADDPGè¡¥å……ï¼Œæ·»åŠ å‚è€ƒä¿¡æ¯
        if maddpg_decisions:
            for role in llm_decisions:
                if role in maddpg_decisions:
                    llm_decisions[role]['maddpg_reference'] = maddpg_decisions[role]['action_vector']
                    llm_decisions[role]['reasoning'] += " [å‚è€ƒMADDPGå»ºè®®]"
        
        return llm_decisions
    
    elif maddpg_decisions:
        logger.info("ğŸ¤– ä½¿ç”¨MADDPGè¡¥å……å†³ç­–")
        return maddpg_decisions
    
    else:
        logger.info("ğŸ”„ ä½¿ç”¨é™çº§å†³ç­–")
        return self._process_fallback_decisions()
```

## ğŸ›ï¸ è®®ä¼š-è®­ç»ƒç”Ÿå‘½å‘¨æœŸ

### æ—¶åºå…³ç³»å›¾

```
ä»¿çœŸæ­¥éª¤ â†’ LLMä¸»å¯¼å†³ç­– â†’ æ”¶é›†ç»éªŒ â†’ è®®ä¼šå¬å¼€ â†’ MADDPGè®­ç»ƒ â†’ ä»¿çœŸç»§ç»­
    â†‘                                                        â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è®­ç»ƒå®Œæˆï¼Œè®®ä¼šç»“æŸ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### çŠ¶æ€æœºè½¬æ¢

```python
# æ­£å¸¸ä»¿çœŸçŠ¶æ€
if self._should_hold_parliament():
    # è®®ä¼šçŠ¶æ€
    step_data['parliament_meeting'] = True
    step_data['parliament_result'] = self._run_parliament_meeting(step_data)
    
    # è®®ä¼šç»“æŸåå¯åŠ¨MADDPGè®­ç»ƒ
    self._start_maddpg_training_after_parliament()

elif self.is_training_maddpg:
    # è®­ç»ƒçŠ¶æ€
    step_data['parliament_waiting'] = True
    step_data['training_status'] = self._get_training_status()
    # æ³¨æ„ï¼šè®­ç»ƒæœŸé—´MADDPGä¸å‚ä¸å†³ç­–è¡¥å……
```

### è®­ç»ƒè§¦å‘æœºåˆ¶

```python
def _start_maddpg_training_after_parliament(self):
    """è®®ä¼šç»“æŸåå¯åŠ¨MADDPGè®­ç»ƒ"""
    
    # æ£€æŸ¥æ•°æ®å……è¶³æ€§
    if len(self.experience_buffer) < self.config.maddpg_batch_size:
        logger.info(f"ğŸ“Š ç»éªŒæ•°æ®ä¸è¶³({len(self.experience_buffer)}/{self.config.maddpg_batch_size})ï¼Œè·³è¿‡è®­ç»ƒ")
        return
    
    # å¯åŠ¨å¼‚æ­¥è®­ç»ƒ
    self.is_training_maddpg = True
    self.last_parliament_step = self.current_step
    
    logger.info(f"ğŸ“ å¯åŠ¨MADDPGè®­ç»ƒ - ç»éªŒæ•°æ®: {len(self.experience_buffer)}")
    
    try:
        self._train_maddpg_model()
    except Exception as e:
        logger.error(f"âŒ MADDPGè®­ç»ƒå¤±è´¥: {e}")
        self.is_training_maddpg = False
```

## ğŸ“Š å†³ç­–è´¨é‡è¯„ä¼°

### æ™ºèƒ½ä¼˜å…ˆçº§ç³»ç»Ÿ

```python
decision_hierarchy = {
    'LLM_Enhanced': {
        'confidence': 0.85,
        'capabilities': [
            'è¯­ä¹‰ç†è§£', 
            'ä¸Šä¸‹æ–‡æ¨ç†', 
            'å¤æ‚å†³ç­–', 
            'åˆ›æ–°æ€§æ€ç»´'
        ]
    },
    'MADDPG_Supplement': {
        'confidence': 0.8,
        'capabilities': [
            'æ•°å€¼ä¼˜åŒ–', 
            'ç»éªŒå­¦ä¹ ', 
            'ç¨³å®šå†³ç­–', 
            'é‡åŒ–åˆ†æ'
        ]
    },
    'RoleAgent': {
        'confidence': 0.7,
        'capabilities': [
            'è§’è‰²ç‰¹å¾', 
            'åŸºç¡€å†³ç­–', 
            'é»˜è®¤è¡Œä¸º'
        ]
    },
    'Fallback': {
        'confidence': 0.5,
        'capabilities': [
            'å®‰å…¨å†³ç­–', 
            'ç³»ç»Ÿç¨³å®š'
        ]
    }
}
```

### å†³ç­–è´¨é‡ç›‘æ§

```python
decision_metrics = {
    'llm_usage_rate': llm_decisions_count / total_decisions,
    'maddpg_supplement_rate': maddpg_supplement_count / total_decisions,
    'fallback_rate': fallback_decisions_count / total_decisions,
    'average_confidence': sum(confidences) / len(confidences),
    'decision_diversity': unique_decision_types / total_decision_types
}
```

## ğŸ¤ åä½œæœºåˆ¶è¯¦è§£

### 1. ä¿¡æ¯å…±äº«

```python
# LLMå†³ç­–ä¸­åŒ…å«MADDPGå‚è€ƒ
if maddpg_decisions:
    llm_decisions[role]['maddpg_reference'] = maddpg_decisions[role]['action_vector']
    llm_decisions[role]['reasoning'] += " [å‚è€ƒMADDPGå»ºè®®]"
```

### 2. ç»éªŒåé¦ˆ

```python
# LLMå†³ç­–ç»éªŒç”¨äºè®­ç»ƒMADDPG
experience = {
    'role': role,
    'state': observation,
    'action': llm_action_vector,  # LLMç”Ÿæˆçš„åŠ¨ä½œ
    'reward': computed_reward,
    'next_state': next_observation
}
self.experience_buffer.append(experience)
```

### 3. äº’è¡¥ä¼˜åŠ¿

| èƒ½åŠ›ç»´åº¦ | LLMä¼˜åŠ¿ | MADDPGä¼˜åŠ¿ |
|---------|---------|-----------|
| è¯­ä¹‰ç†è§£ | âœ… ä¼˜ç§€ | âŒ æœ‰é™ |
| æ•°å€¼ä¼˜åŒ– | âŒ ä¸€èˆ¬ | âœ… ä¼˜ç§€ |
| ä¸Šä¸‹æ–‡æ„ŸçŸ¥ | âœ… ä¼˜ç§€ | âŒ æœ‰é™ |
| ç»éªŒå­¦ä¹  | âŒ æœ‰é™ | âœ… ä¼˜ç§€ |
| åˆ›æ–°æ€ç»´ | âœ… ä¼˜ç§€ | âŒ æœ‰é™ |
| ç¨³å®šæ€§ | âŒ å¯å˜ | âœ… ç¨³å®š |

### 4. åŠ¨æ€å¹³è¡¡

```python
# æ ¹æ®ç³»ç»ŸçŠ¶æ€åŠ¨æ€è°ƒæ•´åä½œç­–ç•¥
if system_performance > threshold_high:
    # æ€§èƒ½è‰¯å¥½æ—¶ï¼Œæ›´å¤šä¾èµ–LLMåˆ›æ–°
    llm_weight = 0.9
    maddpg_weight = 0.1
elif system_performance < threshold_low:
    # æ€§èƒ½ä¸ä½³æ—¶ï¼Œæ›´å¤šä¾èµ–MADDPGç¨³å®šæ€§
    llm_weight = 0.7
    maddpg_weight = 0.3
else:
    # æ­£å¸¸æƒ…å†µä¸‹å‡è¡¡åä½œ
    llm_weight = 0.8
    maddpg_weight = 0.2
```

## ğŸ¯ ä¸Šä¸‹æ–‡æ„ŸçŸ¥æœºåˆ¶

### ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ„å»º

```python
context = {
    # åŸºç¡€ä¿¡æ¯
    'role': role,
    'observation': observation.tolist(),
    'step': self.current_step,
    'simulation_time': self.simulation_time,
    
    # ç³»ç»ŸçŠ¶æ€
    'system_state': current_state,
    'holy_code_state': holy_code_state,
    
    # å†å²ä¿¡æ¯
    'recent_performance': recent_metrics,
    'parliament_status': parliament_info,
    
    # åä½œä¿¡æ¯
    'maddpg_reference': maddpg_suggestions,
    'peer_decisions': other_agent_actions
}
```

### æ™ºèƒ½ä½“è§’è‰²æ˜ å°„

```python
role_mapping = {
    # æ³¨å†Œä¸­å¿ƒè§’è‰² â†’ æ§åˆ¶ç³»ç»Ÿè§’è‰²
    'doctors': 'doctor',
    'interns': 'intern', 
    'patients': 'patient',
    'accountants': 'accountant',
    'government': 'government'
}
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. è®¡ç®—æ•ˆç‡ä¼˜åŒ–

```python
# å¹¶è¡Œå†³ç­–ç”Ÿæˆ
with ThreadPoolExecutor(max_workers=5) as executor:
    futures = []
    for role, agent in agents.items():
        future = executor.submit(generate_decision, role, agent)
        futures.append(future)
    
    decisions = {}
    for future in as_completed(futures):
        role, decision = future.result()
        decisions[role] = decision
```

### 2. å†…å­˜ç®¡ç†ä¼˜åŒ–

```python
# æ™ºèƒ½ç¼“å­˜ç®¡ç†
if len(self.decision_cache) > max_cache_size:
    # æ¸…ç†æœ€æ—§çš„å†³ç­–ç¼“å­˜
    self.decision_cache = self.decision_cache[-max_cache_size//2:]

# ç»éªŒç¼“å†²åŒºä¼˜åŒ–
if len(self.experience_buffer) > self.config.maddpg_buffer_size:
    self.experience_buffer = self.experience_buffer[-self.config.maddpg_buffer_size//2:]
```

### 3. å¼‚æ­¥å¤„ç†

```python
# å¼‚æ­¥MADDPGè®­ç»ƒ
async def train_maddpg_async(self):
    try:
        await asyncio.get_event_loop().run_in_executor(
            None, self._train_maddpg_model
        )
    finally:
        self.is_training_maddpg = False
```

## ğŸ” æ•…éšœå¤„ç†å’Œå®¹é”™

### 1. LLMå†³ç­–å¤±è´¥å¤„ç†

```python
try:
    llm_response = agent.llm_generator.generate_action_sync(...)
    action_vector, reasoning = self._parse_llm_response(llm_response, role)
except Exception as e:
    logger.warning(f"âš ï¸ LLM+è§’è‰²æ™ºèƒ½ä½“ {role} å†³ç­–å¤±è´¥: {e}")
    # é™çº§åˆ°è§’è‰²æ™ºèƒ½ä½“é»˜è®¤å†³ç­–
    action = agent.sample_action(observation)
    action_vector = action.tolist()
    reasoning = f'{role}ä½¿ç”¨é»˜è®¤å†³ç­–ï¼ˆLLMå¤±è´¥ï¼‰'
```

### 2. MADDPGå†³ç­–å¤±è´¥å¤„ç†

```python
try:
    maddpg_actions = self.maddpg_model.get_actions(observations, training=False)
except Exception as e:
    logger.error(f"âŒ MADDPGè¡¥å……å†³ç­–å¤±è´¥: {e}")
    return None  # è¿”å›Noneï¼Œè®©LLMä¸»å¯¼
```

### 3. å®Œå…¨é™çº§æœºåˆ¶

```python
def _process_fallback_decisions(self):
    """å½“æ‰€æœ‰é«˜çº§å†³ç­–éƒ½å¤±è´¥æ—¶çš„é™çº§æœºåˆ¶"""
    fallback_actions = {}
    for role in self.agent_registry.get_all_agents().keys():
        # ä½¿ç”¨é¢„å®šä¹‰çš„å®‰å…¨åŠ¨ä½œ
        dim = self._get_action_dimension(role)
        fallback_actions[role] = {
            'action_vector': [0.1] * dim,  # ä¸­æ€§ä½é£é™©åŠ¨ä½œ
            'agent_type': 'Fallback',
            'confidence': 0.5,
            'reasoning': f'{role}ä½¿ç”¨å®‰å…¨é™çº§å†³ç­–'
        }
    return fallback_actions
```

## ğŸ“Š ç›‘æ§å’Œè°ƒè¯•

### å…³é”®æŒ‡æ ‡ç›‘æ§

```python
collaboration_metrics = {
    # å†³ç­–åˆ†å¸ƒ
    'llm_primary_rate': llm_primary_count / total_decisions,
    'maddpg_supplement_rate': maddpg_supplement_count / total_decisions,
    'collaboration_rate': collaboration_count / total_decisions,
    
    # æ€§èƒ½æŒ‡æ ‡
    'decision_latency': average_decision_time,
    'decision_quality': average_decision_confidence,
    'system_stability': stability_score,
    
    # å­¦ä¹ æ•ˆæœ
    'training_frequency': training_episodes / total_episodes,
    'model_improvement': performance_delta,
    'experience_quality': valid_experience_ratio
}
```

### è°ƒè¯•æ—¥å¿—

```python
logger.info(f"ğŸ¤– å†³ç­–åä½œçŠ¶æ€:")
logger.info(f"   LLMä¸»å¯¼: {llm_decisions is not None}")
logger.info(f"   MADDPGè¡¥å……: {maddpg_decisions is not None}")
logger.info(f"   åä½œæ¨¡å¼: {'èåˆ' if both_available else 'å•ä¸€'}")
logger.info(f"   å†³ç­–å»¶è¿Ÿ: {decision_latency:.3f}ç§’")
logger.info(f"   å¹³å‡ç½®ä¿¡åº¦: {average_confidence:.3f}")
```

## ğŸš€ ç³»ç»Ÿä¼˜åŠ¿æ€»ç»“

### 1. æ™ºèƒ½äº’è¡¥
- **LLM**: æä¾›è¯­ä¹‰ç†è§£ã€åˆ›æ–°æ€ç»´ã€å¤æ‚æ¨ç†
- **MADDPG**: æä¾›æ•°å€¼ä¼˜åŒ–ã€ç»éªŒå­¦ä¹ ã€ç¨³å®šå†³ç­–

### 2. åŠ¨æ€é€‚åº”
- æ ¹æ®ç³»ç»ŸçŠ¶æ€è‡ªåŠ¨è°ƒæ•´åä½œç­–ç•¥
- æ™ºèƒ½é™çº§ç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œ
- æŒç»­å­¦ä¹ æå‡å†³ç­–è´¨é‡

### 3. å¯æ‰©å±•æ€§
- æ¨¡å—åŒ–è®¾è®¡æ”¯æŒæ–°çš„å†³ç­–ç®—æ³•é›†æˆ
- æ ‡å‡†åŒ–æ¥å£ä¾¿äºç³»ç»Ÿæ‰©å±•
- çµæ´»é…ç½®æ»¡è¶³ä¸åŒåº”ç”¨éœ€æ±‚

### 4. å¯é æ€§
- å¤šå±‚å®¹é”™æœºåˆ¶ä¿è¯ç³»ç»Ÿç¨³å®š
- å¼‚å¸¸å¤„ç†ç¡®ä¿ä¼˜é›…é™çº§
- ç›‘æ§æœºåˆ¶åŠæ—¶å‘ç°é—®é¢˜

## ğŸ¯ åº”ç”¨åœºæ™¯

### 1. å¤æ‚å†³ç­–åœºæ™¯
å½“é¢ä¸´éœ€è¦è¯­ä¹‰ç†è§£å’Œåˆ›æ–°æ€ç»´çš„å¤æ‚å†³ç­–æ—¶ï¼ŒLLMä¸»å¯¼ï¼ŒMADDPGæä¾›æ•°å€¼å‚è€ƒã€‚

### 2. ç¨³å®šè¿è¥åœºæ™¯
å½“ç³»ç»Ÿéœ€è¦ç¨³å®šã€å¯é¢„æµ‹çš„å†³ç­–æ—¶ï¼ŒMADDPGä¸»å¯¼ï¼ŒLLMæä¾›åˆ›æ–°å»ºè®®ã€‚

### 3. å­¦ä¹ ä¼˜åŒ–åœºæ™¯
é€šè¿‡LLMå†³ç­–çš„å¤šæ ·æ€§ä¸ºMADDPGæä¾›ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ï¼Œå®ç°æŒç»­å­¦ä¹ ã€‚

### 4. å±æœºå¤„ç†åœºæ™¯
åœ¨ç³»ç»Ÿå¼‚å¸¸æˆ–å¤–éƒ¨å†²å‡»æ—¶ï¼Œåä½œæœºåˆ¶ç¡®ä¿ç¨³å®šçš„åº”æ€¥å“åº”ã€‚

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. é…ç½®ä¼˜åŒ–
- æ ¹æ®åº”ç”¨åœºæ™¯è°ƒæ•´LLMå’ŒMADDPGçš„æƒé‡
- åˆç†è®¾ç½®è®­ç»ƒé¢‘ç‡å’Œæ‰¹æ¬¡å¤§å°
- ä¼˜åŒ–ç¼“å†²åŒºå¤§å°å¹³è¡¡å†…å­˜å’Œæ€§èƒ½

### 2. ç›‘æ§ç®¡ç†
- æŒç»­ç›‘æ§å†³ç­–è´¨é‡å’Œç³»ç»Ÿæ€§èƒ½
- å®šæœŸè¯„ä¼°åä½œæ•ˆæœå’Œå­¦ä¹ è¿›åº¦
- åŠæ—¶è°ƒæ•´å‚æ•°ä¼˜åŒ–ç³»ç»Ÿè¡¨ç°

### 3. æ•…éšœé¢„é˜²
- è®¾ç½®åˆç†çš„é™çº§é˜ˆå€¼
- å»ºç«‹å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
- å®šæœŸå¤‡ä»½æ¨¡å‹å’Œå…³é”®æ•°æ®

### 4. æŒç»­æ”¹è¿›
- æ”¶é›†ç”¨æˆ·åé¦ˆä¼˜åŒ–å†³ç­–é€»è¾‘
- åˆ†æå†å²æ•°æ®å‘ç°æ”¹è¿›æœºä¼š
- è·Ÿè¸ªæŠ€æœ¯å‘å±•å‡çº§ç³»ç»Ÿç»„ä»¶

---

*æœ€åæ›´æ–°: 2025å¹´10æœˆ7æ—¥*
*æ–‡æ¡£ç‰ˆæœ¬: v1.0*
*ç»´æŠ¤è€…: Asclepionå¼€å‘å›¢é˜Ÿ*