#!/usr/bin/env python3
"""
åŒ»é™¢æ²»ç†ç³»ç»Ÿ - çœŸå®LLM APIé›†æˆç‰ˆæœ¬
æ”¯æŒOpenAI GPTã€Anthropic Claudeç­‰çœŸå®LLM API
"""

import numpy as np
import asyncio
import json
import time
import os
from typing import Dict, List, Any, Optional

class RealLLMProvider:
    """çœŸå®LLM APIæä¾›è€…"""
    
    def __init__(self, provider_type: str = 'openai', api_key: str = None, model: str = None):
        self.provider_type = provider_type
        self.api_key = api_key or os.getenv(f'{provider_type.upper()}_API_KEY')
        
        if provider_type == 'openai':
            self.model = model or 'gpt-4'
            self.base_url = "https://api.openai.com/v1"
        elif provider_type == 'anthropic':
            self.model = model or 'claude-3-sonnet-20240229'
            self.base_url = "https://api.anthropic.com/v1"
        else:
            raise ValueError(f"Unsupported provider: {provider_type}")
        
        self.request_count = 0
        self.total_tokens = 0
    
    async def generate_decision(self, role: str, observation: np.ndarray, 
                              constraints: Dict, context: str = "normal") -> np.ndarray:
        """ä½¿ç”¨çœŸå®LLMç”Ÿæˆå†³ç­–"""
        try:
            # æ„å»ºæç¤º
            prompt = self._build_decision_prompt(role, observation, constraints, context)
            
            # è°ƒç”¨LLM API
            response = await self._call_llm_api(prompt)
            
            # è§£æå“åº”ä¸ºæ•°å€¼å‘é‡
            action = self._parse_response_to_action(response, role)
            
            self.request_count += 1
            print(f"[LLM] {role} å†³ç­–: {action.tolist()[:3]}... (æ¥æº: {self.provider_type})")
            
            return action
            
        except Exception as e:
            print(f"[LLMé”™è¯¯] {role}: {e}")
            # å›é€€åˆ°ç®€å•å†³ç­–
            return self._fallback_decision(role, observation, constraints)
    
    def _build_decision_prompt(self, role: str, observation: np.ndarray, 
                             constraints: Dict, context: str) -> str:
        """æ„å»ºå†³ç­–æç¤º"""
        # è§‚æµ‹æ•°æ®æ¦‚è¿°
        obs_summary = f"è§‚æµ‹æ•°æ®: {observation[:5].tolist()}..." if len(observation) > 5 else f"è§‚æµ‹æ•°æ®: {observation.tolist()}"
        
        # çº¦æŸæ¦‚è¿°
        constraints_text = ", ".join([f"{k}={v}" for k, v in constraints.items()]) if constraints else "æ— ç‰¹æ®Šçº¦æŸ"
        
        # è§’è‰²ç‰¹å®šæç¤º
        role_prompts = {
            'doctors': f"""
ä½ æ˜¯åŒ»é™¢çš„ä¸»æ²»åŒ»ç”Ÿå›¢é˜Ÿä»£è¡¨ã€‚åŸºäºå½“å‰åŒ»ç–—ç³»ç»ŸçŠ¶æ€ï¼Œä½ éœ€è¦åšå‡ºå…³é”®å†³ç­–ã€‚

{obs_summary}
å½“å‰çº¦æŸ: {constraints_text}
æƒ…å¢ƒ: {context}

è¯·åŸºäºä»¥ä¸‹è€ƒè™‘å› ç´ åšå‡ºå†³ç­–:
1. æ‚£è€…ç”Ÿå‘½å®‰å…¨å’ŒåŒ»ç–—è´¨é‡ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
2. åŒ»ç–—èµ„æºåˆç†åˆ†é…
3. å·¥ä½œè´Ÿè·å¹³è¡¡
4. åº”æ€¥å“åº”èƒ½åŠ›

è¯·è¿”å›ä¸€ä¸ª4ç»´å†³ç­–å‘é‡ [è´¨é‡æ”¹è¿›åŠ›åº¦, èµ„æºç”³è¯·å¼ºåº¦, å·¥ä½œè´Ÿè·è°ƒæ•´, å®‰å…¨æªæ–½å¼ºåº¦]
æ¯ä¸ªå€¼åœ¨-1.0åˆ°1.0ä¹‹é—´ï¼Œä¾‹å¦‚: [0.6, 0.4, -0.2, 0.8]
""",
            'interns': f"""
ä½ æ˜¯å®ä¹ åŒ»ç”Ÿç¾¤ä½“çš„ä»£è¡¨ã€‚åŸºäºå½“å‰å­¦ä¹ å’Œå·¥ä½œç¯å¢ƒï¼Œä½ éœ€è¦è¡¨è¾¾éœ€æ±‚å’Œå»ºè®®ã€‚

{obs_summary}
å½“å‰çº¦æŸ: {constraints_text}
æƒ…å¢ƒ: {context}

è¯·åŸºäºä»¥ä¸‹è€ƒè™‘å› ç´ åšå‡ºå†³ç­–:
1. æ•™è‚²åŸ¹è®­è´¨é‡å’Œæœºä¼š
2. å·¥ä½œè´Ÿè·çš„åˆç†æ€§
3. èŒä¸šå‘å±•è·¯å¾„
4. å­¦ä¹ èµ„æºè·å–

è¯·è¿”å›ä¸€ä¸ª3ç»´å†³ç­–å‘é‡ [åŸ¹è®­éœ€æ±‚å¼ºåº¦, å·¥ä½œè´Ÿè·è°ƒæ•´, å‘å±•è®¡åˆ’ä¼˜å…ˆçº§]
æ¯ä¸ªå€¼åœ¨-1.0åˆ°1.0ä¹‹é—´ï¼Œä¾‹å¦‚: [0.7, -0.3, 0.5]
""",
            'patients': f"""
ä½ æ˜¯æ‚£è€…ç¾¤ä½“çš„ä»£è¡¨ã€‚åŸºäºå½“å‰åŒ»ç–—æœåŠ¡ä½“éªŒï¼Œä½ éœ€è¦è¡¨è¾¾å…³åˆ‡å’Œéœ€æ±‚ã€‚

{obs_summary}
å½“å‰çº¦æŸ: {constraints_text}
æƒ…å¢ƒ: {context}

è¯·åŸºäºä»¥ä¸‹è€ƒè™‘å› ç´ åšå‡ºå†³ç­–:
1. åŒ»ç–—æœåŠ¡è´¨é‡å’Œæ»¡æ„åº¦
2. åŒ»ç–—å¯åŠæ€§å’Œç­‰å¾…æ—¶é—´
3. æ‚£è€…å®‰å…¨å’Œæƒç›Šä¿æŠ¤
4. åŒ»ç–—è´¹ç”¨åˆç†æ€§

è¯·è¿”å›ä¸€ä¸ª3ç»´å†³ç­–å‘é‡ [æœåŠ¡æ”¹å–„éœ€æ±‚, å¯åŠæ€§ä¼˜åŒ–, å®‰å…¨å…³æ³¨åº¦]
æ¯ä¸ªå€¼åœ¨-1.0åˆ°1.0ä¹‹é—´ï¼Œä¾‹å¦‚: [0.5, 0.7, 0.4]
""",
            'accountants': f"""
ä½ æ˜¯åŒ»é™¢è´¢åŠ¡å›¢é˜Ÿçš„ä»£è¡¨ã€‚åŸºäºå½“å‰è´¢åŠ¡çŠ¶å†µï¼Œä½ éœ€è¦æå‡ºè´¢åŠ¡ç®¡ç†å»ºè®®ã€‚

{obs_summary}
å½“å‰çº¦æŸ: {constraints_text}
æƒ…å¢ƒ: {context}

è¯·åŸºäºä»¥ä¸‹è€ƒè™‘å› ç´ åšå‡ºå†³ç­–:
1. æˆæœ¬æ§åˆ¶å’Œè´¢åŠ¡å¥åº·
2. è¿è¥æ•ˆç‡ä¼˜åŒ–
3. é¢„ç®—åˆ†é…åˆç†æ€§
4. è´¢åŠ¡é€æ˜åº¦å’Œåˆè§„æ€§

è¯·è¿”å›ä¸€ä¸ª3ç»´å†³ç­–å‘é‡ [æˆæœ¬æ§åˆ¶åŠ›åº¦, æ•ˆç‡æå‡ä¼˜å…ˆçº§, é¢„ç®—ä¼˜åŒ–å¼ºåº¦]
æ¯ä¸ªå€¼åœ¨-1.0åˆ°1.0ä¹‹é—´ï¼Œä¾‹å¦‚: [0.6, 0.4, 0.3]
""",
            'government': f"""
ä½ æ˜¯æ”¿åºœç›‘ç®¡éƒ¨é—¨çš„ä»£è¡¨ã€‚åŸºäºå½“å‰åŒ»é™¢ç³»ç»ŸçŠ¶æ€ï¼Œä½ éœ€è¦åˆ¶å®šç›‘ç®¡å’Œæ”¿ç­–æªæ–½ã€‚

{obs_summary}
å½“å‰çº¦æŸ: {constraints_text}
æƒ…å¢ƒ: {context}

è¯·åŸºäºä»¥ä¸‹è€ƒè™‘å› ç´ åšå‡ºå†³ç­–:
1. ç³»ç»Ÿç¨³å®šæ€§å’Œå…¬ä¼—å®‰å…¨
2. ç›‘ç®¡åˆè§„å’Œæ”¿ç­–æ‰§è¡Œ
3. å…¬å¹³æ€§å’Œé€æ˜åº¦
4. åº”æ€¥åè°ƒå’Œå±æœºç®¡ç†

è¯·è¿”å›ä¸€ä¸ª3ç»´å†³ç­–å‘é‡ [ç›‘ç®¡ä»‹å…¥å¼ºåº¦, æ”¿ç­–è°ƒæ•´åŠ›åº¦, åè°ƒæªæ–½ä¼˜å…ˆçº§]
æ¯ä¸ªå€¼åœ¨-1.0åˆ°1.0ä¹‹é—´ï¼Œä¾‹å¦‚: [0.4, 0.2, 0.6]
"""
        }
        
        base_prompt = role_prompts.get(role, "è¯·åšå‡ºåˆç†å†³ç­–å¹¶è¿”å›æ•°å€¼å‘é‡ã€‚")
        
        return base_prompt + "\\n\\né‡è¦ï¼šè¯·ç›´æ¥è¿”å›æ•°å€¼å‘é‡ï¼Œæ ¼å¼å¦‚ [0.6, 0.4, -0.2, 0.8]"
    
    async def _call_llm_api(self, prompt: str) -> str:
        """è°ƒç”¨LLM API"""
        if self.provider_type == 'openai':
            return await self._call_openai_api(prompt)
        elif self.provider_type == 'anthropic':
            return await self._call_anthropic_api(prompt)
    
    async def _call_openai_api(self, prompt: str) -> str:
        """è°ƒç”¨OpenAI API"""
        import httpx
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯åŒ»é™¢æ²»ç†ç³»ç»Ÿçš„æ™ºèƒ½å†³ç­–åŠ©æ‰‹ã€‚è¯·åŸºäºç»™å®šä¿¡æ¯åšå‡ºæ•°å€¼åŒ–å†³ç­–ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 200,
            "temperature": 0.7
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            
            result = response.json()
            return result["choices"][0]["message"]["content"]
    
    async def _call_anthropic_api(self, prompt: str) -> str:
        """è°ƒç”¨Anthropic API"""
        import httpx
        
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": self.model,
            "max_tokens": 200,
            "temperature": 0.7,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.base_url}/messages",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            
            result = response.json()
            return result["content"][0]["text"]
    
    def _parse_response_to_action(self, response: str, role: str) -> np.ndarray:
        """è§£æLLMå“åº”ä¸ºè¡ŒåŠ¨å‘é‡"""
        try:
            import re
            
            # æŸ¥æ‰¾æ•°å€¼å‘é‡æ¨¡å¼
            patterns = [
                r'\\[([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?(?:,\\s*[-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?)*)\\]',
                r'([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?(?:,\\s*[-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?)*)',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, response)
                if match:
                    vector_str = match.group(1)
                    values = [float(x.strip()) for x in vector_str.split(',')]
                    action = np.array(values)
                    
                    # çº¦æŸåˆ°[-1, 1]èŒƒå›´
                    action = np.clip(action, -1.0, 1.0)
                    
                    # ç¡®ä¿æ­£ç¡®ç»´åº¦
                    expected_dims = {'doctors': 4, 'interns': 3, 'patients': 3, 'accountants': 3, 'government': 3}
                    expected_dim = expected_dims.get(role, 4)
                    
                    if len(action) != expected_dim:
                        if len(action) > expected_dim:
                            action = action[:expected_dim]
                        else:
                            padded_action = np.zeros(expected_dim)
                            padded_action[:len(action)] = action
                            action = padded_action
                    
                    return action
            
            # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨æ–‡æœ¬æ¨ç†
            return self._infer_from_text(response, role)
            
        except Exception as e:
            print(f"[è§£æé”™è¯¯] {role}: {e}")
            return self._fallback_decision(role, np.zeros(8), {})
    
    def _infer_from_text(self, text: str, role: str) -> np.ndarray:
        """ä»æ–‡æœ¬ä¸­æ¨æ–­å†³ç­–æ„å›¾"""
        text_lower = text.lower()
        
        # åŸºäºå…³é”®è¯æ¨æ–­å†³ç­–å¼ºåº¦
        action_dims = {'doctors': 4, 'interns': 3, 'patients': 3, 'accountants': 3, 'government': 3}
        action = np.zeros(action_dims.get(role, 4))
        
        # é€šç”¨å…³é”®è¯æ˜ å°„
        if 'ç´§æ€¥' in text or 'å±æœº' in text or 'ä¸¥é‡' in text:
            action[0] = 0.8
        elif 'æé«˜' in text or 'æ”¹å–„' in text or 'å¢å¼º' in text:
            action[0] = 0.6
        elif 'ç»´æŒ' in text or 'ä¿æŒ' in text:
            action[0] = 0.2
        elif 'å‡å°‘' in text or 'é™ä½' in text:
            action[0] = -0.3
        
        if len(action) > 1:
            if 'èµ„æº' in text or 'ç”³è¯·' in text:
                action[1] = 0.5
            if 'è°ƒæ•´' in text or 'ä¼˜åŒ–' in text:
                action[1] = 0.4
        
        if len(action) > 2:
            if 'å®‰å…¨' in text or 'è´¨é‡' in text:
                action[2] = 0.6
        
        return action
    
    def _fallback_decision(self, role: str, observation: np.ndarray, constraints: Dict) -> np.ndarray:
        """å›é€€å†³ç­–ï¼ˆå½“LLMè°ƒç”¨å¤±è´¥æ—¶ï¼‰"""
        action_dims = {'doctors': 4, 'interns': 3, 'patients': 3, 'accountants': 3, 'government': 3}
        dim = action_dims.get(role, 4)
        
        # åŸºäºè§‚æµ‹çš„ç®€å•å†³ç­–
        if len(observation) > 0:
            avg_obs = np.mean(observation)
            if avg_obs < 0.3:
                return np.full(dim, 0.6)  # ç§¯æå¹²é¢„
            elif avg_obs > 0.7:
                return np.full(dim, -0.2)  # é€‚åº¦è°ƒæ•´
            else:
                return np.full(dim, 0.1)  # è½»å¾®è°ƒæ•´
        else:
            return np.zeros(dim)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–APIä½¿ç”¨ç»Ÿè®¡"""
        return {
            'provider': self.provider_type,
            'model': self.model,
            'request_count': self.request_count,
            'total_tokens': self.total_tokens
        }

class AdvancedHospitalSimulation:
    """é«˜çº§åŒ»é™¢ä»¿çœŸç³»ç»Ÿ - é›†æˆçœŸå®LLM"""
    
    def __init__(self, llm_provider: str = 'mock', api_key: str = None, duration: int = 20):
        self.duration = duration
        self.llm_provider_type = llm_provider
        
        # åˆå§‹åŒ–LLMæä¾›è€…
        if llm_provider == 'mock':
            # ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬çš„æ¨¡æ‹Ÿå†³ç­–
            self.use_real_llm = False
            print("ğŸ¤– ä½¿ç”¨æ¨¡æ‹ŸLLMå†³ç­–")
        else:
            self.llm_provider = RealLLMProvider(llm_provider, api_key)
            self.use_real_llm = True
            print(f"ğŸ§  ä½¿ç”¨çœŸå®LLM: {llm_provider}")
        
        # ç³»ç»Ÿåˆå§‹åŒ–
        self.state = np.random.rand(16) * 0.5
        self.time_history = []
        self.state_history = []
        self.control_history = []
        self.rule_history = []
        self.llm_decision_history = []
        
        # ç³»ç»ŸåŠ¨åŠ›å­¦
        self.A = np.eye(16) + np.random.randn(16, 16) * 0.03
        self.B = np.random.randn(16, 17) * 0.1
        
        print(f"ğŸ“Š ç³»ç»Ÿåˆå§‹åŒ–: {len(self.state)}DçŠ¶æ€, ä»¿çœŸ{duration}æ­¥")
    
    async def run_simulation_async(self):
        """å¼‚æ­¥è¿è¡Œä»¿çœŸï¼ˆæ”¯æŒçœŸå®LLM APIè°ƒç”¨ï¼‰"""
        print("\\nğŸš€ å¼€å§‹å¼‚æ­¥ä»¿çœŸ...")
        start_time = time.time()
        
        for step in range(self.duration):
            current_time = step * 0.1
            self.time_history.append(current_time)
            
            # è¯„ä¼°è§„åˆ™
            holy_code_state = self._evaluate_rules(self.state)
            
            # å¤šæ™ºèƒ½ä½“LLMå†³ç­–
            if self.use_real_llm:
                control = await self._compute_llm_control_async(self.state, holy_code_state)
            else:
                control = self._compute_mock_control(self.state, holy_code_state)
            
            # ç³»ç»Ÿæ›´æ–°
            disturbance = np.random.normal(0, 0.05, 16)
            self.state = np.dot(self.A, self.state) + np.dot(self.B, control) + disturbance * 0.1
            self.state = np.clip(self.state, -2, 2)
            
            # è®°å½•æ•°æ®
            self.state_history.append(self.state.copy())
            self.control_history.append(control.copy())
            self.rule_history.append(holy_code_state.copy())
            
            # è¿›åº¦æ˜¾ç¤º
            if step % 5 == 0:
                rules_count = len(holy_code_state['active_rules'])
                health = 1.0 / (1.0 + np.linalg.norm(self.state))
                print(f"æ­¥éª¤ {step:2d}: è§„åˆ™={rules_count}, å¥åº·åº¦={health:.3f}")
        
        simulation_time = time.time() - start_time
        print(f"\\nâœ… å¼‚æ­¥ä»¿çœŸå®Œæˆï¼Œè€—æ—¶ {simulation_time:.2f} ç§’")
        
        if self.use_real_llm:
            stats = self.llm_provider.get_stats()
            print(f"ğŸ“ˆ LLMç»Ÿè®¡: {stats['request_count']} æ¬¡è°ƒç”¨")
        
        return self._analyze_results()
    
    async def _compute_llm_control_async(self, state: np.ndarray, holy_code_state: Dict) -> np.ndarray:
        """å¼‚æ­¥è®¡ç®—LLMæ§åˆ¶ä¿¡å·"""
        control = np.zeros(17)
        
        # è§‚æµ‹åˆ†é…
        observations = {
            'doctors': state,
            'interns': state[:12],
            'patients': state[4:12],
            'accountants': state[8:12],
            'government': state[[0,1,2,3,12,13,14,15]]
        }
        
        # æ§åˆ¶åˆ†é…
        control_slices = {
            'doctors': slice(0, 4),
            'interns': slice(4, 8),
            'patients': slice(8, 11),
            'accountants': slice(11, 14),
            'government': slice(14, 17)
        }
        
        # å¹¶è¡ŒLLMè°ƒç”¨
        tasks = []
        for role, obs in observations.items():
            context = holy_code_state.get('crisis_level', 'normal')
            task = self.llm_provider.generate_decision(
                role, obs, holy_code_state.get('ethical_constraints', {}), context
            )
            tasks.append((role, task))
        
        # ç­‰å¾…æ‰€æœ‰LLMå“åº”
        for role, task in tasks:
            action = await task
            control_slice = control_slices[role]
            slice_size = control_slice.stop - control_slice.start
            
            if len(action) >= slice_size:
                control[control_slice] = action[:slice_size]
            else:
                control[control_slice][:len(action)] = action
        
        return control
    
    def _compute_mock_control(self, state: np.ndarray, holy_code_state: Dict) -> np.ndarray:
        """è®¡ç®—æ¨¡æ‹Ÿæ§åˆ¶ä¿¡å·"""
        control = np.zeros(17)
        
        # ç®€åŒ–å†³ç­–é€»è¾‘
        if np.mean(state[:4]) < 0.3:  # å¥åº·å±æœº
            control[:4] = [0.7, 0.5, -0.2, 0.8]  # åŒ»ç”Ÿç§¯æå“åº”
        
        if np.mean(state[4:8]) < 0.4:  # èµ„æºä¸è¶³
            control[4:8] = [0.6, -0.3, 0.4, 0.2]  # å®ä¹ åŒ»ç”Ÿè°ƒæ•´
        
        control[8:11] = [0.3, 0.4, 0.5]  # æ‚£è€…åŸºç¡€éœ€æ±‚
        control[11:14] = [0.4, 0.5, 0.3]  # ä¼šè®¡æ§åˆ¶
        control[14:17] = [0.2, 0.1, 0.3]  # æ”¿åºœç›‘ç®¡
        
        return control
    
    def _evaluate_rules(self, state: np.ndarray) -> Dict[str, Any]:
        """è¯„ä¼°ç¥åœ£æ³•å…¸è§„åˆ™"""
        rules = []
        constraints = {}
        
        if np.mean(state[:4]) < 0.3:
            rules.append("HEALTH_CRISIS")
            constraints.update({'min_health_level': 0.6, 'emergency_response': True})
        
        if np.std(state) > 0.7:
            rules.append("SYSTEM_INSTABILITY")
            constraints.update({'stability_priority': True})
        
        if np.mean(state[8:12]) < 0.2:
            rules.append("FINANCIAL_CRISIS")
            constraints.update({'cost_control': True})
        
        return {
            'active_rules': rules,
            'ethical_constraints': constraints,
            'crisis_level': 'high' if len(rules) >= 2 else 'normal'
        }
    
    def _analyze_results(self):
        """åˆ†æç»“æœ"""
        print("\\nğŸ“Š ä»¿çœŸç»“æœåˆ†æ:")
        
        final_stability = np.linalg.norm(self.state_history[-1])
        avg_stability = np.mean([np.linalg.norm(s) for s in self.state_history])
        
        print(f"  æœ€ç»ˆç¨³å®šæ€§: {final_stability:.3f}")
        print(f"  å¹³å‡ç¨³å®šæ€§: {avg_stability:.3f}")
        
        # è§„åˆ™ç»Ÿè®¡
        total_rules = sum(len(r['active_rules']) for r in self.rule_history)
        print(f"  æ€»è§„åˆ™æ¿€æ´»: {total_rules} æ¬¡")
        
        return {
            'final_stability': final_stability,
            'average_stability': avg_stability,
            'total_rule_activations': total_rules,
            'llm_provider': self.llm_provider_type
        }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ åŒ»é™¢æ²»ç†ç³»ç»Ÿ - é«˜çº§LLMé›†æˆä»¿çœŸ")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    openai_key = os.getenv('OPENAI_API_KEY')
    anthropic_key = os.getenv('ANTHROPIC_API_KEY')
    
    print("APIå¯†é’¥çŠ¶æ€:")
    print(f"  OpenAI: {'âœ… å·²é…ç½®' if openai_key else 'âŒ æœªé…ç½®'}")
    print(f"  Anthropic: {'âœ… å·²é…ç½®' if anthropic_key else 'âŒ æœªé…ç½®'}")
    
    # é€‰æ‹©LLMæä¾›è€…
    if openai_key:
        provider = 'openai'
        api_key = openai_key
        print("\\nğŸ”§ ä½¿ç”¨ OpenAI GPT")
    elif anthropic_key:
        provider = 'anthropic'
        api_key = anthropic_key
        print("\\nğŸ”§ ä½¿ç”¨ Anthropic Claude")
    else:
        provider = 'mock'
        api_key = None
        print("\\nğŸ”§ ä½¿ç”¨æ¨¡æ‹ŸLLMï¼ˆæ— éœ€APIå¯†é’¥ï¼‰")
    
    # åˆ›å»ºå¹¶è¿è¡Œä»¿çœŸ
    simulation = AdvancedHospitalSimulation(provider, api_key, duration=10)
    
    # è¿è¡Œä»¿çœŸ
    if provider != 'mock':
        # å¼‚æ­¥è¿è¡ŒçœŸå®LLMä»¿çœŸ
        summary = asyncio.run(simulation.run_simulation_async())
    else:
        # åŒæ­¥è¿è¡Œæ¨¡æ‹Ÿä»¿çœŸ
        summary = asyncio.run(simulation.run_simulation_async())
    
    print("\\nğŸ‰ é«˜çº§ä»¿çœŸå®Œæˆï¼")
    print("=" * 60)
    print(f"LLMæä¾›è€…: {summary['llm_provider']}")
    print(f"ç³»ç»Ÿç¨³å®šæ€§: {summary['final_stability']:.3f}")
    print(f"è§„åˆ™æ¿€æ´»æ€»æ•°: {summary['total_rule_activations']}")

if __name__ == '__main__':
    main()