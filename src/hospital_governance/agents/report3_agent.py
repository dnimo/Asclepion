"""
Report 3 Agent - é›†æˆ Fixed LLM Actor + Semantic Critic

åŸºäº Report 3 æ¶æ„çš„å®Œæ•´å®ç°ï¼š
- ç»§æ‰¿ RoleAgent åŸºç±»
- ä½¿ç”¨ FixedLLMCandidateGenerator ç”Ÿæˆå€™é€‰åŠ¨ä½œï¼ˆå‚æ•°å†»ç»“ï¼‰
- ä½¿ç”¨ SemanticCritic è¯„ä¼°åŠ¨ä½œä»·å€¼
- é€šè¿‡ Bellman æ›´æ–°è®­ç»ƒ Critic
"""

import numpy as np
import torch
from typing import Dict, List, Any, Optional, Tuple
import logging

from .role_agents import RoleAgent, AgentConfig, AgentState, SystemState
from .learning_models import (
    FixedLLMCandidateGenerator,
    NaturalLanguageActionParser,
    LLM_PARAMETERS_FROZEN
)
from .semantic_critic import (
    SemanticEncoder,
    SemanticCritic,
    SemanticCriticTrainer,
    SemanticReplayBuffer,
    SemanticTransition,
    create_augmented_state
)

logger = logging.getLogger(__name__)


class Report3Agent(RoleAgent):
    """
    Report 3 æ¶æ„çš„æ™ºèƒ½ä½“å®ç°
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. Fixed LLM Actorï¼šç”Ÿæˆ K ä¸ªå€™é€‰åŠ¨ä½œï¼ˆå‚æ•°å†»ç»“ï¼‰
    2. Semantic Criticï¼šè¯„ä¼° Q(sÌƒ_t, a_t)ï¼Œå…¶ä¸­ sÌƒ_t = [Ï†(x_t), Î¾(HC_t)]
    3. Bellman è®­ç»ƒï¼šé€šè¿‡ç»éªŒå›æ”¾è®­ç»ƒ Critic
    4. Holy Code é›†æˆï¼šè¯­ä¹‰åµŒå…¥ä½œä¸ºçŠ¶æ€å¢å¼º
    """
    
    def __init__(
        self,
        config: AgentConfig,
        num_candidates: int = 5,
        hc_embedding_dim: int = 384,
        action_embedding_dim: int = 384,
        use_mock_llm: bool = True,
        replay_buffer_capacity: int = 10000,
        critic_lr: float = 3e-4,
        gamma: float = 0.99,
        device: str = 'cpu'
    ):
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(config)
        
        self.num_candidates = num_candidates
        self.device = device
        
        # 1. Fixed LLM å€™é€‰ç”Ÿæˆå™¨
        self.llm_generator = FixedLLMCandidateGenerator(
            num_candidates=num_candidates,
            use_mock=use_mock_llm
        )
        
        # 2. è‡ªç„¶è¯­è¨€åŠ¨ä½œè§£æå™¨
        self.action_parser = NaturalLanguageActionParser(
            action_dim=config.action_dim
        )
        
        # 3. è¯­ä¹‰ç¼–ç å™¨
        self.semantic_encoder = SemanticEncoder(
            llm_embedding_dim=hc_embedding_dim,
            use_mock=use_mock_llm
        )
        
        # 4. Semantic Critic ç½‘ç»œ
        self.critic = SemanticCritic(
            state_dim=16,  # å›ºå®šä½¿ç”¨16ç»´å…¨å±€çŠ¶æ€
            hc_embedding_dim=hc_embedding_dim,
            action_embedding_dim=action_embedding_dim
        )
        
        # 5. Critic è®­ç»ƒå™¨
        self.critic_trainer = SemanticCriticTrainer(
            critic=self.critic,
            encoder=self.semantic_encoder,
            lr=critic_lr,
            gamma=gamma,
            device=device
        )
        
        # 6. ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = SemanticReplayBuffer(capacity=replay_buffer_capacity)
        
        # ç»Ÿè®¡
        self.episode_count = 0
        self.training_steps = 0
        self.generation_count = 0
        
        logger.info(
            f"âœ“ Initialized Report3Agent ({config.role}): "
            f"LLM frozen={LLM_PARAMETERS_FROZEN}, K={num_candidates}, "
            f"critic_dim={16 + hc_embedding_dim + action_embedding_dim}"
        )
    
    def select_action(
        self,
        observation: np.ndarray,
        holy_code_guidance: Optional[Dict[str, Any]] = None,
        training: bool = False,
        exploration_epsilon: float = 0.0
    ) -> np.ndarray:
        """
        é€‰æ‹©åŠ¨ä½œï¼šLLM ç”Ÿæˆå€™é€‰ â†’ Critic è¯„ä¼° â†’ é€‰æ‹©æœ€ä¼˜
        
        Args:
            observation: å±€éƒ¨è§‚æµ‹ï¼ˆä¼šè¢«å…¨å±€çŠ¶æ€è¦†ç›–ï¼‰
            holy_code_guidance: Holy Code æŒ‡å¯¼
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼
            exploration_epsilon: æ¢ç´¢ç‡ï¼ˆ> 0 åˆ™éšæœºé€‰æ‹©ï¼‰
            
        Returns:
            action_vector: é€‰å®šçš„åŠ¨ä½œå‘é‡
        """
        # ä½¿ç”¨å…¨å±€16ç»´çŠ¶æ€ï¼ˆå¦‚æœå·²æ³¨å…¥ï¼‰
        if self._global_state_vector is not None and len(self._global_state_vector) == 16:
            system_state = self._global_state_vector
        else:
            # å›é€€ï¼šæ‰©å±•å±€éƒ¨è§‚æµ‹åˆ°16ç»´
            system_state = np.zeros(16)
            system_state[:min(len(observation), 16)] = observation[:16]
        
        # Holy Code çŠ¶æ€
        holy_code_state = holy_code_guidance or {
            'active_rules': ['Patient safety first', 'Resource optimization', 'Ethical compliance'],
            'priority_level': 0.8
        }
        
        # Step 1: LLM ç”Ÿæˆå€™é€‰åŠ¨ä½œï¼ˆå‚æ•°å†»ç»“ï¼‰
        llm_result = self.llm_generator.generate_candidates(
            role=self.role,
            state={'system_state': system_state.tolist()},
            holy_code=holy_code_state
        )
        
        self.generation_count += 1
        
        # Step 2: ç¼–ç  Holy Code
        hc_embedding = self.semantic_encoder.encode_holy_code(holy_code_state)
        
        # Step 3: æ„å»ºå¢å¼ºçŠ¶æ€ sÌƒ_t = [Ï†(x_t), Î¾(HC_t)]
        augmented_state = create_augmented_state(system_state, hc_embedding)
        
        # Step 4: æ¢ç´¢ vs åˆ©ç”¨
        if training and np.random.random() < exploration_epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©å€™é€‰
            selected_idx = np.random.randint(0, len(llm_result.candidates))
            selected_action = llm_result.candidates[selected_idx]
            q_value = 0.0
            logger.info(f"ğŸ² æ¢ç´¢æ¨¡å¼ï¼šéšæœºé€‰æ‹©å€™é€‰ {selected_idx}")
        else:
            # åˆ©ç”¨ï¼šCritic é€‰æ‹©æœ€ä¼˜å€™é€‰
            selected_action, q_value = self.critic_trainer.select_best_action(
                augmented_state=augmented_state,
                action_candidates=llm_result.candidates
            )
            selected_idx = llm_result.candidates.index(selected_action)
            logger.info(f"ğŸ¯ åˆ©ç”¨æ¨¡å¼ï¼šé€‰æ‹©æœ€ä¼˜å€™é€‰ (Q={q_value:.3f})")
        
        # Step 5: è§£æä¸ºåŠ¨ä½œå‘é‡
        action_vector = self.action_parser.parse(selected_action, role=self.role)
        
        # Step 6: ç¼“å­˜åŠ¨ä½œä¿¡æ¯ï¼ˆç”¨äºåç»­ç»éªŒå­˜å‚¨ï¼‰
        action_embedding = self.semantic_encoder.encode_action(selected_action)
        
        self._last_action_info = {
            'action_text': selected_action,
            'action_vector': action_vector,
            'action_embedding': action_embedding.detach().cpu().numpy(),
            'augmented_state': augmented_state,
            'q_value': q_value,
            'candidates': llm_result.candidates,
            'selected_idx': selected_idx,
            'generation_id': llm_result.metadata['generation_id']
        }
        
        return action_vector
    
    def store_transition(
        self,
        reward: float,
        next_observation: np.ndarray,
        next_holy_code_guidance: Optional[Dict[str, Any]] = None,
        done: bool = False
    ):
        """
        å­˜å‚¨è½¬æ¢åˆ°ç»éªŒå›æ”¾ç¼“å†²åŒº
        
        Args:
            reward: å¥–åŠ±
            next_observation: ä¸‹ä¸€çŠ¶æ€è§‚æµ‹
            next_holy_code_guidance: ä¸‹ä¸€çŠ¶æ€çš„ Holy Code
            done: æ˜¯å¦ç»ˆæ­¢
        """
        if not hasattr(self, '_last_action_info'):
            logger.warning("âš ï¸ No action info to store (call select_action first)")
            return
        
        # æ„å»ºä¸‹ä¸€çŠ¶æ€
        if self._global_state_vector is not None and len(self._global_state_vector) == 16:
            next_system_state = self._global_state_vector
        else:
            next_system_state = np.zeros(16)
            next_system_state[:min(len(next_observation), 16)] = next_observation[:16]
        
        # ä¸‹ä¸€ Holy Code çŠ¶æ€
        next_hc_state = next_holy_code_guidance or {
            'active_rules': ['Patient safety first'],
            'priority_level': 0.8
        }
        
        # ç¼–ç ä¸‹ä¸€ Holy Code
        next_hc_embedding = self.semantic_encoder.encode_holy_code(next_hc_state)
        
        # æ„å»ºä¸‹ä¸€å¢å¼ºçŠ¶æ€
        next_augmented_state = create_augmented_state(next_system_state, next_hc_embedding)
        
        # åˆ›å»ºè½¬æ¢
        transition = SemanticTransition(
            augmented_state=self._last_action_info['augmented_state'],
            action_text=self._last_action_info['action_text'],
            action_embedding=self._last_action_info['action_embedding'],
            reward=reward,
            next_augmented_state=next_augmented_state,
            done=done,
            role=self.role
        )
        
        # å­˜å‚¨
        self.replay_buffer.add(transition)
        
        logger.info(f"ğŸ’¾ å­˜å‚¨è½¬æ¢ï¼šreward={reward:.3f}, buffer_size={len(self.replay_buffer)}")
    
    def train_critic(
        self,
        batch_size: int = 32,
        num_epochs: int = 1
    ) -> Dict[str, float]:
        """
        è®­ç»ƒ Semantic Criticï¼ˆBellman æ›´æ–°ï¼‰
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            num_epochs: è®­ç»ƒè½®æ•°
            
        Returns:
            è®­ç»ƒç»Ÿè®¡
        """
        if len(self.replay_buffer) < batch_size:
            logger.info(f"â¸ï¸  ç»éªŒä¸è¶³ï¼š{len(self.replay_buffer)}/{batch_size}")
            return {}
        
        # å®šä¹‰å€™é€‰ç”Ÿæˆå‡½æ•°ï¼ˆç”¨äºè®¡ç®— Bellman ç›®æ ‡ï¼‰
        def next_candidates_fn(role: str, state: np.ndarray) -> List[str]:
            result = self.llm_generator.generate_candidates(
                role=role,
                state={'augmented_state': state[:16].tolist()},
                holy_code=None
            )
            return result.candidates
        
        # è®­ç»ƒå¾ªç¯
        total_stats = {}
        
        for epoch in range(num_epochs):
            batch = self.replay_buffer.sample(batch_size)
            stats = self.critic_trainer.train_step(batch, next_candidates_fn)
            
            # ç´¯åŠ ç»Ÿè®¡
            for k, v in stats.items():
                if k not in total_stats:
                    total_stats[k] = 0.0
                total_stats[k] += v
            
            self.training_steps += 1
            
            logger.info(
                f"ğŸ“Š è®­ç»ƒæ­¥éª¤ {self.training_steps}: "
                f"loss={stats['loss']:.4f}, mean_Q={stats['mean_q']:.3f}"
            )
        
        # å¹³å‡ç»Ÿè®¡
        for k in total_stats:
            total_stats[k] /= num_epochs
        
        return total_stats
    
    def observe(self, environment: Dict[str, Any]) -> np.ndarray:
        """å®ç°çˆ¶ç±»çš„æŠ½è±¡æ–¹æ³•ï¼šè§‚å¯Ÿç¯å¢ƒ"""
        # ä»ç¯å¢ƒä¸­æå–è§’è‰²ç›¸å…³çš„è§‚æµ‹
        obs = np.array([
            environment.get('medical_resource_utilization', 0.7),
            environment.get('patient_waiting_time', 0.3),
            environment.get('financial_indicator', 0.7),
            environment.get('ethical_compliance', 0.9),
            environment.get('education_training_quality', 0.8),
            environment.get('patient_satisfaction', 0.85),
            environment.get('operational_efficiency', 0.75),
            environment.get('crisis_response_capability', 0.8)
        ])
        return obs
    
    def compute_local_value(self, system_state: SystemState, action: int) -> float:
        """å®ç°çˆ¶ç±»çš„æŠ½è±¡æ–¹æ³•ï¼šè®¡ç®—å±€éƒ¨ä»·å€¼"""
        # ä½¿ç”¨ Critic ç½‘ç»œè¯„ä¼°ä»·å€¼
        # è¿™é‡Œéœ€è¦å°† SystemState è½¬æ¢ä¸ºå‘é‡
        state_vec = self._system_state_to_vector(system_state)
        
        # æ„å»ºå¢å¼ºçŠ¶æ€ï¼ˆä½¿ç”¨é»˜è®¤ Holy Codeï¼‰
        hc_embedding = self.semantic_encoder.encode_holy_code({
            'active_rules': ['Default rule'],
            'priority_level': 0.5
        })
        augmented_state = create_augmented_state(state_vec, hc_embedding)
        
        # ç”Ÿæˆå€™é€‰åŠ¨ä½œ
        candidates = self.llm_generator.generate_candidates(
            role=self.role,
            state={'system_state': state_vec.tolist()},
            holy_code=None
        )
        
        # è¯„ä¼°ç¬¬ä¸€ä¸ªå€™é€‰
        if candidates.candidates:
            _, q_value = self.critic_trainer.select_best_action(
                augmented_state=augmented_state,
                action_candidates=candidates.candidates[:1]
            )
            return q_value
        
        return 0.0
    
    def _apply_holy_code_recommendations(
        self,
        action: np.ndarray,
        recommendations: List[str]
    ) -> np.ndarray:
        """å®ç°çˆ¶ç±»çš„æŠ½è±¡æ–¹æ³•ï¼šåº”ç”¨ Holy Code æ¨è"""
        # Report 3 æ¶æ„ä¸­ï¼ŒHoly Code å·²ç»é€šè¿‡è¯­ä¹‰åµŒå…¥é›†æˆ
        # è¿™é‡Œä¿æŒåŠ¨ä½œä¸å˜
        return action
    
    def _system_state_to_vector(self, system_state: SystemState) -> np.ndarray:
        """å°† SystemState å¯¹è±¡è½¬æ¢ä¸º16ç»´å‘é‡"""
        return np.array([
            system_state.medical_resource_utilization,
            system_state.patient_waiting_time,
            system_state.financial_indicator,
            getattr(system_state, 'ethical_compliance', 0.9),
            getattr(system_state, 'education_training_quality', 0.8),
            getattr(system_state, 'intern_satisfaction', 0.7),
            getattr(system_state, 'patient_satisfaction', 0.85),
            getattr(system_state, 'service_accessibility', 0.8),
            getattr(system_state, 'care_quality_index', 0.9),
            getattr(system_state, 'safety_incident_rate', 0.05),
            getattr(system_state, 'operational_efficiency', 0.75),
            getattr(system_state, 'staff_workload_balance', 0.7),
            getattr(system_state, 'crisis_response_capability', 0.8),
            getattr(system_state, 'regulatory_compliance_score', 0.9),
            getattr(system_state, 'innovation_index', 0.7),
            getattr(system_state, 'sustainability_score', 0.8)
        ])
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'role': self.role,
            'episode_count': self.episode_count,
            'training_steps': self.training_steps,
            'generation_count': self.generation_count,
            'replay_buffer_size': len(self.replay_buffer),
            'parameters_frozen': LLM_PARAMETERS_FROZEN,
            'critic_stats': {
                'losses': self.critic_trainer.training_stats['losses'][-10:],
                'q_values': self.critic_trainer.training_stats['q_values'][-10:],
            }
        }


def create_report3_agent(role: str, **kwargs) -> Report3Agent:
    """
    åˆ›å»º Report3Agent çš„å·¥å‚å‡½æ•°
    
    Args:
        role: è§’è‰²åç§° ('doctors', 'interns', 'patients', etc.)
        **kwargs: é¢å¤–é…ç½®å‚æ•°
        
    Returns:
        Report3Agent å®ä¾‹
    """
    # è§’è‰²ç‰¹å®šé…ç½®
    role_configs = {
        'doctors': AgentConfig(role='doctors', action_dim=17, observation_dim=8),
        'interns': AgentConfig(role='interns', action_dim=17, observation_dim=8),
        'patients': AgentConfig(role='patients', action_dim=17, observation_dim=8),
        'accountants': AgentConfig(role='accountants', action_dim=17, observation_dim=8),
        'government': AgentConfig(role='government', action_dim=17, observation_dim=8)
    }
    
    config = role_configs.get(role, AgentConfig(role=role, action_dim=17, observation_dim=8))
    
    return Report3Agent(config, **kwargs)
