#!/usr/bin/env python3
"""
è§’è‰²æ™ºèƒ½ä½“é‡æ„ç‰ˆæœ¬ - åŸºäºæ•°ç†æ¨å¯¼çš„ä¸¥æ ¼å®ç°
å®ç°äº†å®Œæ•´çš„å¤šæ™ºèƒ½ä½“åŒ»é™¢æ²»ç†ç³»ç»Ÿæ•°å­¦æ¨¡å‹

åŸºäºä»¥ä¸‹æ•°ç†æ¨å¯¼ï¼š
1. å‚æ•°åŒ–éšæœºç­–ç•¥: Ï€_i(a_i | o_i; Î¸_i) = exp(Ï†_i(o_i, a_i)^T Î¸_i) / Î£ exp(...)
2. æ”¶ç›Šå‡½æ•°: R_i(x, a_i, a_{-i}) = Î±_i U(x) + Î²_i V_i(x, a_i) - Î³_i D_i(x, x*)
3. ç­–ç•¥æ¢¯åº¦æ›´æ–°: Î¸_i(t+1) = Î¸_i(t) + Î· âˆ‡_{Î¸_i} J_i(Î¸)
4. æé›…æ™®è¯ºå¤«ç¨³å®šæ€§åˆ†æ
5. ç¥åœ£æ³•å…¸åŠ¨æ€ç”Ÿæˆç†æƒ³çŠ¶æ€
"""

import numpy as np
import scipy.linalg as la
from typing import Dict, List, Any, Optional, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
import logging

from .role_agents_old import ParliamentMemberAgent

logger = logging.getLogger(__name__)

@dataclass
class AgentConfig:
    """æ™ºèƒ½ä½“é…ç½® - åŸºäºæ•°ç†æ¨å¯¼çš„ä¸¥æ ¼å‚æ•°åŒ–"""
    role: str
    action_dim: int
    observation_dim: int
    learning_rate: float = 0.001  # Î· in Î¸_i(t+1) = Î¸_i(t) + Î· âˆ‡J_i(Î¸)
    hidden_dims: List[int] = field(default_factory=lambda: [128, 64])
    
    # æ”¶ç›Šå‡½æ•°æƒé‡ R_i(x, a_i, a_{-i}) = Î±_i U(x) + Î²_i V_i(x, a_i) - Î³_i D_i(x, x*)
    alpha: float = 0.3  # å…¨å±€èµ„æºæ•ˆç”¨æƒé‡
    beta: float = 0.5   # å±€éƒ¨ä»·å€¼æƒé‡
    gamma: float = 0.2  # ç†æƒ³çŠ¶æ€åå·®æƒé‡
    
    # ç‰¹å¾æ˜ å°„ç»´åº¦
    feature_dim: int = None
    
    def __post_init__(self):
        if self.feature_dim is None:
            self.feature_dim = self.observation_dim + self.action_dim

@dataclass
class AgentState:
    """æ™ºèƒ½ä½“çŠ¶æ€ - åŸºäº16ç»´çŠ¶æ€ç©ºé—´çš„å±€éƒ¨è§‚æµ‹"""
    position: np.ndarray
    velocity: np.ndarray
    resources: Dict[str, float]
    beliefs: Dict[str, Any]
    goals: List[str]
    
    # ç­–ç•¥å‚æ•° Î¸_i
    policy_params: np.ndarray = None
    
    # æ€§èƒ½å†å²
    performance_score: float = 0.5
    cumulative_reward: float = 0.0
    
    # Qå€¼ä¼°è®¡ç¼“å­˜
    q_value_cache: Dict[str, float] = field(default_factory=dict)
    
    # ä¸Šæ¬¡è§‚æµ‹å’ŒåŠ¨ä½œ
    last_observation: np.ndarray = None
    last_action: int = None

@dataclass 
class SystemState:
    """ç³»ç»ŸçŠ¶æ€å‘é‡ x(t) âˆˆ â„^16 - æ‰©å±•çš„16ç»´çŠ¶æ€ç©ºé—´"""
    # æ ¸å¿ƒåŒ»ç–—æŒ‡æ ‡ (x_1 åˆ° x_4)
    medical_resource_utilization: float  # xâ‚: åŒ»ç–—èµ„æºåˆ©ç”¨ç‡
    patient_waiting_time: float         # xâ‚‚: æ‚£è€…ç­‰å¾…æ—¶é—´  
    financial_indicator: float          # xâ‚ƒ: è´¢åŠ¡å¥åº·æŒ‡æ ‡
    ethical_compliance: float           # xâ‚„: ä¼¦ç†åˆè§„åº¦
    
    # æ•™è‚²å’ŒåŸ¹è®­æŒ‡æ ‡ (x_5 åˆ° x_8)
    education_training_quality: float   # xâ‚…: æ•™è‚²åŸ¹è®­è´¨é‡
    intern_satisfaction: float          # xâ‚†: å®ä¹ ç”Ÿæ»¡æ„åº¦
    professional_development: float     # xâ‚‡: èŒä¸šå‘å±•æŒ‡æ•°
    mentorship_effectiveness: float     # xâ‚ˆ: æŒ‡å¯¼æ•ˆæœ
    
    # æ‚£è€…æœåŠ¡æŒ‡æ ‡ (x_9 åˆ° x_12)
    patient_satisfaction: float         # xâ‚‰: æ‚£è€…æ»¡æ„åº¦
    service_accessibility: float        # xâ‚â‚€: æœåŠ¡å¯åŠæ€§
    care_quality_index: float          # xâ‚â‚: æŠ¤ç†è´¨é‡æŒ‡æ•°
    safety_incident_rate: float        # xâ‚â‚‚: å®‰å…¨äº‹æ•…ç‡(åå‘)
    
    # ç³»ç»Ÿè¿è¥æŒ‡æ ‡ (x_13 åˆ° x_16)
    operational_efficiency: float       # xâ‚â‚ƒ: è¿è¥æ•ˆç‡
    staff_workload_balance: float      # xâ‚â‚„: å‘˜å·¥å·¥ä½œè´Ÿè·å¹³è¡¡
    crisis_response_capability: float   # xâ‚â‚…: å±æœºå“åº”èƒ½åŠ›
    regulatory_compliance_score: float  # xâ‚â‚†: ç›‘ç®¡åˆè§„åˆ†æ•°
    
    def to_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸º16ç»´çŠ¶æ€å‘é‡"""
        return np.array([
            self.medical_resource_utilization,
            self.patient_waiting_time,
            self.financial_indicator,
            self.ethical_compliance,
            self.education_training_quality,
            self.intern_satisfaction,
            self.professional_development,
            self.mentorship_effectiveness,
            self.patient_satisfaction,
            self.service_accessibility,
            self.care_quality_index,
            self.safety_incident_rate,
            self.operational_efficiency,
            self.staff_workload_balance,
            self.crisis_response_capability,
            self.regulatory_compliance_score
        ])
    
    @classmethod
    def from_vector(cls, x: np.ndarray) -> 'SystemState':
        """ä»16ç»´å‘é‡æ„é€ ç³»ç»ŸçŠ¶æ€"""
        return cls(
            medical_resource_utilization=x[0],
            patient_waiting_time=x[1],
            financial_indicator=x[2],
            ethical_compliance=x[3],
            education_training_quality=x[4],
            intern_satisfaction=x[5],
            professional_development=x[6],
            mentorship_effectiveness=x[7],
            patient_satisfaction=x[8],
            service_accessibility=x[9],
            care_quality_index=x[10],
            safety_incident_rate=x[11],
            operational_efficiency=x[12],
            staff_workload_balance=x[13],
            crisis_response_capability=x[14],
            regulatory_compliance_score=x[15]
        )

class RoleAgent(ABC):
    """è§’è‰²æ™ºèƒ½ä½“åŸºç±» - åŸºäºæ•°ç†æ¨å¯¼çš„ä¸¥æ ¼å®ç°
    
    å®ç°å‚æ•°åŒ–éšæœºç­–ç•¥: Ï€_i(a_i | o_i; Î¸_i) = exp(Ï†_i(o_i, a_i)^T Î¸_i) / Î£ exp(...)
    å’Œç­–ç•¥æ¢¯åº¦æ›´æ–°: Î¸_i(t+1) = Î¸_i(t) + Î· âˆ‡_{Î¸_i} J_i(Î¸)
    """
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.role = config.role
        self.state_dim = config.observation_dim
        self.action_dim = config.action_dim
        
        # ç­–ç•¥å‚æ•° Î¸_i âˆˆ â„^d
        self.theta = np.random.normal(0, 0.1, config.feature_dim)
        
        # æ”¶ç›Šå‡½æ•°æƒé‡ç³»æ•°
        self.alpha = config.alpha  # å…¨å±€æ•ˆç”¨æƒé‡
        self.beta = config.beta    # å±€éƒ¨ä»·å€¼æƒé‡  
        self.gamma = config.gamma  # ç†æƒ³çŠ¶æ€åå·®æƒé‡
        
        # æ™ºèƒ½ä½“çŠ¶æ€
        self.state = AgentState(
            position=np.zeros(2),
            velocity=np.zeros(2), 
            resources={},
            beliefs={},
            goals=[],
            policy_params=self.theta.copy()
        )
        
        # è¡Œä¸ºæ¨¡å‹å’Œå­¦ä¹ ç»„ä»¶
        self.behavior_model = None
        self.learning_model = None
        self.llm_generator = None
        
        # å†å²è®°å½•
        self.action_history = []
        self.state_history = []
        self.reward_history = []
        self.q_value_history = []  # Qå€¼å†å²
        
        # ç‰¹å¾æ˜ å°„ç¼“å­˜
        self._feature_cache = {}
        
        # åŸºçº¿å€¼ä¼°è®¡(ç”¨äºæ–¹å·®å‡å°‘)
        self.baseline = 0.0
        self.baseline_lr = 0.1
        
        logger.debug(f"Initialized {self.role} agent with Î¸ shape: {self.theta.shape}")
    
    def feature_mapping(self, observation: np.ndarray, action: int) -> np.ndarray:
        """ç‰¹å¾æ˜ å°„ Ï†_i(o_i, a_i): O_i Ã— A_i â†’ â„^d"""
        cache_key = f"{hash(observation.tobytes())}_{action}"
        if cache_key in self._feature_cache:
            return self._feature_cache[cache_key]
        
        # è§‚æµ‹ç‰¹å¾å½’ä¸€åŒ–
        obs_features = observation / (np.linalg.norm(observation) + 1e-8)
        
        # åŠ¨ä½œç‹¬çƒ­ç¼–ç 
        action_features = np.zeros(self.action_dim)
        if 0 <= action < self.action_dim:
            action_features[action] = 1.0
        
        # ç»„åˆç‰¹å¾
        features = np.concatenate([obs_features, action_features])
        
        # è°ƒæ•´åˆ°é…ç½®çš„ç‰¹å¾ç»´åº¦
        if len(features) > self.config.feature_dim:
            features = features[:self.config.feature_dim]
        elif len(features) < self.config.feature_dim:
            padding = np.zeros(self.config.feature_dim - len(features))
            features = np.concatenate([features, padding])
        
        self._feature_cache[cache_key] = features
        return features
    
    def compute_policy_probabilities(self, observation: np.ndarray) -> np.ndarray:
        """è®¡ç®—ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ Ï€_i(a_i | o_i; Î¸_i)"""
        logits = np.zeros(self.action_dim)
        
        for action in range(self.action_dim):
            phi = self.feature_mapping(observation, action)
            logits[action] = np.dot(phi, self.theta)
        
        # Softmax with numerical stability
        logits = logits - np.max(logits)
        exp_logits = np.exp(logits)
        probabilities = exp_logits / np.sum(exp_logits)
        
        return probabilities
    
    def sample_action(self, observation: np.ndarray) -> int:
        """ä»ç­–ç•¥åˆ†å¸ƒä¸­é‡‡æ ·åŠ¨ä½œ"""
        probabilities = self.compute_policy_probabilities(observation)
        action = np.random.choice(self.action_dim, p=probabilities)
        
        # æ›´æ–°çŠ¶æ€
        self.state.last_observation = observation.copy()
        self.state.last_action = action
        
        return action
    
    @abstractmethod
    def observe(self, environment: Dict[str, Any]) -> np.ndarray:
        """è§‚å¯Ÿç¯å¢ƒï¼Œè¿”å›å±€éƒ¨è§‚æµ‹ o_i"""
        pass
    
    @abstractmethod
    def compute_local_value(self, system_state: SystemState, action: int) -> float:
        """è®¡ç®—å±€éƒ¨ä»·å€¼å‡½æ•° V_i(x, a_i) - åŸºäºè§’è‰²ç‰¹å¼‚æ€§"""
        pass
    
    def compute_reward(self, system_state: SystemState, action: int, 
                      global_utility: float, ideal_state: SystemState) -> float:
        """è®¡ç®—æ”¶ç›Šå‡½æ•° R_i(x, a_i, a_{-i}) = Î±_i U(x) + Î²_i V_i(x, a_i) - Î³_i D_i(x, x*)"""
        
        # å±€éƒ¨ä»·å€¼å‡½æ•° V_i
        local_value = self.compute_local_value(system_state, action)
        
        # åˆ°ç†æƒ³çŠ¶æ€çš„åå·® D_i
        state_vec = system_state.to_vector()
        ideal_vec = ideal_state.to_vector()
        deviation = np.linalg.norm(state_vec - ideal_vec)
        
        # ç»„åˆæ”¶ç›Š
        reward = (self.alpha * global_utility + 
                 self.beta * local_value - 
                 self.gamma * deviation)
        
        return reward
    
    def update_policy(self, observation: np.ndarray, action: int, 
                     q_value: float, next_observation: Optional[np.ndarray] = None):
        """ç­–ç•¥æ¢¯åº¦æ›´æ–° Î¸_i(t+1) = Î¸_i(t) + Î· âˆ‡_{Î¸_i} J_i(Î¸)"""
        
        # æ›´æ–°åŸºçº¿ä¼°è®¡
        self.baseline = (1 - self.baseline_lr) * self.baseline + self.baseline_lr * q_value
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        advantage = q_value - self.baseline
        
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦ âˆ‡log Ï€_i(a_i|o_i)
        probabilities = self.compute_policy_probabilities(observation)
        
        # å½“å‰åŠ¨ä½œçš„ç‰¹å¾
        phi_current = self.feature_mapping(observation, action)
        
        # æœŸæœ›ç‰¹å¾ï¼ˆæ‰€æœ‰åŠ¨ä½œçš„åŠ æƒå¹³å‡ï¼‰
        phi_expected = np.zeros_like(phi_current)
        for a in range(self.action_dim):
            phi_a = self.feature_mapping(observation, a)
            phi_expected += probabilities[a] * phi_a
        
        # ç­–ç•¥æ¢¯åº¦
        grad_log_pi = phi_current - phi_expected
        
        # å‚æ•°æ›´æ–°
        self.theta += self.config.learning_rate * advantage * grad_log_pi
        
        # æ›´æ–°æ€§èƒ½åˆ†æ•°
        self.state.performance_score = 0.9 * self.state.performance_score + 0.1 * q_value
        self.state.cumulative_reward += q_value
        
        # è®°å½•å†å²
        self.reward_history.append(q_value)
        self.q_value_history.append(q_value)
        
        logger.debug(f"{self.role} policy updated: advantage={advantage:.3f}, ||grad||={np.linalg.norm(grad_log_pi):.3f}")
    
    def get_performance_metrics(self) -> Dict[str, float]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        if not self.reward_history:
            return {'performance_score': self.state.performance_score}
        
        recent_rewards = self.reward_history[-100:]  # æœ€è¿‘100æ­¥
        return {
            'performance_score': self.state.performance_score,
            'mean_reward': np.mean(recent_rewards),
            'std_reward': np.std(recent_rewards),
            'cumulative_reward': self.state.cumulative_reward,
            'policy_norm': np.linalg.norm(self.theta),
            'baseline_value': self.baseline,
            'total_actions': len(self.action_history)
        }
    
    # ç»§æ‰¿ç±»éœ€è¦å®ç°çš„æŠ½è±¡æ–¹æ³•ä¿æŒåŸæœ‰æ¥å£
    def select_action(self, observation: np.ndarray, 
                     holy_code_guidance: Optional[Dict[str, Any]] = None,
                     training: bool = False) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ - å…¼å®¹åŸæœ‰æ¥å£"""
        discrete_action = self.sample_action(observation)
        
        # è½¬æ¢ä¸ºè¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆå¦‚æœéœ€è¦ï¼‰
        continuous_action = np.zeros(self.action_dim)
        continuous_action[discrete_action] = 1.0
        
        # åº”ç”¨Holy CodeæŒ‡å¯¼
        if holy_code_guidance:
            priority_boost = holy_code_guidance.get('priority_boost', 1.0)
            continuous_action *= priority_boost
            
            # åº”ç”¨è§„åˆ™å»ºè®®
            recommendations = holy_code_guidance.get('rule_recommendations', [])
            continuous_action = self._apply_holy_code_recommendations(
                continuous_action, recommendations
            )
        
        return continuous_action
    
    @abstractmethod
    def _apply_holy_code_recommendations(self, action: np.ndarray, 
                                       recommendations: List[str]) -> np.ndarray:
        """åº”ç”¨ç¥åœ£æ³•å…¸å»ºè®® - å„è§’è‰²å®ç°ä¸åŒé€»è¾‘"""
        pass
    
    def add_experience(self, state: np.ndarray, action: np.ndarray,
                      reward: float, next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒåˆ°å†å²"""
        experience = {
            'role': self.role,
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done
        }
        
        self.action_history.append(action)
        self.state_history.append(state)
        self.reward_history.append(reward)
        
        return experience

class DoctorAgent(RoleAgent):
    """åŒ»ç”Ÿæ™ºèƒ½ä½“ - å…³æ³¨åŒ»ç–—è´¨é‡å’Œæ‚£è€…å®‰å…¨"""
    
    def observe(self, environment: Dict[str, Any]) -> np.ndarray:
        """è§‚å¯Ÿç¯å¢ƒï¼Œå…³æ³¨åŒ»ç–—è´¨é‡ç›¸å…³æŒ‡æ ‡"""
        observation = np.zeros(self.state_dim)
        
        # æ ¸å¿ƒåŒ»ç–—æŒ‡æ ‡ (ç´¢å¼• 0-3)
        observation[0] = environment.get('medical_resource_utilization', 0.7)
        observation[1] = environment.get('patient_waiting_time', 0.3)
        observation[2] = environment.get('care_quality_index', 0.8)
        observation[3] = environment.get('safety_incident_rate', 0.1)
        
        # æ‚£è€…æœåŠ¡æŒ‡æ ‡ (ç´¢å¼• 4-7)
        observation[4] = environment.get('patient_satisfaction', 0.8)
        observation[5] = environment.get('service_accessibility', 0.7)
        observation[6] = environment.get('ethical_compliance', 0.9)
        observation[7] = environment.get('crisis_response_capability', 0.8)
        
        return observation
    
    def compute_local_value(self, system_state: SystemState, action: int) -> float:
        """åŒ»ç”Ÿçš„å±€éƒ¨ä»·å€¼å‡½æ•° - é‡è§†åŒ»ç–—è´¨é‡å’Œæ‚£è€…å®‰å…¨"""
        return (0.3 * system_state.medical_resource_utilization +
                0.25 * system_state.care_quality_index +
                0.25 * system_state.patient_satisfaction +
                0.2 * (1.0 - system_state.safety_incident_rate))  # å®‰å…¨äº‹æ•…ç‡è¶Šä½è¶Šå¥½
    
    def _apply_holy_code_recommendations(self, action: np.ndarray, 
                                       recommendations: List[str]) -> np.ndarray:
        """åº”ç”¨åŒ»ç”Ÿç›¸å…³çš„ç¥åœ£æ³•å…¸å»ºè®®"""
        for rec in recommendations:
            if 'åŒ»ç–—è´¨é‡' in rec or 'æ‚£è€…å®‰å…¨' in rec:
                action[0] = max(action[0], 0.8)  # å¢å¼ºåŒ»ç–—å†³ç­–æƒé‡
            elif 'èµ„æºé…ç½®' in rec:
                action[1] = max(action[1], 0.7)
            elif 'ç´§æ€¥å“åº”' in rec:
                action[2] = max(action[2], 0.9)
        
        return np.clip(action, 0, 1)

class InternAgent(RoleAgent):
    """å®ä¹ åŒ»ç”Ÿæ™ºèƒ½ä½“ - å…³æ³¨æ•™è‚²åŸ¹è®­å’ŒèŒä¸šå‘å±•"""
    
    def observe(self, environment: Dict[str, Any]) -> np.ndarray:
        """è§‚å¯Ÿç¯å¢ƒï¼Œå…³æ³¨æ•™è‚²å’Œå‘å±•æŒ‡æ ‡"""
        observation = np.zeros(self.state_dim)
        
        # æ•™è‚²åŸ¹è®­æŒ‡æ ‡ (ç´¢å¼• 0-3)
        observation[0] = environment.get('education_training_quality', 0.7)
        observation[1] = environment.get('intern_satisfaction', 0.6)
        observation[2] = environment.get('professional_development', 0.5)
        observation[3] = environment.get('mentorship_effectiveness', 0.7)
        
        # å·¥ä½œç¯å¢ƒæŒ‡æ ‡ (ç´¢å¼• 4-7)
        observation[4] = environment.get('staff_workload_balance', 0.6)
        observation[5] = environment.get('medical_resource_utilization', 0.7)
        observation[6] = environment.get('operational_efficiency', 0.7)
        observation[7] = environment.get('ethical_compliance', 0.9)
        
        return observation
    
    def compute_local_value(self, system_state: SystemState, action: int) -> float:
        """å®ä¹ åŒ»ç”Ÿçš„å±€éƒ¨ä»·å€¼å‡½æ•° - é‡è§†æ•™è‚²å’Œå‘å±•æœºä¼š"""
        return (0.35 * system_state.education_training_quality +
                0.25 * system_state.intern_satisfaction +
                0.2 * system_state.professional_development +
                0.2 * system_state.staff_workload_balance)
    
    def _apply_holy_code_recommendations(self, action: np.ndarray, 
                                       recommendations: List[str]) -> np.ndarray:
        """åº”ç”¨å®ä¹ åŒ»ç”Ÿç›¸å…³çš„ç¥åœ£æ³•å…¸å»ºè®®"""
        for rec in recommendations:
            if 'æ•™è‚²åŸ¹è®­' in rec or 'åŸ¹è®­è¯·æ±‚' in rec:
                action[0] = max(action[0], 0.8)
            elif 'å·¥ä½œè´Ÿè·' in rec or 'å·¥ä½œè´Ÿè·è°ƒæ•´' in rec:
                action[1] = max(action[1], 0.7)
            elif 'èŒä¸šå‘å±•' in rec:
                action[2] = max(action[2], 0.6)
        
        return np.clip(action, 0, 1)

class PatientAgent(RoleAgent):
    """æ‚£è€…ä»£è¡¨æ™ºèƒ½ä½“ - å…³æ³¨æ‚£è€…æƒç›Šå’ŒæœåŠ¡è´¨é‡"""
    
    def observe(self, environment: Dict[str, Any]) -> np.ndarray:
        """è§‚å¯Ÿç¯å¢ƒï¼Œå…³æ³¨æ‚£è€…æœåŠ¡è´¨é‡"""
        observation = np.zeros(self.state_dim)
        
        # æ‚£è€…æœåŠ¡æŒ‡æ ‡ (ç´¢å¼• 0-3)
        observation[0] = environment.get('patient_satisfaction', 0.8)
        observation[1] = environment.get('service_accessibility', 0.7)
        observation[2] = environment.get('care_quality_index', 0.8)
        observation[3] = environment.get('patient_waiting_time', 0.3)
        
        # ç³»ç»ŸæœåŠ¡è´¨é‡ (ç´¢å¼• 4-7)
        observation[4] = environment.get('safety_incident_rate', 0.1)
        observation[5] = environment.get('ethical_compliance', 0.9)
        observation[6] = environment.get('operational_efficiency', 0.7)
        observation[7] = environment.get('crisis_response_capability', 0.8)
        
        return observation
    
    def compute_local_value(self, system_state: SystemState, action: int) -> float:
        """æ‚£è€…ä»£è¡¨çš„å±€éƒ¨ä»·å€¼å‡½æ•° - é‡è§†æ‚£è€…ä½“éªŒå’Œå®‰å…¨"""
        return (0.3 * system_state.patient_satisfaction +
                0.25 * system_state.service_accessibility +
                0.25 * system_state.care_quality_index +
                0.2 * (1.0 - system_state.safety_incident_rate))
    
    def _apply_holy_code_recommendations(self, action: np.ndarray, 
                                       recommendations: List[str]) -> np.ndarray:
        """åº”ç”¨æ‚£è€…ç›¸å…³çš„ç¥åœ£æ³•å…¸å»ºè®®"""
        for rec in recommendations:
            if 'æ‚£è€…æ»¡æ„åº¦' in rec or 'æ»¡æ„åº¦æ”¹è¿›' in rec:
                action[0] = max(action[0], 0.8)
            elif 'æœåŠ¡å¯åŠæ€§' in rec or 'å¯åŠæ€§æ”¹å–„' in rec:
                action[1] = max(action[1], 0.7)
            elif 'ç­‰å¾…æ—¶é—´' in rec or 'ç­‰å¾…æ—¶é—´ä¼˜åŒ–' in rec:
                action[2] = max(action[2], 0.6)
        
        return np.clip(action, 0, 1)

class AccountantAgent(RoleAgent):
    """ä¼šè®¡æ™ºèƒ½ä½“ - å…³æ³¨è´¢åŠ¡å¥åº·å’Œè¿è¥æ•ˆç‡"""
    
    def observe(self, environment: Dict[str, Any]) -> np.ndarray:
        """è§‚å¯Ÿç¯å¢ƒï¼Œå…³æ³¨è´¢åŠ¡å’Œè¿è¥æŒ‡æ ‡"""
        observation = np.zeros(self.state_dim)
        
        # è´¢åŠ¡æŒ‡æ ‡ (ç´¢å¼• 0-3)
        observation[0] = environment.get('financial_indicator', 0.7)
        observation[1] = environment.get('operational_efficiency', 0.7)
        observation[2] = environment.get('medical_resource_utilization', 0.7)
        observation[3] = environment.get('staff_workload_balance', 0.6)
        
        # ç³»ç»Ÿæ•ˆç‡æŒ‡æ ‡ (ç´¢å¼• 4-7)
        observation[4] = environment.get('patient_waiting_time', 0.3)
        observation[5] = environment.get('service_accessibility', 0.7)
        observation[6] = environment.get('regulatory_compliance_score', 0.8)
        observation[7] = environment.get('crisis_response_capability', 0.8)
        
        return observation
    
    def compute_local_value(self, system_state: SystemState, action: int) -> float:
        """ä¼šè®¡çš„å±€éƒ¨ä»·å€¼å‡½æ•° - é‡è§†è´¢åŠ¡å¥åº·å’Œæ•ˆç‡"""
        return (0.4 * system_state.financial_indicator +
                0.3 * system_state.operational_efficiency +
                0.2 * system_state.medical_resource_utilization +
                0.1 * system_state.regulatory_compliance_score)
    
    def _apply_holy_code_recommendations(self, action: np.ndarray, 
                                       recommendations: List[str]) -> np.ndarray:
        """åº”ç”¨ä¼šè®¡ç›¸å…³çš„ç¥åœ£æ³•å…¸å»ºè®®"""
        for rec in recommendations:
            if 'æˆæœ¬æ§åˆ¶' in rec or 'è´¢åŠ¡ä¼˜åŒ–' in rec:
                action[0] = max(action[0], 0.8)
            elif 'èµ„æºé…ç½®' in rec or 'èµ„æºä¼˜åŒ–' in rec:
                action[1] = max(action[1], 0.7)
            elif 'æ•ˆç‡æå‡' in rec or 'è¿è¥æ•ˆç‡' in rec:
                action[2] = max(action[2], 0.7)
        
        return np.clip(action, 0, 1)

class GovernmentAgent(RoleAgent):
    """æ”¿åºœä»£ç†æ™ºèƒ½ä½“ - å…³æ³¨ç›‘ç®¡åˆè§„å’Œå…¬å…±åˆ©ç›Š"""
    
    def observe(self, environment: Dict[str, Any]) -> np.ndarray:
        """è§‚å¯Ÿç¯å¢ƒï¼Œå…³æ³¨ç›‘ç®¡å’Œç³»ç»Ÿæ•´ä½“çŠ¶æ€"""
        observation = np.zeros(self.state_dim)
        
        # ç›‘ç®¡åˆè§„æŒ‡æ ‡ (ç´¢å¼• 0-3)
        observation[0] = environment.get('regulatory_compliance_score', 0.8)
        observation[1] = environment.get('ethical_compliance', 0.9)
        observation[2] = environment.get('crisis_response_capability', 0.8)
        observation[3] = environment.get('operational_efficiency', 0.7)
        
        # å…¬å…±åˆ©ç›ŠæŒ‡æ ‡ (ç´¢å¼• 4-7)
        observation[4] = environment.get('patient_satisfaction', 0.8)
        observation[5] = environment.get('service_accessibility', 0.7)
        observation[6] = environment.get('safety_incident_rate', 0.1)
        observation[7] = environment.get('financial_indicator', 0.7)
        
        return observation
    
    def compute_local_value(self, system_state: SystemState, action: int) -> float:
        """æ”¿åºœä»£ç†çš„å±€éƒ¨ä»·å€¼å‡½æ•° - é‡è§†æ•´ä½“ç³»ç»Ÿç¨³å®šå’Œå…¬å…±åˆ©ç›Š"""
        return (0.25 * system_state.regulatory_compliance_score +
                0.25 * system_state.ethical_compliance +
                0.2 * system_state.crisis_response_capability +
                0.15 * system_state.patient_satisfaction +
                0.15 * system_state.service_accessibility)
    
    def _apply_holy_code_recommendations(self, action: np.ndarray, 
                                       recommendations: List[str]) -> np.ndarray:
        """åº”ç”¨æ”¿åºœç›¸å…³çš„ç¥åœ£æ³•å…¸å»ºè®®"""
        for rec in recommendations:
            if 'ç›‘ç®¡åˆè§„' in rec or 'åˆè§„æ£€æŸ¥' in rec:
                action[0] = max(action[0], 0.9)
            elif 'ç³»ç»Ÿç¨³å®š' in rec or 'ç¨³å®šæªæ–½' in rec:
                action[1] = max(action[1], 0.8)
            elif 'å…¬å…±åˆ©ç›Š' in rec or 'é€æ˜åº¦æå‡' in rec:
                action[2] = max(action[2], 0.7)
            elif 'å±æœºå“åº”' in rec:
                action[3] = max(action[3], 0.9)
        
        return np.clip(action, 0, 1)

class RoleManager:
    """è§’è‰²ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ™ºèƒ½ä½“"""
    
    def __init__(self):
        self.agents: Dict[str, RoleAgent] = {}
        self.agent_configs: Dict[str, AgentConfig] = {}
        self._setup_default_configs()
    
    def _setup_default_configs(self):
        """è®¾ç½®é»˜è®¤é…ç½®"""
        roles = ['doctors', 'interns', 'patients', 'accountants', 'government']
        
        for role in roles:
            config = AgentConfig(
                role=role,
                action_dim=5,  # 5ä¸ªç¦»æ•£åŠ¨ä½œ
                observation_dim=8,  # 8ç»´å±€éƒ¨è§‚æµ‹
                learning_rate=0.001,
                alpha=0.3, beta=0.5, gamma=0.2  # æ”¶ç›Šå‡½æ•°æƒé‡
            )
            self.agent_configs[role] = config
    
    def create_all_agents(self, custom_configs: Optional[Dict[str, AgentConfig]] = None):
        """åˆ›å»ºæ‰€æœ‰æ™ºèƒ½ä½“"""
        if custom_configs:
            self.agent_configs.update(custom_configs)
        
        agent_classes = {
            'doctors': DoctorAgent,
            'interns': InternAgent,
            'patients': PatientAgent,
            'accountants': AccountantAgent,
            'government': GovernmentAgent  # æ–°å¢æ”¿åºœä»£ç†
        }
        
        for role, agent_class in agent_classes.items():
            config = self.agent_configs[role]
            agent = agent_class(config)
            self.agents[role] = agent
            logger.info(f"Created {role} agent with config: {config}")
    
    def get_agent(self, role: str) -> Optional[RoleAgent]:
        """è·å–æŒ‡å®šè§’è‰²çš„æ™ºèƒ½ä½“"""
        return self.agents.get(role)
    
    def get_all_agents(self) -> List[RoleAgent]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“"""
        return list(self.agents.values())
    
    def get_agent_count(self) -> int:
        """è·å–æ™ºèƒ½ä½“æ•°é‡"""
        return len(self.agents)
    
    def update_all_policies(self, observations: Dict[str, np.ndarray], 
                           actions: Dict[str, int], q_values: Dict[str, float]):
        """æ‰¹é‡æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“ç­–ç•¥"""
        for role, agent in self.agents.items():
            if role in observations and role in actions and role in q_values:
                agent.update_policy(observations[role], actions[role], q_values[role])
    
    def get_performance_summary(self) -> Dict[str, Dict[str, float]]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„æ€§èƒ½æ‘˜è¦"""
        summary = {}
        for role, agent in self.agents.items():
            summary[role] = agent.get_performance_metrics()
        return summary

def create_default_agent_system() -> RoleManager:
    """åˆ›å»ºé»˜è®¤çš„æ™ºèƒ½ä½“ç³»ç»Ÿ"""
    manager = RoleManager()
    manager.create_all_agents()
    
    logger.info(f"Created agent system with {manager.get_agent_count()} agents:")
    for role in manager.agents.keys():
        logger.info(f"  - {role}")
    
    return manager

# æµ‹è¯•éªŒè¯å‡½æ•°
def test_agent_system():
    """æµ‹è¯•æ™ºèƒ½ä½“ç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = create_default_agent_system()
    
    # æ¨¡æ‹Ÿç¯å¢ƒçŠ¶æ€
    env_state = {
        'medical_resource_utilization': 0.8,
        'patient_waiting_time': 0.3,
        'financial_indicator': 0.7,
        'ethical_compliance': 0.9,
        'education_training_quality': 0.8,
        'intern_satisfaction': 0.7,
        'patient_satisfaction': 0.85,
        'service_accessibility': 0.8,
        'care_quality_index': 0.9,
        'safety_incident_rate': 0.05,
        'operational_efficiency': 0.75,
        'staff_workload_balance': 0.7,
        'crisis_response_capability': 0.8,
        'regulatory_compliance_score': 0.9
    }
    
    # æµ‹è¯•æ¯ä¸ªæ™ºèƒ½ä½“
    for role, agent in manager.agents.items():
        print(f"\næµ‹è¯• {role} æ™ºèƒ½ä½“:")
        
        # è§‚æµ‹
        observation = agent.observe(env_state)
        print(f"  è§‚æµ‹ç»´åº¦: {observation.shape}")
        print(f"  è§‚æµ‹æ ·æœ¬: {observation[:4]}")  # æ˜¾ç¤ºå‰4ä¸ªå€¼
        
        # é‡‡æ ·åŠ¨ä½œ
        action = agent.sample_action(observation)
        print(f"  é‡‡æ ·åŠ¨ä½œ: {action}")
        
        # è®¡ç®—ç­–ç•¥æ¦‚ç‡
        probs = agent.compute_policy_probabilities(observation)
        print(f"  ç­–ç•¥æ¦‚ç‡: {probs}")
        print(f"  æ¦‚ç‡å’Œ: {np.sum(probs):.3f}")
        
        # æ¨¡æ‹Ÿç³»ç»ŸçŠ¶æ€
        system_state = SystemState.from_vector(np.random.rand(16))
        ideal_state = SystemState.from_vector(np.random.rand(16))
        
        # è®¡ç®—å±€éƒ¨ä»·å€¼
        local_value = agent.compute_local_value(system_state, action)
        print(f"  å±€éƒ¨ä»·å€¼: {local_value:.3f}")
        
        # è®¡ç®—æ”¶ç›Š
        reward = agent.compute_reward(system_state, action, 0.8, ideal_state)
        print(f"  æ”¶ç›Š: {reward:.3f}")
        
        # ç­–ç•¥æ›´æ–°
        agent.update_policy(observation, action, reward)
        print(f"  ç­–ç•¥å‚æ•°æ›´æ–°å®Œæˆ")
    
    print(f"\nâœ… æ™ºèƒ½ä½“ç³»ç»Ÿæµ‹è¯•å®Œæˆ")
    print(f"æ€»è®¡ {manager.get_agent_count()} ä¸ªæ™ºèƒ½ä½“æ­£å¸¸å·¥ä½œ")
    
    return manager

if __name__ == "__main__":
    test_agent_system()