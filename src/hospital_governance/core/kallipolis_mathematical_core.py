#!/usr/bin/env python3
"""
Kallipolis Medical Republic - æ•°ç†æ¨å¯¼ä¸¥æ ¼å®ç°æ¨¡å— (é‡æ„ç‰ˆæœ¬)
åŸºäºå®Œæ•´æ•°ç†æ¨å¯¼æ¡†æ¶çš„æ ¸å¿ƒç®—æ³•å®ç°

æ•°å­¦æ¨¡å‹æ‰©å±•:
1. 16ç»´çŠ¶æ€ç©ºé—´ x(t) âˆˆ â„^16  
2. 5ä¸ªæ™ºèƒ½ä½“è§’è‰² (åŒ»ç”Ÿã€å®ä¹ ç”Ÿã€æ‚£è€…ä»£è¡¨ã€ä¼šè®¡ã€æ”¿åºœä»£ç†)
3. å‚æ•°åŒ–éšæœºç­–ç•¥ Ï€_i(a_i | o_i; Î¸_i)
4. æé›…æ™®è¯ºå¤«ç¨³å®šæ€§åˆ†æ
5. ç¥åœ£æ³•å…¸åŠ¨æ€æ¼”åŒ–
"""


import numpy as np
import scipy.linalg as la
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging
from hospital_governance.core.state_space import StateSpace, SystemState
from hospital_governance.stability.lyapunov_analysis import LyapunovAnalyzer

logger = logging.getLogger(__name__)

@dataclass
class SystemState:
    """ç³»ç»ŸçŠ¶æ€ x(t) âˆˆ â„^16 - æ‰©å±•çš„16ç»´çŠ¶æ€ç©ºé—´"""
    # æ ¸å¿ƒåŒ»ç–—æŒ‡æ ‡ (x_1 åˆ° x_4)
    medical_resource_utilization: float  # xâ‚: åŒ»ç–—èµ„æºåˆ©ç”¨ç‡
    patient_waiting_time: float         # xâ‚‚: æ‚£è€…ç­‰å¾…æ—¶é—´  
    financial_indicator: float          # xâ‚ƒ: è´¢åŠ¡å¥åº·æŒ‡æ ‡
    ethical_compliance: float           # xâ‚„: ä¼¦ç†åˆè§„åº¦
    
    # æ•™è‚²å’ŒåŸ¹è®­æŒ‡æ ‡ (x_5 åˆ° x_8)
    education_training_quality: float   # xâ‚…: æ•™è‚²åŸ¹è®­è´¨é‡
    intern_satisfaction: float          # xâ‚†: å®ä¹ ç”Ÿæ»¡æ„åº¦
    professional_development: float     # xâ‚‡: èŒä¸šå‘å±•æŒ‡æ•°
    mentorship_effectiveness: float     # xâ‚ˆ: æŒ‡å¯¼æ•ˆæœ
    
    # æ‚£è€…æœåŠ¡æŒ‡æ ‡ (x_9 åˆ° x_12)
    patient_satisfaction: float         # xâ‚‰: æ‚£è€…æ»¡æ„åº¦
    service_accessibility: float        # xâ‚â‚€: æœåŠ¡å¯åŠæ€§
    care_quality_index: float          # xâ‚â‚: æŠ¤ç†è´¨é‡æŒ‡æ•°
    safety_incident_rate: float        # xâ‚â‚‚: å®‰å…¨äº‹æ•…ç‡(åå‘)
    
    # ç³»ç»Ÿè¿è¥æŒ‡æ ‡ (x_13 åˆ° x_16)
    operational_efficiency: float       # xâ‚â‚ƒ: è¿è¥æ•ˆç‡
    staff_workload_balance: float      # xâ‚â‚„: å‘˜å·¥å·¥ä½œè´Ÿè·å¹³è¡¡
    crisis_response_capability: float   # xâ‚â‚…: å±æœºå“åº”èƒ½åŠ›
    regulatory_compliance_score: float  # xâ‚â‚†: ç›‘ç®¡åˆè§„åˆ†æ•°
    
    def to_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸º16ç»´çŠ¶æ€å‘é‡"""
        return np.array([
            self.medical_resource_utilization,
            self.patient_waiting_time,
            self.financial_indicator,
            self.ethical_compliance,
            self.education_training_quality,
            self.intern_satisfaction,
            self.professional_development,
            self.mentorship_effectiveness,
            self.patient_satisfaction,
            self.service_accessibility,
            self.care_quality_index,
            self.safety_incident_rate,
            self.operational_efficiency,
            self.staff_workload_balance,
            self.crisis_response_capability,
            self.regulatory_compliance_score
        ])
    
    @classmethod
    def from_vector(cls, x: np.ndarray) -> 'SystemState':
        """ä»16ç»´å‘é‡æ„é€ ç³»ç»ŸçŠ¶æ€"""
        if len(x) < 16:
            # å¦‚æœè¾“å…¥å‘é‡ä¸å¤Ÿ16ç»´ï¼Œç”¨é»˜è®¤å€¼å¡«å……
            x_extended = np.zeros(16)
            x_extended[:len(x)] = x
            x_extended[len(x):] = 0.5  # é»˜è®¤å€¼
            x = x_extended
            
        return cls(
            medical_resource_utilization=x[0],
            patient_waiting_time=x[1],
            financial_indicator=x[2],
            ethical_compliance=x[3],
            education_training_quality=x[4],
            intern_satisfaction=x[5],
            professional_development=x[6],
            mentorship_effectiveness=x[7],
            patient_satisfaction=x[8],
            service_accessibility=x[9],
            care_quality_index=x[10],
            safety_incident_rate=x[11],
            operational_efficiency=x[12],
            staff_workload_balance=x[13],
            crisis_response_capability=x[14],
            regulatory_compliance_score=x[15]
        )
    
    def get_component_names(self) -> List[str]:
        """è·å–çŠ¶æ€åˆ†é‡åç§°"""
        return [
            'åŒ»ç–—èµ„æºåˆ©ç”¨ç‡', 'æ‚£è€…ç­‰å¾…æ—¶é—´', 'è´¢åŠ¡å¥åº·æŒ‡æ ‡', 'ä¼¦ç†åˆè§„åº¦',
            'æ•™è‚²åŸ¹è®­è´¨é‡', 'å®ä¹ ç”Ÿæ»¡æ„åº¦', 'èŒä¸šå‘å±•æŒ‡æ•°', 'æŒ‡å¯¼æ•ˆæœ',
            'æ‚£è€…æ»¡æ„åº¦', 'æœåŠ¡å¯åŠæ€§', 'æŠ¤ç†è´¨é‡æŒ‡æ•°', 'å®‰å…¨äº‹æ•…ç‡',
            'è¿è¥æ•ˆç‡', 'å‘˜å·¥å·¥ä½œè´Ÿè·å¹³è¡¡', 'å±æœºå“åº”èƒ½åŠ›', 'ç›‘ç®¡åˆè§„åˆ†æ•°'
        ]

@dataclass
class HolyCodeRule:
    """ç¥åœ£æ³•å…¸è§„åˆ™ (R_k, W_k, C_k) - æ‰©å±•æ”¯æŒ16ç»´çŠ¶æ€"""
    rule_id: str
    logic_function: Callable[[SystemState], float]  # R_k: S â†’ â„
    weight: float                                   # W_k âˆˆ â„âº
    context: Callable[[SystemState], bool]         # C_k âŠ‚ S (æŒ‡ç¤ºå‡½æ•°)
    target_value: float                            # R_k*
    description: str
    category: str = "general"  # è§„åˆ™ç±»åˆ«
    
    def evaluate(self, state: SystemState) -> Tuple[bool, float]:
        """è¯„ä¼°è§„åˆ™æ¿€æ´»çŠ¶æ€å’Œä¸¥é‡ç¨‹åº¦"""
        if not self.context(state):
            return False, 0.0
        
        current_value = self.logic_function(state)
        deviation = abs(current_value - self.target_value)
        activated = deviation > 0.1  # é˜ˆå€¼å¯é…ç½®
        severity = self.weight * deviation
        
        return activated, severity

class HolyCode:
    """ç¥åœ£æ³•å…¸ HC(t) - æ‰©å±•æ”¯æŒ16ç»´çŠ¶æ€ç©ºé—´"""
    
    def __init__(self):
        self.rules: Dict[str, HolyCodeRule] = {}
        self._initialize_extended_rules()
    
    def _initialize_extended_rules(self):
        """åˆå§‹åŒ–æ‰©å±•çš„è§„åˆ™é›†åˆ - æ”¯æŒ16ç»´çŠ¶æ€ç©ºé—´"""
        
        # æ ¸å¿ƒåŒ»ç–—è§„åˆ™
        self.rules['patient_safety_protocol'] = HolyCodeRule(
            rule_id='patient_safety_protocol',
            logic_function=lambda s: s.care_quality_index * (1.0 - s.safety_incident_rate),
            weight=1.2,
            context=lambda s: True,  # å§‹ç»ˆé€‚ç”¨
            target_value=0.85,
            description='æ‚£è€…å®‰å…¨åè®® - ç¡®ä¿æŠ¤ç†è´¨é‡ä¸å®‰å…¨',
            category='medical'
        )
        
        # èµ„æºä¼˜åŒ–è§„åˆ™
        self.rules['resource_optimization'] = HolyCodeRule(
            rule_id='resource_optimization',
            logic_function=lambda s: s.medical_resource_utilization * s.operational_efficiency,
            weight=0.9,
            context=lambda s: s.financial_indicator > 0.3,
            target_value=0.8,
            description='èµ„æºä¼˜åŒ–è§„åˆ™ - å¹³è¡¡èµ„æºåˆ©ç”¨ä¸æ•ˆç‡',
            category='operational'
        )
        
        # æ•™è‚²å‘å±•è§„åˆ™
        self.rules['education_excellence'] = HolyCodeRule(
            rule_id='education_excellence',
            logic_function=lambda s: (s.education_training_quality + s.intern_satisfaction + s.professional_development) / 3,
            weight=0.8,
            context=lambda s: s.intern_satisfaction < 0.8 or s.education_training_quality < 0.8,
            target_value=0.85,
            description='æ•™è‚²å“è¶Šè§„åˆ™ - æå‡åŸ¹è®­è´¨é‡ä¸æ»¡æ„åº¦',
            category='education'
        )
        
        # æ‚£è€…æœåŠ¡è§„åˆ™
        self.rules['patient_service_excellence'] = HolyCodeRule(
            rule_id='patient_service_excellence',
            logic_function=lambda s: (s.patient_satisfaction + s.service_accessibility) / 2,
            weight=1.0,
            context=lambda s: s.patient_satisfaction < 0.8 or s.service_accessibility < 0.7,
            target_value=0.85,
            description='æ‚£è€…æœåŠ¡å“è¶Šè§„åˆ™ - æå‡æ‚£è€…ä½“éªŒ',
            category='service'
        )
        
        # è´¢åŠ¡ç¨³å®šè§„åˆ™
        self.rules['financial_stability'] = HolyCodeRule(
            rule_id='financial_stability',
            logic_function=lambda s: s.financial_indicator * s.operational_efficiency,
            weight=0.85,
            context=lambda s: s.financial_indicator < 0.6,
            target_value=0.75,
            description='è´¢åŠ¡ç¨³å®šè§„åˆ™ - ç»´æŠ¤è´¢åŠ¡å¥åº·',
            category='financial'
        )
        
        # ç›‘ç®¡åˆè§„è§„åˆ™
        self.rules['regulatory_compliance'] = HolyCodeRule(
            rule_id='regulatory_compliance',
            logic_function=lambda s: (s.regulatory_compliance_score + s.ethical_compliance) / 2,
            weight=1.1,
            context=lambda s: s.regulatory_compliance_score < 0.9 or s.ethical_compliance < 0.9,
            target_value=0.95,
            description='ç›‘ç®¡åˆè§„è§„åˆ™ - ç¡®ä¿åˆè§„æ€§',
            category='governance'
        )
        
        # å±æœºå“åº”è§„åˆ™
        self.rules['crisis_response'] = HolyCodeRule(
            rule_id='crisis_response',
            logic_function=lambda s: s.crisis_response_capability * (1.0 - s.patient_waiting_time),
            weight=1.3,
            context=lambda s: s.crisis_response_capability < 0.8 or s.patient_waiting_time > 0.4,
            target_value=0.8,
            description='å±æœºå“åº”è§„åˆ™ - å¿«é€Ÿå“åº”èƒ½åŠ›',
            category='emergency'
        )
        
        # å·¥ä½œè´Ÿè·å¹³è¡¡è§„åˆ™
        self.rules['workload_balance'] = HolyCodeRule(
            rule_id='workload_balance',
            logic_function=lambda s: s.staff_workload_balance * s.intern_satisfaction,
            weight=0.7,
            context=lambda s: s.staff_workload_balance < 0.7,
            target_value=0.8,
            description='å·¥ä½œè´Ÿè·å¹³è¡¡è§„åˆ™ - å‘˜å·¥ç¦ç¥‰',
            category='welfare'
        )
    
    def compute_ideal_state(self, current_state: SystemState, disturbance: np.ndarray) -> SystemState:
        """è®¡ç®—ç†æƒ³çŠ¶æ€ x*(t) = Î¨(HC(t), d(t)) - 16ç»´ä¼˜åŒ–"""
        x = current_state.to_vector()
        x_ideal = x.copy()
        
        # åŸºäºç¥åœ£æ³•å…¸ä¼˜åŒ–ç†æƒ³çŠ¶æ€
        total_weight = sum(rule.weight for rule in self.rules.values())
        
        # åˆ†ç±»åˆ«å¤„ç†è§„åˆ™å½±å“
        category_adjustments = {
            'medical': np.zeros(16),
            'operational': np.zeros(16),
            'education': np.zeros(16),
            'service': np.zeros(16),
            'financial': np.zeros(16),
            'governance': np.zeros(16),
            'emergency': np.zeros(16),
            'welfare': np.zeros(16)
        }
        
        for rule in self.rules.values():
            if rule.context(current_state):
                current_value = rule.logic_function(current_state)
                adjustment = rule.weight / total_weight * (rule.target_value - current_value)
                
                # æ ¹æ®è§„åˆ™ç±»åˆ«åˆ†é…è°ƒæ•´
                if rule.category == 'medical':
                    category_adjustments['medical'][[0, 10, 11]] += adjustment * 0.1
                elif rule.category == 'operational':
                    category_adjustments['operational'][[0, 12]] += adjustment * 0.1
                elif rule.category == 'education':
                    category_adjustments['education'][[4, 5, 6, 7]] += adjustment * 0.1
                elif rule.category == 'service':
                    category_adjustments['service'][[8, 9]] += adjustment * 0.1
                elif rule.category == 'financial':
                    category_adjustments['financial'][[2, 12]] += adjustment * 0.1
                elif rule.category == 'governance':
                    category_adjustments['governance'][[3, 15]] += adjustment * 0.1
                elif rule.category == 'emergency':
                    category_adjustments['emergency'][[1, 14]] += adjustment * 0.1
                elif rule.category == 'welfare':
                    category_adjustments['welfare'][[13, 5]] += adjustment * 0.1
        
        # åº”ç”¨æ‰€æœ‰è°ƒæ•´
        for adjustments in category_adjustments.values():
            x_ideal += adjustments
        
        # åŠ å…¥æ‰°åŠ¨å½±å“
        if len(disturbance) >= 16:
            x_ideal += 0.05 * disturbance[:16]
        else:
            x_ideal[:len(disturbance)] += 0.05 * disturbance
        
        x_ideal = np.clip(x_ideal, 0, 1)
        
        return SystemState.from_vector(x_ideal)
    
    def get_active_rules(self, state: SystemState) -> Dict[str, Dict]:
        """è·å–å½“å‰æ¿€æ´»çš„è§„åˆ™"""
        active_rules = {}
        for rule_id, rule in self.rules.items():
            activated, severity = rule.evaluate(state)
            if activated:
                active_rules[rule_id] = {
                    'description': rule.description,
                    'severity': severity,
                    'category': rule.category,
                    'target_value': rule.target_value,
                    'current_value': rule.logic_function(state)
                }
        return active_rules

class Agent:
    """æ™ºèƒ½ä½“ i âˆˆ A - æ”¯æŒ5ä¸ªè§’è‰²çš„æ‰©å±•ç‰ˆæœ¬"""
    
    def __init__(self, agent_id: str, role: str, action_space_size: int = 8):
        self.agent_id = agent_id
        self.role = role
        self.action_space_size = action_space_size
        
        # ç­–ç•¥å‚æ•° Î¸_i
        self.theta = np.random.normal(0, 0.1, action_space_size)
        
        # æ”¶ç›Šå‡½æ•°æƒé‡ - åŸºäºè§’è‰²å·®å¼‚åŒ–
        if role == 'doctor':
            self.alpha, self.beta, self.gamma = 0.2, 0.6, 0.2  # é‡è§†å±€éƒ¨åŒ»ç–—ä»·å€¼
        elif role == 'intern':
            self.alpha, self.beta, self.gamma = 0.3, 0.5, 0.2  # å¹³è¡¡å‘å±•
        elif role == 'patient':
            self.alpha, self.beta, self.gamma = 0.4, 0.4, 0.2  # é‡è§†å…¨å±€å’Œå±€éƒ¨æœåŠ¡
        elif role == 'accountant':
            self.alpha, self.beta, self.gamma = 0.5, 0.3, 0.2  # é‡è§†å…¨å±€èµ„æºæ•ˆç”¨
        elif role == 'government':
            self.alpha, self.beta, self.gamma = 0.6, 0.2, 0.2  # æœ€é‡è§†å…¨å±€åˆ©ç›Š
        else:
            self.alpha, self.beta, self.gamma = 0.3, 0.5, 0.2  # é»˜è®¤æƒé‡
        
        # å­¦ä¹ ç‡
        self.learning_rate = 0.01
        
        # ç‰¹å¾æ˜ å°„ç¼“å­˜
        self._feature_cache = {}
        
        # åŸºçº¿å€¼
        self.baseline = 0.0
        
        logger.debug(f"Created {role} agent: Î±={self.alpha}, Î²={self.beta}, Î³={self.gamma}")
    
    def feature_mapping(self, observation: np.ndarray, action: int) -> np.ndarray:
        """ç‰¹å¾æ˜ å°„ Ï†_i(o_i, a_i) - æ”¯æŒ16ç»´è§‚æµ‹"""
        # ç®€å•çš„çº¿æ€§ç‰¹å¾æ˜ å°„
        obs_features = observation / (np.linalg.norm(observation) + 1e-8)
        action_features = np.zeros(self.action_space_size)
        if 0 <= action < self.action_space_size:
            action_features[action] = 1.0
        
        # ç»„åˆç‰¹å¾ï¼Œé™åˆ¶åˆ°ç­–ç•¥å‚æ•°ç»´åº¦
        if len(obs_features) + len(action_features) > len(self.theta):
            # å¦‚æœç‰¹å¾ç»´åº¦è¶…è¿‡å‚æ•°ç»´åº¦ï¼Œæˆªæ–­è§‚æµ‹ç‰¹å¾
            obs_dim = len(self.theta) - len(action_features)
            obs_features = obs_features[:obs_dim]
        
        features = np.concatenate([obs_features, action_features])
        
        # è°ƒæ•´åˆ°æ­£ç¡®ç»´åº¦
        if len(features) < len(self.theta):
            padding = np.zeros(len(self.theta) - len(features))
            features = np.concatenate([features, padding])
        elif len(features) > len(self.theta):
            features = features[:len(self.theta)]
        
        return features
    
    def policy(self, observation: np.ndarray, action: Optional[int] = None) -> np.ndarray:
        """éšæœºç­–ç•¥ Ï€_i(a_i | o_i; Î¸_i)"""
        if action is not None:
            # è®¡ç®—ç‰¹å®šåŠ¨ä½œçš„æ¦‚ç‡
            phi = self.feature_mapping(observation, action)
            logit = np.dot(phi, self.theta)
        else:
            # è®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ
            logits = []
            for a in range(self.action_space_size):
                phi = self.feature_mapping(observation, a)
                logits.append(np.dot(phi, self.theta))
            logit = np.array(logits)
        
        # Softmax with numerical stability
        if isinstance(logit, np.ndarray):
            logit = logit - np.max(logit)
            exp_logits = np.exp(logit)
            return exp_logits / np.sum(exp_logits)
        else:
            return np.exp(logit)  # å•ä¸ªåŠ¨ä½œçš„éå½’ä¸€åŒ–æ¦‚ç‡
    
    def sample_action(self, observation: np.ndarray) -> int:
        """é‡‡æ ·åŠ¨ä½œ"""
        probs = self.policy(observation)
        return np.random.choice(self.action_space_size, p=probs)
    
    def compute_reward(self, state: SystemState, action: int, 
                      global_utility: float, ideal_state: SystemState) -> float:
        """æ”¶ç›Šå‡½æ•° R_i(x, a_i, a_{-i}) = Î±_i U(x) + Î²_i V_i(x, a_i) - Î³_i D_i(x, x*)"""
        
        # å±€éƒ¨ä»·å€¼å‡½æ•° V_i - åŸºäºè§’è‰²ç‰¹å¼‚æ€§ (16ç»´çŠ¶æ€)
        if self.role == 'doctor':
            local_value = (state.medical_resource_utilization * 0.25 + 
                          state.care_quality_index * 0.3 + 
                          state.patient_satisfaction * 0.25 + 
                          (1.0 - state.safety_incident_rate) * 0.2)
        elif self.role == 'intern':
            local_value = (state.education_training_quality * 0.3 + 
                          state.intern_satisfaction * 0.25 + 
                          state.professional_development * 0.25 + 
                          state.mentorship_effectiveness * 0.2)
        elif self.role == 'patient':
            local_value = (state.patient_satisfaction * 0.3 + 
                          state.service_accessibility * 0.25 + 
                          state.care_quality_index * 0.25 + 
                          (1.0 - state.safety_incident_rate) * 0.2)
        elif self.role == 'accountant':
            local_value = (state.financial_indicator * 0.35 + 
                          state.operational_efficiency * 0.3 + 
                          state.medical_resource_utilization * 0.2 + 
                          state.regulatory_compliance_score * 0.15)
        elif self.role == 'government':
            local_value = (state.regulatory_compliance_score * 0.25 + 
                          state.ethical_compliance * 0.25 + 
                          state.crisis_response_capability * 0.2 + 
                          state.patient_satisfaction * 0.15 + 
                          state.service_accessibility * 0.15)
        else:
            local_value = 0.5
        
        # åˆ°ç†æƒ³çŠ¶æ€çš„åå·® D_i
        state_vec = state.to_vector()
        ideal_vec = ideal_state.to_vector()
        deviation = np.linalg.norm(state_vec - ideal_vec)
        
        # ç»„åˆæ”¶ç›Š
        reward = (self.alpha * global_utility + 
                 self.beta * local_value - 
                 self.gamma * deviation)
        
        return reward
    
    def update_policy(self, observation: np.ndarray, action: int, 
                     q_value: float, baseline: float = 0.0):
        """ç­–ç•¥æ¢¯åº¦æ›´æ–° Î¸_i(t+1) = Î¸_i(t) + Î· âˆ‡J_i(Î¸)"""
        # æ›´æ–°åŸºçº¿ä¼°è®¡
        self.baseline = 0.9 * self.baseline + 0.1 * q_value
        
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        phi = self.feature_mapping(observation, action)
        
        # è®¡ç®— âˆ‡log Ï€_i(a_i|o_i)
        probs = self.policy(observation)
        grad_log_pi = phi - sum([probs[a] * self.feature_mapping(observation, a) 
                               for a in range(self.action_space_size)])
        
        # ç­–ç•¥æ¢¯åº¦
        advantage = q_value - self.baseline
        policy_gradient = grad_log_pi * advantage
        
        # æ›´æ–°å‚æ•°
        self.theta += self.learning_rate * policy_gradient


class KallipolisMedicalSystem:
    """KallipolisåŒ»ç–—å…±å’Œå›½ç³»ç»Ÿ - æ•°ç†æ¨å¯¼å®Œæ•´å®ç° (é‡æ„ç‰ˆæœ¬)"""
    
    def __init__(self):
        # ç³»ç»Ÿç»„ä»¶
        self.holy_code = HolyCode()
        self.agents: List[Agent] = []
        self.lyapunov_analyzer = LyapunovAnalyzer(16, 5, 8)  # 16ç»´çŠ¶æ€ï¼Œ5ä¸ªæ™ºèƒ½ä½“ï¼Œ8ç»´å‚æ•°
        # 16ç»´ç³»ç»ŸçŠ¶æ€åˆå§‹åŒ–
        self.current_state = SystemState(
            medical_resource_utilization=0.7,
            patient_waiting_time=0.3,
            financial_indicator=0.65,
            ethical_compliance=0.8,
            education_training_quality=0.75,
            intern_satisfaction=0.7,
            professional_development=0.6,
            mentorship_effectiveness=0.8,
            patient_satisfaction=0.85,
            service_accessibility=0.8,
            care_quality_index=0.9,
            safety_incident_rate=0.05,
            operational_efficiency=0.75,
            staff_workload_balance=0.7,
            crisis_response_capability=0.8,
            regulatory_compliance_score=0.9
        )
        
        # æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics = {
            'disturbance_adaptation_time': 0.0,
            'rule_update_success_rate': 0.0,
            'consensus_convergence_rate': 0.0,
            'rule_update_response_time': 0.0
        }
        
        # è½¨è¿¹è®°å½•
        self.trajectory: List[Tuple[SystemState, List[np.ndarray]]] = []
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self._initialize_agents()
    
    def _initialize_agents(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“é›†åˆ A = {åŒ»ç”Ÿ, å®ä¹ ç”Ÿ, æ‚£è€…ä»£è¡¨, ä¼šè®¡, æ”¿åºœä»£ç†}"""
        agent_configs = [
            ('doctor', 'åŒ»ç”Ÿ'),
            ('intern', 'å®ä¹ åŒ»ç”Ÿ'), 
            ('patient', 'æ‚£è€…ä»£è¡¨'),
            ('accountant', 'ä¼šè®¡'),
            ('government', 'æ”¿åºœä»£ç†')  # æ–°å¢æ”¿åºœä»£ç†
        ]
        
        for agent_id, role in agent_configs:
            agent = Agent(agent_id, role)
            self.agents.append(agent)
            logger.info(f"Initialized agent: {agent_id} ({role})")
    
    def system_step(self, disturbance: np.ndarray) -> Dict:
        """æ‰§è¡Œä¸€æ­¥ç³»ç»ŸåŠ¨æ€ - 16ç»´çŠ¶æ€ç©ºé—´"""
        # ç¡®ä¿æ‰°åŠ¨ç»´åº¦æ­£ç¡®
        if len(disturbance) < 16:
            disturbance_extended = np.zeros(16)
            disturbance_extended[:len(disturbance)] = disturbance
            disturbance = disturbance_extended
        
        # 1. è®¡ç®—ç†æƒ³çŠ¶æ€
        ideal_state = self.holy_code.compute_ideal_state(self.current_state, disturbance)
        
        # 2. æ™ºèƒ½ä½“è§‚æµ‹å’Œå†³ç­–
        observations = []
        actions = []
        rewards = []
        
        # 16ç»´ç³»ç»ŸçŠ¶æ€ä½œä¸ºè§‚æµ‹åŸºç¡€
        base_observation = self.current_state.to_vector()
        
        for agent in self.agents:
            # å±€éƒ¨è§‚æµ‹ï¼ˆåŸºäºè§’è‰²çš„ä¸åŒè§‚æµ‹çª—å£ï¼‰
            if agent.role == 'doctor':
                obs_indices = [0, 1, 2, 3, 8, 9, 10, 11]  # åŒ»ç–—ç›¸å…³æŒ‡æ ‡
            elif agent.role == 'intern':
                obs_indices = [4, 5, 6, 7, 13, 0, 12, 3]  # æ•™è‚²åŸ¹è®­ç›¸å…³
            elif agent.role == 'patient':
                obs_indices = [8, 9, 10, 11, 1, 3, 12, 14]  # æ‚£è€…æœåŠ¡ç›¸å…³
            elif agent.role == 'accountant':
                obs_indices = [2, 12, 0, 15, 1, 9, 14, 13]  # è´¢åŠ¡è¿è¥ç›¸å…³
            elif agent.role == 'government':
                obs_indices = [15, 3, 14, 12, 8, 9, 11, 2]  # ç›‘ç®¡æ²»ç†ç›¸å…³
            else:
                obs_indices = list(range(8))  # é»˜è®¤å‰8ç»´
            
            obs = base_observation[obs_indices] + np.random.normal(0, 0.02, 8)
            observations.append(obs)
            
            # é‡‡æ ·åŠ¨ä½œ
            action = agent.sample_action(obs)
            actions.append(action)
        
        # 3. è®¡ç®—å…¨å±€æ•ˆç”¨
        global_utility = self._compute_global_utility(self.current_state)
        
        # 4. è®¡ç®—æ”¶ç›Šå’Œæ›´æ–°ç­–ç•¥
        for i, agent in enumerate(self.agents):
            reward = agent.compute_reward(self.current_state, actions[i], 
                                        global_utility, ideal_state)
            rewards.append(reward)
            
            # Qå€¼ç®€åŒ–è®¡ç®—ï¼ˆå®é™…åº”è¯¥ç”¨æ—¶åºå·®åˆ†å­¦ä¹ ï¼‰
            q_value = reward + 0.9 * global_utility  # Î³=0.9
            
            # ç­–ç•¥æ›´æ–°
            agent.update_policy(observations[i], actions[i], q_value)
        
        # 5. çŠ¶æ€è½¬ç§»ï¼ˆç®€åŒ–çš„åŠ¨æ€æ–¹ç¨‹ï¼‰ - 16ç»´çŠ¶æ€ç©ºé—´
        state_vec = self.current_state.to_vector()
        
        # æ™ºèƒ½ä½“åŠ¨ä½œå¯¹ä¸åŒçŠ¶æ€åˆ†é‡çš„å½±å“
        action_effects = np.zeros(16)
        for i, agent in enumerate(self.agents):
            action_weight = 0.02 * actions[i] / agent.action_space_size
            
            if agent.role == 'doctor':
                action_effects[[0, 10, 11]] += action_weight
            elif agent.role == 'intern':
                action_effects[[4, 5, 6, 7]] += action_weight
            elif agent.role == 'patient':
                action_effects[[8, 9]] += action_weight
            elif agent.role == 'accountant':
                action_effects[[2, 12]] += action_weight
            elif agent.role == 'government':
                action_effects[[15, 3, 14]] += action_weight
        
        new_state_vec = state_vec + action_effects + disturbance * 0.05 + np.random.normal(0, 0.005, 16)
        new_state_vec = np.clip(new_state_vec, 0, 1)
        
        self.current_state = SystemState.from_vector(new_state_vec)
        
        # 6. è®°å½•è½¨è¿¹
        agent_params = [agent.theta.copy() for agent in self.agents]
        self.trajectory.append((self.current_state, agent_params))
        
        # 7. è§„åˆ™æ¿€æ´»æ£€æŸ¥
        rule_activations = self.holy_code.get_active_rules(self.current_state)
        
        return {
            'state': self.current_state,
            'ideal_state': ideal_state,
            'actions': actions,
            'rewards': rewards,
            'rule_activations': rule_activations,
            'global_utility': global_utility,
            'observations': observations
        }
    
    def _compute_global_utility(self, state: SystemState) -> float:
        """è®¡ç®—å…¨å±€èµ„æºæ•ˆç”¨å‡½æ•° U(x) - 16ç»´çŠ¶æ€ç©ºé—´"""
        state_vec = state.to_vector()
        
        # 16ç»´èµ„æºæ•ˆç”¨åŠ æƒç»„åˆ - åŸºäºåŒ»é™¢æ²»ç†é‡è¦æ€§
        weights = np.array([
            0.12,  # åŒ»ç–—èµ„æºåˆ©ç”¨ç‡
            -0.08, # æ‚£è€…ç­‰å¾…æ—¶é—´ (è´Ÿæƒé‡)
            0.10,  # è´¢åŠ¡å¥åº·æŒ‡æ ‡
            0.12,  # ä¼¦ç†åˆè§„åº¦
            0.06,  # æ•™è‚²åŸ¹è®­è´¨é‡
            0.04,  # å®ä¹ ç”Ÿæ»¡æ„åº¦
            0.03,  # èŒä¸šå‘å±•æŒ‡æ•°
            0.03,  # æŒ‡å¯¼æ•ˆæœ
            0.15,  # æ‚£è€…æ»¡æ„åº¦
            0.10,  # æœåŠ¡å¯åŠæ€§
            0.12,  # æŠ¤ç†è´¨é‡æŒ‡æ•°
            -0.06, # å®‰å…¨äº‹æ•…ç‡ (è´Ÿæƒé‡)
            0.08,  # è¿è¥æ•ˆç‡
            0.05,  # å‘˜å·¥å·¥ä½œè´Ÿè·å¹³è¡¡
            0.06,  # å±æœºå“åº”èƒ½åŠ›
            0.08   # ç›‘ç®¡åˆè§„åˆ†æ•°
        ])
        
        utility = np.dot(weights, state_vec)
        return np.clip(utility, 0, 1)
    
    def analyze_system_stability(self) -> Dict:
        """åˆ†æç³»ç»Ÿç¨³å®šæ€§ï¼ˆè°ƒç”¨ stability/lyapunov_analysis.py å®ç°ï¼‰"""
        if len(self.trajectory) < 10:
            return {'error': 'insufficient_trajectory_data'}
        return self.lyapunov_analyzer.analyze_stability(self.trajectory[-50:])
    
    def get_performance_metrics(self) -> Dict:
        """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        if len(self.trajectory) < 2:
            return self.performance_metrics
        
        # è®¡ç®—æ‰°åŠ¨é€‚åº”æ—¶é—´ (DAT)
        recent_states = [item[0].to_vector() for item in self.trajectory[-10:]]
        if len(recent_states) >= 2:
            state_variations = [np.std(states) for states in zip(*recent_states)]
            avg_variation = np.mean(state_variations)
            self.performance_metrics['disturbance_adaptation_time'] = min(10.0, 1.0 / (avg_variation + 1e-6))
        
        # è®¡ç®—è§„åˆ™æ›´æ–°æˆåŠŸç‡ (RUSR)
        rule_consistency = 0.0
        if len(self.trajectory) >= 5:
            recent_activations = []
            for state, _ in self.trajectory[-5:]:
                activations = []
                for rule in self.holy_code.rules.values():
                    activated, _ = rule.evaluate(state)
                    activations.append(activated)
                recent_activations.append(activations)
            
            if recent_activations:
                consistency_scores = []
                for i in range(len(recent_activations[0])):
                    rule_sequence = [activations[i] for activations in recent_activations]
                    consistency = 1.0 - np.std(rule_sequence)
                    consistency_scores.append(consistency)
                rule_consistency = np.mean(consistency_scores)
        
        self.performance_metrics['rule_update_success_rate'] = rule_consistency
        
        # è®¡ç®—å…±è¯†æ”¶æ•›ç‡ (CCR)
        if len(self.trajectory) >= 5:
            recent_params = [params for _, params in self.trajectory[-5:]]
            param_variations = []
            for i in range(len(self.agents)):
                agent_param_sequence = [params[i] for params in recent_params]
                param_std = np.std(agent_param_sequence, axis=0)
                param_variations.append(np.mean(param_std))
            
            avg_param_variation = np.mean(param_variations)
            self.performance_metrics['consensus_convergence_rate'] = max(0.0, 1.0 - avg_param_variation)
        
        return self.performance_metrics
    
    def get_system_summary(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€æ‘˜è¦"""
        return {
            'current_state': self.current_state,
            'state_vector': self.current_state.to_vector(),
            'state_names': self.current_state.get_component_names(),
            'num_agents': len(self.agents),
            'agent_roles': [agent.role for agent in self.agents],
            'active_rules': self.holy_code.get_active_rules(self.current_state),
            'trajectory_length': len(self.trajectory),
            'performance_metrics': self.get_performance_metrics()
        }
    
    def reset(self):
        """é‡ç½®ç³»ç»ŸçŠ¶æ€ - 16ç»´çŠ¶æ€ç©ºé—´"""
        self.current_state = SystemState(
            medical_resource_utilization=0.7,
            patient_waiting_time=0.3,
            financial_indicator=0.65,
            ethical_compliance=0.8,
            education_training_quality=0.75,
            intern_satisfaction=0.7,
            professional_development=0.6,
            mentorship_effectiveness=0.8,
            patient_satisfaction=0.85,
            service_accessibility=0.8,
            care_quality_index=0.9,
            safety_incident_rate=0.05,
            operational_efficiency=0.75,
            staff_workload_balance=0.7,
            crisis_response_capability=0.8,
            regulatory_compliance_score=0.9
        )
        
        # é‡ç½®æ™ºèƒ½ä½“ç­–ç•¥å‚æ•°
        for agent in self.agents:
            agent.theta = np.random.normal(0, 0.1, agent.action_space_size)
            agent.baseline = 0.0
        
        # æ¸…ç©ºè½¨è¿¹
        self.trajectory = []
        
        logger.info("Kallipolis Medical System (16D, 5 agents) has been reset")

# éªŒè¯å‡½æ•°
def verify_extended_mathematical_implementation():
    """éªŒè¯æ‰©å±•æ•°ç†æ¨å¯¼å®ç°çš„æ­£ç¡®æ€§"""
    print("ğŸ”¬ Kallipolis Medical Republic - æ‰©å±•æ•°ç†æ¨å¯¼éªŒè¯")
    print("=" * 70)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    system = KallipolisMedicalSystem()
    
    print("âœ… ç³»ç»Ÿç»„ä»¶åˆå§‹åŒ–:")
    print(f"  - æ™ºèƒ½ä½“æ•°é‡: {len(system.agents)} (åŒ…å«æ”¿åºœä»£ç†)")
    print(f"  - ç¥åœ£æ³•å…¸è§„åˆ™æ•°: {len(system.holy_code.rules)}")
    print(f"  - çŠ¶æ€ç©ºé—´ç»´åº¦: {len(system.current_state.to_vector())} (16ç»´æ‰©å±•)")
    print(f"  - æ™ºèƒ½ä½“è§’è‰²: {[agent.role for agent in system.agents]}")
    
    # æ˜¾ç¤ºçŠ¶æ€åˆ†é‡
    print("\\nğŸ“Š 16ç»´çŠ¶æ€ç©ºé—´åˆ†é‡:")
    for i, name in enumerate(system.current_state.get_component_names()):
        value = system.current_state.to_vector()[i]
        print(f"  x_{i+1:2d}: {name:<20} = {value:.3f}")
    
    # è¿è¡Œä»¿çœŸæ­¥éª¤
    print("\\nğŸ”„ æ‰§è¡Œä»¿çœŸæ­¥éª¤:")
    disturbances = [np.random.normal(0, 0.03, 16) for _ in range(30)]
    
    for step in range(30):
        result = system.system_step(disturbances[step])
        
        if step % 5 == 0:
            print(f"  æ­¥éª¤ {step:2d}:")
            print(f"    å…¨å±€æ•ˆç”¨: {result['global_utility']:.4f}")
            print(f"    æ¿€æ´»è§„åˆ™æ•°: {len(result['rule_activations'])}")
            print(f"    å¹³å‡æ”¶ç›Š: {np.mean(result['rewards']):.4f}")
            print(f"    çŠ¶æ€å˜åŒ–: {np.linalg.norm(result['state'].to_vector() - system.current_state.to_vector()):.4f}")
    
    # ç¨³å®šæ€§åˆ†æ
    print("\\nğŸ“Š ç¨³å®šæ€§åˆ†æ:")
    stability_result = system.analyze_system_stability()
    if 'error' not in stability_result:
        print(f"  ç³»ç»Ÿç¨³å®šæ€§: {'âœ… ç¨³å®š' if stability_result['stable'] else 'âŒ ä¸ç¨³å®š'}")
        print(f"  æ”¶æ•›ç‡: {stability_result['convergence_rate']:.6f}")
        print(f"  æé›…æ™®è¯ºå¤«å‡½æ•°å€¼: {stability_result['final_value']:.6f}")
    else:
        print(f"  âš ï¸  {stability_result['error']}")
    
    # æ€§èƒ½æŒ‡æ ‡
    print("\\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    metrics = system.get_performance_metrics()
    for metric, value in metrics.items():
        print(f"  {metric:<30}: {value:.4f}")
    
    # æ™ºèƒ½ä½“æ”¶ç›Šå‡½æ•°æƒé‡
    print("\\nğŸ‘¥ æ™ºèƒ½ä½“é…ç½®:")
    for agent in system.agents:
        print(f"  {agent.role:<12}: Î±={agent.alpha:.1f}, Î²={agent.beta:.1f}, Î³={agent.gamma:.1f}")
    
    # ç¥åœ£æ³•å…¸è§„åˆ™çŠ¶æ€
    print("\\nğŸ“œ ç¥åœ£æ³•å…¸è§„åˆ™çŠ¶æ€:")
    active_rules = system.holy_code.get_active_rules(system.current_state)
    if active_rules:
        for rule_id, rule_info in active_rules.items():
            print(f"  ğŸ”´ {rule_id}: {rule_info['description']}")
            print(f"     ä¸¥é‡ç¨‹åº¦: {rule_info['severity']:.3f}, ç±»åˆ«: {rule_info['category']}")
    else:
        print("  âœ… æ‰€æœ‰è§„åˆ™å‡å¤„äºåˆè§„çŠ¶æ€")
    
    print("\\nğŸ¯ éªŒè¯å®Œæˆ - æ‰©å±•æ•°ç†æ¨å¯¼å®ç°æ­£å¸¸å·¥ä½œ")
    print(f"ğŸ“Š ç³»ç»Ÿæ‘˜è¦: 16ç»´çŠ¶æ€ç©ºé—´, 5ä¸ªæ™ºèƒ½ä½“è§’è‰², {len(system.holy_code.rules)}ä¸ªåŠ¨æ€è§„åˆ™")
    
    return system

if __name__ == "__main__":
    verify_extended_mathematical_implementation()