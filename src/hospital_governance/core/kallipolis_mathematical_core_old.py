#!/usr/bin/env python3
"""
Kallipolis Medical Republic - æ•°ç†æ¨å¯¼ä¸¥æ ¼å®ç°æ¨¡å—
åŸºäºå®Œæ•´æ•°ç†æ¨å¯¼æ¡†æ¶çš„æ ¸å¿ƒç®—æ³•å®ç°
"""

import numpy as np
import scipy.linalg as la
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging

logger = logging.getLogger(__name__)

@dataclass
class SystemState:
    """ç³»ç»ŸçŠ¶æ€ x(t) âˆˆ â„^n"""
    medical_resource_utilization: float  # xâ‚: åŒ»ç–—èµ„æºåˆ©ç”¨ç‡
    patient_waiting_time: float         # xâ‚‚: æ‚£è€…ç­‰å¾…æ—¶é—´
    financial_indicator: float          # xâ‚ƒ: è´¢åŠ¡æŒ‡æ ‡
    ethical_compliance: float           # xâ‚„: ä¼¦ç†åˆè§„åº¦
    education_training: float           # xâ‚…: æ•™è‚²åŸ¹è®­æŒ‡æ ‡
    patient_satisfaction: float         # xâ‚†: æ‚£è€…æ»¡æ„åº¦
    emergency_queue_length: float       # xâ‚‡: æ€¥è¯Šé˜Ÿåˆ—é•¿åº¦
    
    def to_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸ºçŠ¶æ€å‘é‡"""
        return np.array([
            self.medical_resource_utilization,
            self.patient_waiting_time,
            self.financial_indicator,
            self.ethical_compliance,
            self.education_training,
            self.patient_satisfaction,
            self.emergency_queue_length
        ])
    
    @classmethod
    def from_vector(cls, x: np.ndarray) -> 'SystemState':
        """ä»çŠ¶æ€å‘é‡æ„é€ """
        return cls(
            medical_resource_utilization=x[0],
            patient_waiting_time=x[1],
            financial_indicator=x[2],
            ethical_compliance=x[3],
            education_training=x[4],
            patient_satisfaction=x[5],
            emergency_queue_length=x[6]
        )

@dataclass
class HolyCodeRule:
    """ç¥åœ£æ³•å…¸è§„åˆ™ (R_k, W_k, C_k)"""
    rule_id: str
    logic_function: Callable[[SystemState], float]  # R_k: S â†’ â„
    weight: float                                   # W_k âˆˆ â„âº
    context: Callable[[SystemState], bool]         # C_k âŠ‚ S (æŒ‡ç¤ºå‡½æ•°)
    target_value: float                            # R_k*
    description: str
    
    def evaluate(self, state: SystemState) -> Tuple[bool, float]:
        """è¯„ä¼°è§„åˆ™æ¿€æ´»çŠ¶æ€å’Œä¸¥é‡ç¨‹åº¦"""
        if not self.context(state):
            return False, 0.0
        
        current_value = self.logic_function(state)
        deviation = abs(current_value - self.target_value)
        activated = deviation > 0.1  # é˜ˆå€¼å¯é…ç½®
        severity = self.weight * deviation
        
        return activated, severity

class HolyCode:
    """ç¥åœ£æ³•å…¸ HC(t)"""
    
    def __init__(self):
        self.rules: Dict[str, HolyCodeRule] = {}
        self._initialize_default_rules()
    
    def _initialize_default_rules(self):
        """åˆå§‹åŒ–é»˜è®¤è§„åˆ™é›†åˆ"""
        # æ‚£è€…å®‰å…¨åè®®
        self.rules['patient_safety'] = HolyCodeRule(
            rule_id='patient_safety',
            logic_function=lambda s: s.patient_satisfaction * s.ethical_compliance,
            weight=1.0,
            context=lambda s: True,  # å§‹ç»ˆé€‚ç”¨
            target_value=0.8,
            description='æ‚£è€…å®‰å…¨åè®® - ç¡®ä¿æ‚£è€…å®‰å…¨å’Œæ»¡æ„åº¦'
        )
        
        # èµ„æºåˆ†é…è§„åˆ™
        self.rules['resource_allocation'] = HolyCodeRule(
            rule_id='resource_allocation',
            logic_function=lambda s: s.medical_resource_utilization,
            weight=0.8,
            context=lambda s: s.financial_indicator > 0.3,
            target_value=0.75,
            description='èµ„æºåˆ†é…è§„åˆ™ - ä¼˜åŒ–åŒ»ç–—èµ„æºé…ç½®'
        )
        
        # ç´§æ€¥å“åº”åè®®
        self.rules['emergency_response'] = HolyCodeRule(
            rule_id='emergency_response',
            logic_function=lambda s: 1.0 - s.emergency_queue_length,
            weight=1.2,
            context=lambda s: s.emergency_queue_length > 0.5,
            target_value=0.7,
            description='ç´§æ€¥å“åº”åè®® - å¿«é€Ÿå“åº”ç´§æ€¥æƒ…å†µ'
        )
        
        # è´¨é‡ä¿è¯è§„åˆ™
        self.rules['quality_assurance'] = HolyCodeRule(
            rule_id='quality_assurance',
            logic_function=lambda s: (s.education_training + s.ethical_compliance) / 2,
            weight=0.9,
            context=lambda s: True,
            target_value=0.85,
            description='è´¨é‡ä¿è¯è§„åˆ™ - ç»´æŠ¤åŒ»ç–—è´¨é‡æ ‡å‡†'
        )
    
    def compute_ideal_state(self, current_state: SystemState, disturbance: np.ndarray) -> SystemState:
        """è®¡ç®—ç†æƒ³çŠ¶æ€ x*(t) = Î¨(HC(t), d(t))"""
        x = current_state.to_vector()
        x_ideal = x.copy()
        
        # åŸºäºç¥åœ£æ³•å…¸ä¼˜åŒ–ç†æƒ³çŠ¶æ€
        total_weight = sum(rule.weight for rule in self.rules.values())
        
        for rule in self.rules.values():
            if rule.context(current_state):
                # è®¡ç®—è§„åˆ™å¯¹ç†æƒ³çŠ¶æ€çš„å½±å“
                current_value = rule.logic_function(current_state)
                adjustment = rule.weight / total_weight * (rule.target_value - current_value)
                
                # æ ¹æ®è§„åˆ™ç±»å‹è°ƒæ•´ç›¸åº”çŠ¶æ€åˆ†é‡
                if 'safety' in rule.rule_id or 'patient' in rule.rule_id:
                    x_ideal[5] += 0.1 * adjustment  # æ‚£è€…æ»¡æ„åº¦
                elif 'resource' in rule.rule_id:
                    x_ideal[0] += 0.1 * adjustment  # èµ„æºåˆ©ç”¨ç‡
                elif 'emergency' in rule.rule_id:
                    x_ideal[6] += 0.1 * adjustment  # æ€¥è¯Šé˜Ÿåˆ—
                elif 'quality' in rule.rule_id:
                    x_ideal[4] += 0.1 * adjustment  # æ•™è‚²åŸ¹è®­
        
        # åŠ å…¥æ‰°åŠ¨å½±å“
        x_ideal += 0.1 * disturbance[:len(x_ideal)]
        x_ideal = np.clip(x_ideal, 0, 1)
        
        return SystemState.from_vector(x_ideal)

class Agent:
    """æ™ºèƒ½ä½“ i âˆˆ A"""
    
    def __init__(self, agent_id: str, role: str, action_space_size: int = 5):
        self.agent_id = agent_id
        self.role = role
        self.action_space_size = action_space_size
        
        # ç­–ç•¥å‚æ•° Î¸_i
        self.theta = np.random.normal(0, 0.1, action_space_size)
        
        # æ”¶ç›Šå‡½æ•°æƒé‡
        self.alpha = 0.3  # å…¨å±€æ•ˆç”¨æƒé‡
        self.beta = 0.5   # å±€éƒ¨ä»·å€¼æƒé‡
        self.gamma = 0.2  # ç†æƒ³çŠ¶æ€åå·®æƒé‡
        
        # å­¦ä¹ ç‡
        self.learning_rate = 0.01
        
        # ç‰¹å¾æ˜ å°„ç¼“å­˜
        self._feature_cache = {}
    
    def feature_mapping(self, observation: np.ndarray, action: int) -> np.ndarray:
        """ç‰¹å¾æ˜ å°„ Ï†_i(o_i, a_i)"""
        # ç®€å•çš„çº¿æ€§ç‰¹å¾æ˜ å°„
        obs_features = observation / np.linalg.norm(observation + 1e-8)
        action_features = np.zeros(self.action_space_size)
        action_features[action] = 1.0
        
        # ç»„åˆç‰¹å¾
        features = np.concatenate([obs_features, action_features])
        return features[:len(self.theta)]
    
    def policy(self, observation: np.ndarray, action: Optional[int] = None) -> np.ndarray:
        """éšæœºç­–ç•¥ Ï€_i(a_i | o_i; Î¸_i)"""
        if action is not None:
            # è®¡ç®—ç‰¹å®šåŠ¨ä½œçš„æ¦‚ç‡
            phi = self.feature_mapping(observation, action)
            logit = np.dot(phi, self.theta)
        else:
            # è®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ
            logits = []
            for a in range(self.action_space_size):
                phi = self.feature_mapping(observation, a)
                logits.append(np.dot(phi, self.theta))
            logit = np.array(logits)
        
        # Softmax
        exp_logits = np.exp(logit - np.max(logit))
        if action is not None:
            return exp_logits / np.sum(exp_logits)
        else:
            return exp_logits / np.sum(exp_logits)
    
    def sample_action(self, observation: np.ndarray) -> int:
        """é‡‡æ ·åŠ¨ä½œ"""
        probs = self.policy(observation)
        return np.random.choice(self.action_space_size, p=probs)
    
    def compute_reward(self, state: SystemState, action: int, 
                      global_utility: float, ideal_state: SystemState) -> float:
        """æ”¶ç›Šå‡½æ•° R_i(x, a_i, a_{-i})"""
        # å±€éƒ¨ä»·å€¼å‡½æ•° V_i - åŸºäºè§’è‰²ç‰¹å¼‚æ€§
        if self.role == 'doctor':
            local_value = state.medical_resource_utilization * 0.6 + state.patient_satisfaction * 0.4
        elif self.role == 'intern':
            local_value = state.education_training * 0.7 + state.medical_resource_utilization * 0.3
        elif self.role == 'patient':
            local_value = state.patient_satisfaction * 0.8 + state.patient_waiting_time * (-0.2)
        elif self.role == 'accountant':
            local_value = state.financial_indicator * 0.8 + state.medical_resource_utilization * 0.2
        elif self.role == 'government':
            local_value = state.ethical_compliance * 0.6 + state.patient_satisfaction * 0.4
        else:
            local_value = 0.5
        
        # åˆ°ç†æƒ³çŠ¶æ€çš„åå·® D_i
        state_vec = state.to_vector()
        ideal_vec = ideal_state.to_vector()
        deviation = np.linalg.norm(state_vec - ideal_vec)
        
        # ç»„åˆæ”¶ç›Š
        reward = (self.alpha * global_utility + 
                 self.beta * local_value - 
                 self.gamma * deviation)
        
        return reward
    
    def update_policy(self, observation: np.ndarray, action: int, 
                     q_value: float, baseline: float = 0.0):
        """ç­–ç•¥æ¢¯åº¦æ›´æ–° Î¸_i(t+1) = Î¸_i(t) + Î· âˆ‡J_i(Î¸)"""
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        phi = self.feature_mapping(observation, action)
        
        # è®¡ç®— âˆ‡log Ï€_i(a_i|o_i)
        probs = self.policy(observation)
        grad_log_pi = phi - np.sum([probs[a] * self.feature_mapping(observation, a) 
                                   for a in range(self.action_space_size)], axis=0)
        
        # ç­–ç•¥æ¢¯åº¦
        advantage = q_value - baseline
        policy_gradient = grad_log_pi * advantage
        
        # æ›´æ–°å‚æ•°
        self.theta += self.learning_rate * policy_gradient

class LyapunovAnalyzer:
    """æé›…æ™®è¯ºå¤«ç¨³å®šæ€§åˆ†æå™¨"""
    
    def __init__(self, state_dim: int, num_agents: int, param_dim: int):
        self.state_dim = state_dim
        self.num_agents = num_agents
        self.param_dim = param_dim
        
        # æé›…æ™®è¯ºå¤«å‡½æ•°å‚æ•°
        self.P = np.eye(state_dim)  # çŠ¶æ€æƒé‡çŸ©é˜µ
        self.Q = [np.eye(param_dim) for _ in range(num_agents)]  # ç­–ç•¥å‚æ•°æƒé‡çŸ©é˜µ
        
    def compute_lyapunov_function(self, state: SystemState, agents: List[Agent], 
                                 ideal_state: SystemState, ideal_params: List[np.ndarray]) -> float:
        """è®¡ç®—æé›…æ™®è¯ºå¤«å‡½æ•° V(z)"""
        x = state.to_vector()
        x_star = ideal_state.to_vector()
        
        # çŠ¶æ€åå·®é¡¹
        state_term = (x - x_star).T @ self.P @ (x - x_star)
        
        # ç­–ç•¥å‚æ•°åå·®é¡¹
        param_term = 0.0
        for i, agent in enumerate(agents):
            theta_diff = agent.theta - ideal_params[i]
            param_term += theta_diff.T @ self.Q[i] @ theta_diff
        
        return state_term + param_term
    
    def analyze_stability(self, trajectory: List[Tuple[SystemState, List[np.ndarray]]]) -> Dict:
        """åˆ†æç³»ç»Ÿç¨³å®šæ€§"""
        if len(trajectory) < 2:
            return {'stable': False, 'reason': 'insufficient_data'}
        
        # è®¡ç®—æé›…æ™®è¯ºå¤«å‡½æ•°å€¼åºåˆ—
        v_values = []
        ideal_state = trajectory[-1][0]  # å‡è®¾æœ€ç»ˆçŠ¶æ€ä¸ºç†æƒ³çŠ¶æ€
        ideal_params = trajectory[-1][1]
        
        for state, params in trajectory:
            # æ¨¡æ‹Ÿæ™ºèƒ½ä½“å¯¹è±¡
            mock_agents = []
            for i, param in enumerate(params):
                agent = Agent(f'agent_{i}', 'mock')
                agent.theta = param
                mock_agents.append(agent)
            
            v = self.compute_lyapunov_function(state, mock_agents, ideal_state, ideal_params)
            v_values.append(v)
        
        # æ£€æŸ¥å•è°ƒé€’å‡æ€§
        is_decreasing = all(v_values[i] >= v_values[i+1] for i in range(len(v_values)-1))
        
        # è®¡ç®—æ”¶æ•›ç‡
        if len(v_values) > 1:
            convergence_rate = np.mean([abs(v_values[i] - v_values[i+1]) / v_values[i] 
                                      for i in range(len(v_values)-1) if v_values[i] > 0])
        else:
            convergence_rate = 0.0
        
        return {
            'stable': is_decreasing,
            'convergence_rate': convergence_rate,
            'lyapunov_values': v_values,
            'final_value': v_values[-1] if v_values else 0.0
        }

class KallipolisMedicalSystem:
    """KallipolisåŒ»ç–—å…±å’Œå›½ç³»ç»Ÿ - æ•°ç†æ¨å¯¼å®Œæ•´å®ç°"""
    
    def __init__(self):
        # ç³»ç»Ÿç»„ä»¶
        self.holy_code = HolyCode()
        self.agents: List[Agent] = []
        self.lyapunov_analyzer = LyapunovAnalyzer(7, 5, 5)
        
        # ç³»ç»ŸçŠ¶æ€
        self.current_state = SystemState(
            medical_resource_utilization=0.7,
            patient_waiting_time=0.6,
            financial_indicator=0.65,
            ethical_compliance=0.8,
            education_training=0.9,
            patient_satisfaction=0.85,
            emergency_queue_length=0.2
        )
        
        # æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics = {
            'disturbance_adaptation_time': 0.0,
            'rule_update_success_rate': 0.0,
            'consensus_convergence_rate': 0.0,
            'rule_update_response_time': 0.0
        }
        
        # è½¨è¿¹è®°å½•
        self.trajectory: List[Tuple[SystemState, List[np.ndarray]]] = []
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self._initialize_agents()
    
    def _initialize_agents(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“é›†åˆ A"""
        agent_configs = [
            ('doctor', 'åŒ»ç”Ÿ'),
            ('intern', 'å®ä¹ åŒ»ç”Ÿ'), 
            ('accountant', 'ä¼šè®¡'),
            ('patient', 'æ‚£è€…ä»£è¡¨'),
            ('government', 'æ”¿åºœä»£ç†')
        ]
        
        for agent_id, role in agent_configs:
            agent = Agent(agent_id, role)
            self.agents.append(agent)
    
    def system_step(self, disturbance: np.ndarray) -> Dict:
        """æ‰§è¡Œä¸€æ­¥ç³»ç»ŸåŠ¨æ€"""
        # 1. è®¡ç®—ç†æƒ³çŠ¶æ€
        ideal_state = self.holy_code.compute_ideal_state(self.current_state, disturbance)
        
        # 2. æ™ºèƒ½ä½“è§‚æµ‹å’Œå†³ç­–
        observations = []
        actions = []
        rewards = []
        
        for agent in self.agents:
            # å±€éƒ¨è§‚æµ‹ï¼ˆç®€åŒ–ä¸ºå…¨çŠ¶æ€è§‚æµ‹åŠ å™ªå£°ï¼‰
            obs = self.current_state.to_vector() + np.random.normal(0, 0.05, 7)
            observations.append(obs)
            
            # é‡‡æ ·åŠ¨ä½œ
            action = agent.sample_action(obs)
            actions.append(action)
        
        # 3. è®¡ç®—å…¨å±€æ•ˆç”¨
        global_utility = self._compute_global_utility(self.current_state)
        
        # 4. è®¡ç®—æ”¶ç›Šå’Œæ›´æ–°ç­–ç•¥
        for i, agent in enumerate(self.agents):
            reward = agent.compute_reward(self.current_state, actions[i], 
                                        global_utility, ideal_state)
            rewards.append(reward)
            
            # Qå€¼ç®€åŒ–è®¡ç®—ï¼ˆå®é™…åº”è¯¥ç”¨æ—¶åºå·®åˆ†å­¦ä¹ ï¼‰
            q_value = reward + 0.9 * global_utility  # Î³=0.9
            
            # ç­–ç•¥æ›´æ–°
            agent.update_policy(observations[i], actions[i], q_value)
        
        # 5. çŠ¶æ€è½¬ç§»ï¼ˆç®€åŒ–çš„åŠ¨æ€æ–¹ç¨‹ï¼‰
        state_vec = self.current_state.to_vector()
        action_effects = np.array([sum(actions[i] * 0.02 for i in range(len(actions)))] * 7)
        new_state_vec = state_vec + action_effects + disturbance + np.random.normal(0, 0.01, 7)
        new_state_vec = np.clip(new_state_vec, 0, 1)
        
        self.current_state = SystemState.from_vector(new_state_vec)
        
        # 6. è®°å½•è½¨è¿¹
        agent_params = [agent.theta.copy() for agent in self.agents]
        self.trajectory.append((self.current_state, agent_params))
        
        # 7. è§„åˆ™æ¿€æ´»æ£€æŸ¥
        rule_activations = {}
        for rule_id, rule in self.holy_code.rules.items():
            activated, severity = rule.evaluate(self.current_state)
            rule_activations[rule_id] = {
                'activated': activated,
                'severity': severity,
                'description': rule.description
            }
        
        return {
            'state': self.current_state,
            'ideal_state': ideal_state,
            'actions': actions,
            'rewards': rewards,
            'rule_activations': rule_activations,
            'global_utility': global_utility
        }
    
    def _compute_global_utility(self, state: SystemState) -> float:
        """è®¡ç®—å…¨å±€èµ„æºæ•ˆç”¨å‡½æ•° U(x)"""
        state_vec = state.to_vector()
        
        # èµ„æºæ•ˆç”¨åŠ æƒç»„åˆ
        weights = np.array([0.2, -0.1, 0.15, 0.2, 0.15, 0.25, -0.15])
        utility = np.dot(weights, state_vec)
        
        return np.clip(utility, 0, 1)
    
    def analyze_system_stability(self) -> Dict:
        """åˆ†æç³»ç»Ÿç¨³å®šæ€§"""
        if len(self.trajectory) < 10:
            return {'error': 'insufficient_trajectory_data'}
        
        return self.lyapunov_analyzer.analyze_stability(self.trajectory[-50:])  # åˆ†ææœ€è¿‘50æ­¥
    
    def get_performance_metrics(self) -> Dict:
        """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        if len(self.trajectory) < 2:
            return self.performance_metrics
        
        # è®¡ç®—æ‰°åŠ¨é€‚åº”æ—¶é—´ (DAT)
        # ç®€åŒ–è®¡ç®—ï¼šçŠ¶æ€å˜åŒ–ç¨³å®šæ‰€éœ€æ—¶é—´
        recent_states = [item[0].to_vector() for item in self.trajectory[-10:]]
        if len(recent_states) >= 2:
            state_variations = [np.std(states) for states in zip(*recent_states)]
            avg_variation = np.mean(state_variations)
            self.performance_metrics['disturbance_adaptation_time'] = min(10.0, 1.0 / (avg_variation + 1e-6))
        
        # è®¡ç®—è§„åˆ™æ›´æ–°æˆåŠŸç‡ (RUSR)
        # åŸºäºè§„åˆ™æ¿€æ´»çš„ä¸€è‡´æ€§
        rule_consistency = 0.0
        if len(self.trajectory) >= 5:
            recent_activations = []
            for state, _ in self.trajectory[-5:]:
                activations = []
                for rule in self.holy_code.rules.values():
                    activated, _ = rule.evaluate(state)
                    activations.append(activated)
                recent_activations.append(activations)
            
            if recent_activations:
                consistency_scores = []
                for i in range(len(recent_activations[0])):
                    rule_sequence = [activations[i] for activations in recent_activations]
                    consistency = 1.0 - np.std(rule_sequence)
                    consistency_scores.append(consistency)
                rule_consistency = np.mean(consistency_scores)
        
        self.performance_metrics['rule_update_success_rate'] = rule_consistency
        
        # è®¡ç®—å…±è¯†æ”¶æ•›ç‡ (CCR)
        # åŸºäºæ™ºèƒ½ä½“ç­–ç•¥å‚æ•°çš„æ”¶æ•›æ€§
        if len(self.trajectory) >= 5:
            recent_params = [params for _, params in self.trajectory[-5:]]
            param_variations = []
            for i in range(len(self.agents)):
                agent_param_sequence = [params[i] for params in recent_params]
                param_std = np.std(agent_param_sequence, axis=0)
                param_variations.append(np.mean(param_std))
            
            avg_param_variation = np.mean(param_variations)
            self.performance_metrics['consensus_convergence_rate'] = max(0.0, 1.0 - avg_param_variation)
        
        return self.performance_metrics
    
    def reset(self):
        """é‡ç½®ç³»ç»ŸçŠ¶æ€"""
        self.current_state = SystemState(
            medical_resource_utilization=0.7,
            patient_waiting_time=0.6,
            financial_indicator=0.65,
            ethical_compliance=0.8,
            education_training=0.9,
            patient_satisfaction=0.85,
            emergency_queue_length=0.2
        )
        
        # é‡ç½®æ™ºèƒ½ä½“ç­–ç•¥å‚æ•°
        for agent in self.agents:
            agent.theta = np.random.normal(0, 0.1, agent.action_space_size)
        
        # æ¸…ç©ºè½¨è¿¹
        self.trajectory = []
        
        logger.info("Kallipolis Medical System has been reset")

# éªŒè¯å‡½æ•°
def verify_mathematical_implementation():
    """éªŒè¯æ•°ç†æ¨å¯¼å®ç°çš„æ­£ç¡®æ€§"""
    print("ğŸ”¬ Kallipolis Medical Republic - æ•°ç†æ¨å¯¼éªŒè¯")
    print("=" * 60)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    system = KallipolisMedicalSystem()
    
    print("âœ… ç³»ç»Ÿç»„ä»¶åˆå§‹åŒ–:")
    print(f"  - æ™ºèƒ½ä½“æ•°é‡: {len(system.agents)}")
    print(f"  - ç¥åœ£æ³•å…¸è§„åˆ™æ•°: {len(system.holy_code.rules)}")
    print(f"  - çŠ¶æ€ç©ºé—´ç»´åº¦: {len(system.current_state.to_vector())}")
    
    # è¿è¡Œä»¿çœŸæ­¥éª¤
    print("\nğŸ”„ æ‰§è¡Œä»¿çœŸæ­¥éª¤:")
    disturbances = [np.random.normal(0, 0.05, 7) for _ in range(20)]
    
    for step in range(20):
        result = system.system_step(disturbances[step])
        
        if step % 5 == 0:
            print(f"  æ­¥éª¤ {step}:")
            print(f"    å…¨å±€æ•ˆç”¨: {result['global_utility']:.3f}")
            print(f"    æ¿€æ´»è§„åˆ™æ•°: {sum(1 for r in result['rule_activations'].values() if r['activated'])}")
            print(f"    å¹³å‡æ”¶ç›Š: {np.mean(result['rewards']):.3f}")
    
    # ç¨³å®šæ€§åˆ†æ
    print("\nğŸ“Š ç¨³å®šæ€§åˆ†æ:")
    stability_result = system.analyze_system_stability()
    if 'error' not in stability_result:
        print(f"  ç³»ç»Ÿç¨³å®šæ€§: {'âœ… ç¨³å®š' if stability_result['stable'] else 'âŒ ä¸ç¨³å®š'}")
        print(f"  æ”¶æ•›ç‡: {stability_result['convergence_rate']:.6f}")
        print(f"  æé›…æ™®è¯ºå¤«å‡½æ•°å€¼: {stability_result['final_value']:.6f}")
    
    # æ€§èƒ½æŒ‡æ ‡
    print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    metrics = system.get_performance_metrics()
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.3f}")
    
    print("\nğŸ¯ éªŒè¯å®Œæˆ - æ•°ç†æ¨å¯¼å®ç°æ­£å¸¸å·¥ä½œ")
    return system

if __name__ == "__main__":
    verify_mathematical_implementation()