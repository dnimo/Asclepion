"""
åŒ»é™¢æ²»ç†ç³»ç»Ÿ - æ•°æ®å¯¼å‡ºæ¥å£
æ”¯æŒå¤šç§æ ¼å¼çš„ä»¿çœŸæ•°æ®å¯¼å‡ºåŠŸèƒ½
"""

import json
import csv
import pickle
import sqlite3
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
import logging

# å¯é€‰ä¾èµ–
try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    pd = None

try:
    import openpyxl
    HAS_OPENPYXL = True
except ImportError:
    HAS_OPENPYXL = False

logger = logging.getLogger(__name__)

@dataclass
class SimulationMetadata:
    """ä»¿çœŸå…ƒæ•°æ®"""
    simulation_id: str
    start_time: datetime
    end_time: datetime
    duration_steps: int
    llm_provider: str
    system_parameters: Dict[str, Any]
    performance_metrics: Dict[str, float]
    
@dataclass
class TimeSeriesData:
    """æ—¶åºæ•°æ®"""
    timestamps: List[float]
    states: List[np.ndarray]
    controls: List[np.ndarray]
    observations: List[np.ndarray]
    rule_activations: List[Dict[str, Any]]
    performance_indices: List[float]
    stability_metrics: List[float]

@dataclass
class AgentDecisionData:
    """æ™ºèƒ½ä½“å†³ç­–æ•°æ®"""
    agent_id: str
    decision_history: List[Dict[str, Any]]
    llm_responses: List[str]
    reasoning_chains: List[List[str]]
    performance_scores: List[float]

class DataExporter:
    """æ•°æ®å¯¼å‡ºå™¨"""
    
    def __init__(self, output_dir: str = "simulation_data"):
        """
        åˆå§‹åŒ–æ•°æ®å¯¼å‡ºå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def export_simulation_results(
        self,
        metadata: SimulationMetadata,
        time_series: TimeSeriesData,
        agent_data: List[AgentDecisionData],
        format_type: str = "all"
    ) -> Dict[str, str]:
        """
        å¯¼å‡ºå®Œæ•´ä»¿çœŸç»“æœ
        
        Args:
            metadata: ä»¿çœŸå…ƒæ•°æ®
            time_series: æ—¶åºæ•°æ®
            agent_data: æ™ºèƒ½ä½“æ•°æ®
            format_type: å¯¼å‡ºæ ¼å¼ ("json", "csv", "excel", "pickle", "sqlite", "all")
            
        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"simulation_{metadata.simulation_id}_{timestamp}"
        
        exported_files = {}
        
        if format_type in ["json", "all"]:
            json_file = self._export_to_json(metadata, time_series, agent_data, base_filename)
            exported_files["json"] = str(json_file)
            
        if format_type in ["csv", "all"]:
            csv_files = self._export_to_csv(metadata, time_series, agent_data, base_filename)
            exported_files["csv"] = csv_files
            
        if format_type in ["excel", "all"]:
            excel_file = self._export_to_excel(metadata, time_series, agent_data, base_filename)
            if excel_file:  # åªæœ‰åœ¨æˆåŠŸå¯¼å‡ºæ—¶æ‰æ·»åŠ 
                exported_files["excel"] = str(excel_file)
            
        if format_type in ["pickle", "all"]:
            pickle_file = self._export_to_pickle(metadata, time_series, agent_data, base_filename)
            exported_files["pickle"] = str(pickle_file)
            
        if format_type in ["sqlite", "all"]:
            db_file = self._export_to_sqlite(metadata, time_series, agent_data, base_filename)
            exported_files["sqlite"] = str(db_file)
            
        logger.info(f"å¯¼å‡ºå®Œæˆ: {len(exported_files)} ä¸ªæ–‡ä»¶")
        return exported_files
    
    def _export_to_json(
        self,
        metadata: SimulationMetadata,
        time_series: TimeSeriesData,
        agent_data: List[AgentDecisionData],
        base_filename: str
    ) -> Path:
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        json_file = self.output_dir / f"{base_filename}.json"
        
        # å‡†å¤‡JSONæ•°æ®
        json_data = {
            "metadata": asdict(metadata),
            "time_series": {
                "timestamps": time_series.timestamps,
                "states": [state.tolist() for state in time_series.states],
                "controls": [ctrl.tolist() for ctrl in time_series.controls],
                "observations": [obs.tolist() for obs in time_series.observations],
                "rule_activations": time_series.rule_activations,
                "performance_indices": time_series.performance_indices,
                "stability_metrics": time_series.stability_metrics
            },
            "agents": [asdict(agent) for agent in agent_data]
        }
        
        # å¤„ç†datetimeåºåˆ—åŒ–
        def json_serializer(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False, default=json_serializer)
            
        return json_file
    
    def _export_to_csv(
        self,
        metadata: SimulationMetadata,
        time_series: TimeSeriesData,
        agent_data: List[AgentDecisionData],
        base_filename: str
    ) -> Dict[str, str]:
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        csv_files = {}
        
        # å…ƒæ•°æ®CSV
        metadata_file = self.output_dir / f"{base_filename}_metadata.csv"
        with open(metadata_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=asdict(metadata).keys())
            writer.writeheader()
            
            # å¤„ç†å¤æ‚å¯¹è±¡
            metadata_dict = asdict(metadata)
            metadata_dict['start_time'] = metadata.start_time.isoformat()
            metadata_dict['end_time'] = metadata.end_time.isoformat()
            metadata_dict['system_parameters'] = json.dumps(metadata.system_parameters)
            metadata_dict['performance_metrics'] = json.dumps(metadata.performance_metrics)
            
            writer.writerow(metadata_dict)
        csv_files["metadata"] = str(metadata_file)
        
        # æ—¶åºæ•°æ®CSV
        timeseries_file = self.output_dir / f"{base_filename}_timeseries.csv"
        with open(timeseries_file, 'w', newline='', encoding='utf-8') as f:
            # å‡†å¤‡å­—æ®µå
            fieldnames = ['timestamp', 'performance_index', 'stability_metric']
            
            # æ·»åŠ çŠ¶æ€å­—æ®µ
            if time_series.states:
                state_dim = len(time_series.states[0])
                fieldnames.extend([f'state_{i}' for i in range(state_dim)])
                
            # æ·»åŠ æ§åˆ¶å­—æ®µ
            if time_series.controls:
                control_dim = len(time_series.controls[0])
                fieldnames.extend([f'control_{i}' for i in range(control_dim)])
                
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            # å†™å…¥æ•°æ®è¡Œ
            for i in range(len(time_series.timestamps)):
                row = {
                    'timestamp': time_series.timestamps[i],
                    'performance_index': time_series.performance_indices[i],
                    'stability_metric': time_series.stability_metrics[i]
                }
                
                # æ·»åŠ çŠ¶æ€æ•°æ®
                if time_series.states:
                    for j, val in enumerate(time_series.states[i]):
                        row[f'state_{j}'] = float(val)
                        
                # æ·»åŠ æ§åˆ¶æ•°æ®
                if time_series.controls:
                    for j, val in enumerate(time_series.controls[i]):
                        row[f'control_{j}'] = float(val)
                        
                writer.writerow(row)
                
        csv_files["timeseries"] = str(timeseries_file)
        
        # æ™ºèƒ½ä½“æ•°æ®CSV
        for agent in agent_data:
            agent_file = self.output_dir / f"{base_filename}_agent_{agent.agent_id}.csv"
            
            with open(agent_file, 'w', newline='', encoding='utf-8') as f:
                # å‡†å¤‡åŸºç¡€å­—æ®µå
                base_fieldnames = ['step', 'agent_id', 'performance_score', 'llm_response']
                
                # æ”¶é›†æ‰€æœ‰å†³ç­–å­—æ®µï¼ˆä»æ‰€æœ‰å†³ç­–ä¸­è·å–ï¼‰
                decision_fields = set()
                for decision in agent.decision_history:
                    for key in decision.keys():
                        decision_fields.add(f'decision_{key}')
                
                # ç»„åˆæ‰€æœ‰å­—æ®µå
                fieldnames = base_fieldnames + sorted(list(decision_fields))
                
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                # å†™å…¥æ™ºèƒ½ä½“æ•°æ®
                for i, decision in enumerate(agent.decision_history):
                    row = {
                        'step': i,
                        'agent_id': agent.agent_id,
                        'performance_score': agent.performance_scores[i] if i < len(agent.performance_scores) else None,
                        'llm_response': agent.llm_responses[i] if i < len(agent.llm_responses) else "",
                    }
                    
                    # æ·»åŠ å†³ç­–è¯¦æƒ…ï¼ˆå…ˆåˆå§‹åŒ–æ‰€æœ‰å­—æ®µä¸ºç©ºï¼‰
                    for field in decision_fields:
                        row[field] = ""
                    
                    # å¡«å…¥å®é™…å†³ç­–æ•°æ®
                    for key, value in decision.items():
                        field_name = f'decision_{key}'
                        if field_name in fieldnames:
                            row[field_name] = str(value)
                        
                    writer.writerow(row)
                    
            csv_files[f"agent_{agent.agent_id}"] = str(agent_file)
        
        return csv_files
    
    def _export_to_excel(
        self,
        metadata: SimulationMetadata,
        time_series: TimeSeriesData,
        agent_data: List[AgentDecisionData],
        base_filename: str
    ) -> Path:
        """å¯¼å‡ºä¸ºExcelæ ¼å¼ï¼ˆéœ€è¦pandaså’Œopenpyxlï¼‰"""
        if not HAS_PANDAS or not HAS_OPENPYXL:
            logger.warning("Excelå¯¼å‡ºéœ€è¦pandaså’Œopenpyxlåº“ï¼Œè·³è¿‡Excelå¯¼å‡º")
            return None
            
        excel_file = self.output_dir / f"{base_filename}.xlsx"
        
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # å…ƒæ•°æ®å·¥ä½œè¡¨
            metadata_dict = asdict(metadata)
            metadata_dict['start_time'] = metadata.start_time.isoformat()
            metadata_dict['end_time'] = metadata.end_time.isoformat()
            metadata_dict['system_parameters'] = json.dumps(metadata.system_parameters)
            metadata_dict['performance_metrics'] = json.dumps(metadata.performance_metrics)
            
            metadata_df = pd.DataFrame([metadata_dict])
            metadata_df.to_excel(writer, sheet_name='Metadata', index=False)
            
            # æ—¶åºæ•°æ®å·¥ä½œè¡¨
            timeseries_df = pd.DataFrame({
                'timestamp': time_series.timestamps,
                'performance_index': time_series.performance_indices,
                'stability_metric': time_series.stability_metrics
            })
            
            # æ·»åŠ çŠ¶æ€å’Œæ§åˆ¶æ•°æ®
            for i in range(len(time_series.states[0])):
                timeseries_df[f'state_{i}'] = [state[i] for state in time_series.states]
            for i in range(len(time_series.controls[0])):
                timeseries_df[f'control_{i}'] = [ctrl[i] for ctrl in time_series.controls]
                
            timeseries_df.to_excel(writer, sheet_name='TimeSeries', index=False)
            
            # è§„åˆ™æ¿€æ´»å·¥ä½œè¡¨
            rule_activations_data = []
            for i, activations in enumerate(time_series.rule_activations):
                for rule_name, details in activations.items():
                    rule_activations_data.append({
                        'step': i,
                        'rule_name': rule_name,
                        'activated': details.get('activated', False),
                        'severity': details.get('severity', 0),
                        'description': details.get('description', '')
                    })
            
            if rule_activations_data:
                rules_df = pd.DataFrame(rule_activations_data)
                rules_df.to_excel(writer, sheet_name='RuleActivations', index=False)
            
            # æ™ºèƒ½ä½“æ•°æ®å·¥ä½œè¡¨
            for agent in agent_data:
                agent_rows = []
                for i, decision in enumerate(agent.decision_history):
                    row = {
                        'step': i,
                        'performance_score': agent.performance_scores[i] if i < len(agent.performance_scores) else None,
                        'llm_response_length': len(agent.llm_responses[i]) if i < len(agent.llm_responses) else 0,
                    }
                    # æ·»åŠ å†³ç­–æ‘˜è¦
                    row.update({f'decision_{k}': str(v)[:100] for k, v in decision.items()})
                    agent_rows.append(row)
                
                if agent_rows:
                    agent_df = pd.DataFrame(agent_rows)
                    sheet_name = f'Agent_{agent.agent_id}'[:31]  # Excelå·¥ä½œè¡¨åç§°é™åˆ¶
                    agent_df.to_excel(writer, sheet_name=sheet_name, index=False)
        
        return excel_file
    
    def _export_to_pickle(
        self,
        metadata: SimulationMetadata,
        time_series: TimeSeriesData,
        agent_data: List[AgentDecisionData],
        base_filename: str
    ) -> Path:
        """å¯¼å‡ºä¸ºPickleæ ¼å¼ï¼ˆä¿æŒPythonå¯¹è±¡ç»“æ„ï¼‰"""
        pickle_file = self.output_dir / f"{base_filename}.pkl"
        
        data = {
            'metadata': metadata,
            'time_series': time_series,
            'agent_data': agent_data
        }
        
        with open(pickle_file, 'wb') as f:
            pickle.dump(data, f)
            
        return pickle_file
    
    def _export_to_sqlite(
        self,
        metadata: SimulationMetadata,
        time_series: TimeSeriesData,
        agent_data: List[AgentDecisionData],
        base_filename: str
    ) -> Path:
        """å¯¼å‡ºä¸ºSQLiteæ•°æ®åº“"""
        db_file = self.output_dir / f"{base_filename}.db"
        
        conn = sqlite3.connect(db_file)
        
        try:
            # åˆ›å»ºè¡¨ç»“æ„
            self._create_sqlite_schema(conn)
            
            # æ’å…¥å…ƒæ•°æ®
            self._insert_metadata(conn, metadata)
            
            # æ’å…¥æ—¶åºæ•°æ®
            self._insert_timeseries(conn, time_series, metadata.simulation_id)
            
            # æ’å…¥æ™ºèƒ½ä½“æ•°æ®
            self._insert_agent_data(conn, agent_data, metadata.simulation_id)
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            logger.error(f"SQLiteå¯¼å‡ºå¤±è´¥: {e}")
            raise
        finally:
            conn.close()
            
        return db_file
    
    def _create_sqlite_schema(self, conn: sqlite3.Connection):
        """åˆ›å»ºSQLiteæ•°æ®åº“æ¶æ„"""
        cursor = conn.cursor()
        
        # ä»¿çœŸå…ƒæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS simulations (
                simulation_id TEXT PRIMARY KEY,
                start_time TEXT,
                end_time TEXT,
                duration_steps INTEGER,
                llm_provider TEXT,
                system_parameters TEXT,
                performance_metrics TEXT
            )
        ''')
        
        # æ—¶åºæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS timeseries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                simulation_id TEXT,
                step INTEGER,
                timestamp REAL,
                state_vector TEXT,
                control_vector TEXT,
                observation_vector TEXT,
                performance_index REAL,
                stability_metric REAL,
                FOREIGN KEY (simulation_id) REFERENCES simulations (simulation_id)
            )
        ''')
        
        # è§„åˆ™æ¿€æ´»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS rule_activations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                simulation_id TEXT,
                step INTEGER,
                rule_name TEXT,
                activated BOOLEAN,
                severity REAL,
                description TEXT,
                FOREIGN KEY (simulation_id) REFERENCES simulations (simulation_id)
            )
        ''')
        
        # æ™ºèƒ½ä½“å†³ç­–è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS agent_decisions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                simulation_id TEXT,
                agent_id TEXT,
                step INTEGER,
                decision_data TEXT,
                llm_response TEXT,
                reasoning_chain TEXT,
                performance_score REAL,
                FOREIGN KEY (simulation_id) REFERENCES simulations (simulation_id)
            )
        ''')
        
    def _insert_metadata(self, conn: sqlite3.Connection, metadata: SimulationMetadata):
        """æ’å…¥å…ƒæ•°æ®"""
        cursor = conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO simulations 
            (simulation_id, start_time, end_time, duration_steps, llm_provider, 
             system_parameters, performance_metrics)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            metadata.simulation_id,
            metadata.start_time.isoformat(),
            metadata.end_time.isoformat(),
            metadata.duration_steps,
            metadata.llm_provider,
            json.dumps(metadata.system_parameters),
            json.dumps(metadata.performance_metrics)
        ))
        
    def _insert_timeseries(self, conn: sqlite3.Connection, time_series: TimeSeriesData, sim_id: str):
        """æ’å…¥æ—¶åºæ•°æ®"""
        cursor = conn.cursor()
        
        for i, timestamp in enumerate(time_series.timestamps):
            # æ’å…¥æ—¶åºè®°å½•
            cursor.execute('''
                INSERT INTO timeseries 
                (simulation_id, step, timestamp, state_vector, control_vector, 
                 observation_vector, performance_index, stability_metric)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                sim_id, i, timestamp,
                json.dumps(time_series.states[i].tolist()),
                json.dumps(time_series.controls[i].tolist()),
                json.dumps(time_series.observations[i].tolist()),
                time_series.performance_indices[i],
                time_series.stability_metrics[i]
            ))
            
            # æ’å…¥è§„åˆ™æ¿€æ´»è®°å½•
            if i < len(time_series.rule_activations):
                for rule_name, details in time_series.rule_activations[i].items():
                    cursor.execute('''
                        INSERT INTO rule_activations 
                        (simulation_id, step, rule_name, activated, severity, description)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        sim_id, i, rule_name,
                        details.get('activated', False),
                        details.get('severity', 0),
                        details.get('description', '')
                    ))
    
    def _insert_agent_data(self, conn: sqlite3.Connection, agent_data: List[AgentDecisionData], sim_id: str):
        """æ’å…¥æ™ºèƒ½ä½“æ•°æ®"""
        cursor = conn.cursor()
        
        for agent in agent_data:
            for i, decision in enumerate(agent.decision_history):
                cursor.execute('''
                    INSERT INTO agent_decisions 
                    (simulation_id, agent_id, step, decision_data, llm_response, 
                     reasoning_chain, performance_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    sim_id, agent.agent_id, i,
                    json.dumps(decision),
                    agent.llm_responses[i] if i < len(agent.llm_responses) else "",
                    json.dumps(agent.reasoning_chains[i] if i < len(agent.reasoning_chains) else []),
                    agent.performance_scores[i] if i < len(agent.performance_scores) else None
                ))

class DataImporter:
    """æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self, data_dir: str = "simulation_data"):
        """
        åˆå§‹åŒ–æ•°æ®å¯¼å…¥å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
        """
        self.data_dir = Path(data_dir)
        
    def import_from_json(self, json_file: Union[str, Path]) -> Dict[str, Any]:
        """ä»JSONæ–‡ä»¶å¯¼å…¥æ•°æ®"""
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # é‡å»ºnumpyæ•°ç»„
        if 'time_series' in data:
            ts = data['time_series']
            ts['states'] = [np.array(state) for state in ts['states']]
            ts['controls'] = [np.array(ctrl) for ctrl in ts['controls']]
            ts['observations'] = [np.array(obs) for obs in ts['observations']]
            
        return data
    
    def import_from_pickle(self, pickle_file: Union[str, Path]) -> Dict[str, Any]:
        """ä»Pickleæ–‡ä»¶å¯¼å…¥æ•°æ®"""
        with open(pickle_file, 'rb') as f:
            return pickle.load(f)
    
    def import_from_sqlite(self, db_file: Union[str, Path], simulation_id: str) -> Dict[str, Any]:
        """ä»SQLiteæ•°æ®åº“å¯¼å…¥æ•°æ®"""
        conn = sqlite3.connect(db_file)
        
        try:
            # å¯¼å…¥å…ƒæ•°æ®
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM simulations WHERE simulation_id = ?", [simulation_id])
            metadata_row = cursor.fetchone()
            
            if not metadata_row:
                raise ValueError(f"æœªæ‰¾åˆ°ä»¿çœŸID: {simulation_id}")
            
            # è·å–åˆ—å
            cursor.execute("PRAGMA table_info(simulations)")
            columns = [row[1] for row in cursor.fetchall()]
            metadata = dict(zip(columns, metadata_row))
                
            # å¯¼å…¥æ—¶åºæ•°æ®
            cursor.execute("SELECT * FROM timeseries WHERE simulation_id = ? ORDER BY step", [simulation_id])
            timeseries_rows = cursor.fetchall()
            
            cursor.execute("PRAGMA table_info(timeseries)")
            ts_columns = [row[1] for row in cursor.fetchall()]
            timeseries = [dict(zip(ts_columns, row)) for row in timeseries_rows]
            
            # å¯¼å…¥è§„åˆ™æ¿€æ´»æ•°æ®
            cursor.execute("SELECT * FROM rule_activations WHERE simulation_id = ? ORDER BY step, rule_name", [simulation_id])
            rules_rows = cursor.fetchall()
            
            cursor.execute("PRAGMA table_info(rule_activations)")
            rules_columns = [row[1] for row in cursor.fetchall()]
            rules = [dict(zip(rules_columns, row)) for row in rules_rows]
            
            # å¯¼å…¥æ™ºèƒ½ä½“æ•°æ®
            cursor.execute("SELECT * FROM agent_decisions WHERE simulation_id = ? ORDER BY agent_id, step", [simulation_id])
            agents_rows = cursor.fetchall()
            
            cursor.execute("PRAGMA table_info(agent_decisions)")
            agents_columns = [row[1] for row in cursor.fetchall()]
            agents = [dict(zip(agents_columns, row)) for row in agents_rows]
            
            return {
                'metadata': metadata,
                'timeseries': timeseries,
                'rules': rules,
                'agents': agents
            }
            
        finally:
            conn.close()

def create_sample_data() -> tuple:
    """åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•"""
    # ç¤ºä¾‹å…ƒæ•°æ®
    metadata = SimulationMetadata(
        simulation_id="test_sim_001",
        start_time=datetime.now(),
        end_time=datetime.now(),
        duration_steps=10,
        llm_provider="mock",
        system_parameters={"param1": 1.0, "param2": "test"},
        performance_metrics={"stability": 0.95, "efficiency": 0.88}
    )
    
    # ç¤ºä¾‹æ—¶åºæ•°æ®
    time_series = TimeSeriesData(
        timestamps=list(range(10)),
        states=[np.random.rand(5) for _ in range(10)],
        controls=[np.random.rand(3) for _ in range(10)],
        observations=[np.random.rand(4) for _ in range(10)],
        rule_activations=[
            {"rule1": {"activated": True, "severity": 0.5, "description": "æµ‹è¯•è§„åˆ™"}}
            for _ in range(10)
        ],
        performance_indices=np.random.rand(10).tolist(),
        stability_metrics=np.random.rand(10).tolist()
    )
    
    # ç¤ºä¾‹æ™ºèƒ½ä½“æ•°æ®
    agent_data = [
        AgentDecisionData(
            agent_id="doctor_agent",
            decision_history=[{"action": f"action_{i}", "confidence": 0.8} for i in range(10)],
            llm_responses=[f"Response {i}" for i in range(10)],
            reasoning_chains=[[f"step{i}_1", f"step{i}_2"] for i in range(10)],
            performance_scores=np.random.rand(10).tolist()
        )
    ]
    
    return metadata, time_series, agent_data

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®å¯¼å‡ºåŠŸèƒ½
    print("ğŸ§ª æµ‹è¯•æ•°æ®å¯¼å‡ºåŠŸèƒ½...")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    metadata, time_series, agent_data = create_sample_data()
    
    # åˆ›å»ºå¯¼å‡ºå™¨
    exporter = DataExporter("test_export")
    
    # å¯¼å‡ºæ•°æ®
    try:
        exported_files = exporter.export_simulation_results(
            metadata, time_series, agent_data, format_type="all"
        )
        
        print("âœ… å¯¼å‡ºæˆåŠŸ!")
        for format_name, file_path in exported_files.items():
            print(f"  {format_name}: {file_path}")
            
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        
    print("\nğŸ“š æ•°æ®å¯¼å‡ºæ¨¡å—ä½¿ç”¨ç¤ºä¾‹:")
    print("  from src.hospital_governance.interfaces.data_export import DataExporter")
    print("  exporter = DataExporter('output_dir')")
    print("  files = exporter.export_simulation_results(metadata, time_series, agent_data)")